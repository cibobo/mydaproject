\chapter{Implementierung}

\begin{figure}[ftb]
\centering
\includegraphics[scale=0.55]{Abbildungen/HauptAblauf.png}
\caption{Ablaufdiagramm}
\label{hAblauf}
\end{figure}

Die Implementierung kann in vier Hauptmodule unterteilt werden:

\begin{itemize}
\item Markenanalyse,
\item Objekteinlernen,
\item Objekterkennung und Verfolgung,
\item Bilder Steuerung.
\end{itemize}

Der allgemeine Ablaufprozess des Programms ist in Abbildung~\ref{hAblauf} gegeben. Das ganze Programm ist eine Schleife, in jeden deren Schritte die Information der PMD Kamera angesammelt und als Eingabe genutzt wird. Nach Bewertung der neuen Eingabe bzw. der Rückkopplung vom letzten Ablauf werden die entsprechenden Bilddaten vom Modul {\itshape Bilder Steuerung} für die weiteren Schritte ausgewählt. Das Modul {\itshape Markenanalyse} analysiert die eingegebenen Bilddaten und versucht die Marken zu finden und zu verfolgen. Die gefundenen Marken werden entweder von dem {\itshape Objekteinlernen} oder durch die {\itshape Objekterkennung und Verfolgung} benutzt, das am Anfang des Programms durch den Benutzer entschieden wird. In der Einlernphase wird zuerst die Transformation des Objekts bestimmt, und dann der charakteristische Graph des Objekts durch die Marken dargestellt. Wenn man ein Objekt wieder erkennen möchte, werden die Marken vom neuen Objekt zu den vorhandenen charakteristischen Graphen verglichen, die nach dem Objekteinlernen im Speicher hinterlegt werden. Das Ergebnis wird in einem OpenGL Fenster angezeigt und als Rückkopplung für den nächsten Schritt genutzt. Alle diese Module werden in den folgenden Unterabschnitten genau erklärt. Ein ausführliches Ablaufdiagramm wird in Abbildung~\ref{gAblauf} gezeigt.  

\begin{figure}
\centering
\includegraphics[scale=0.7]{Abbildungen/Ablaufsbild.png}
\caption{Ausführliches Ablaufdiagramm}
\label{gAblauf}
\end{figure}

\section{Markenanalyse}
\label{MAna}
Zuerst müssen alle Marken aus den Eingabebildern entweder für das Objekteinlernen oder die Objekterkennung und Verfolgung erkannt werden. In diesem Abschnitt werden alle dafür benötigten Verfahren und Algorithmen vorgestellt. Die Objekte sollen aus den Eingabebildern vor dem Lernen segmentiert werden, damit die Störung von der Umgebung vermieden werden kann. Danach  vergrößert die Helligkeitssteuerung den Unterschied zwischen den Marken und anderen Bildpunkten. Daraufhin wird der STAR Detektor durchgeführt. Dadurch können viele Merkmale über die künstlichen Marken erkannt werden. Da der Detektor keine 1 zu 1 Korrespondenz zwischen den realen Marken und den erkannten Merkmalen garantiert, ist die zusätzliche Kombination der Merkmale für gleiche Marke notwendig. Die Segmentierung verteilt die Marken auf die verschiedenen Objekte und die Verfolgung liefert die Abhängigkeit der Marken zwischen den unterschiedlichen Bildern.

\subsection{Bildvorverarbeitung}

\subsubsection{Datenstruktur des Eingabebilds}
Wie im Abschnitt \ref{PMDData} erklärt, liefert die PMD Kamera insgesamt vier verschiedenen Arten der Vermessungsdaten. Alle diese Daten eines Bildes werden in einer Instanz von Klassen \textbf{BildData} gespeichert. Dazwischen sind die Amplituden und 3D-Koordinaten von hoher Relevanz und werden in verschiedenen Teilen des Programms verwendet: die Amplituden werden für die Markenerkennung und die 3D-Koordinaten mit räumlicher Information werden später für die Korrespondenzuntersuchung und Schätzung der Lage des Objekts benutzt. Da die definierte Dimension und das definierte Intervall dieser zwei Arten von Daten sich von anderen unterscheiden, ist vor der weiteren Bildverarbeitung dieser Arbeit die Übereinstimmung beider Daten für jeden Pixel notwendig. Die Klasse \textbf{PMDPoint} wird nun definiert, um diesen Zweck zu erfüllen. Die Klassendiagramme beider Klassen werden in der Abbildung~\ref{BD} gezeigt.

\begin{figure}[ftb]
\centering
\includegraphics[scale=1]{Abbildungen/BildData.png}
\caption{Die Klassendiagramme für \textbf{BildData} und \textbf{PMDPoint}.}
\label{BD}
\end{figure}

\subsubsection{Abstand Filter}
Die Verbesserung der Ergebnisse der Segmentierung des fokussierten Objekts aus der Umgebung wurde im Abschnitt \ref{sbSeg} bemerkt. Deshalb soll am Anfang der Markenanalyse ein Abstand-Filter definiert werden. Der Abstand-Filter dieser Arbeit besteht aus zwei Teilen: der Teil der Initialisierung bzw. der Teil der Filterung. Zwischen der Initialisierung des Filters werden eine Menge der Tiefdaten der Eingabebilder mit vordefinierter Größe angesammelt, damit das durchschnittliche Tiefenbild der Szene erzeugt werden kann. Der Pseudocode wird in Algorithmus~\ref{AFIni} gezeigt.

\begin{algorithm}
\caption{Initialisierung des Abstand-Filters}
\label{AFIni}
\begin{algorithmic}
    \State Anzahl der notwendigen Eingabebilder: $N$
    \State Tiefbild für die durchschnittliche Szene: $D$
    \For {$i$ von $1$ bis $N$}
    	\State $E \gets$ aktuelle \textbf{BildData}
        \If {$D$ == NULL}
            \State $D \gets E.3DKoordinaten.Z$ 
        \Else
        	\For {jedes Pixel $d_j$ von $D$}
        		\State $d_j \gets \frac{1}{2}(d_j + e.3DKoordinaten_{j_z})$
        	\EndFor
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}

Durch Vergleich der in Initialisierungsphase erzeugten Szene und die aktuelle Tiefdaten der 3D-Eingabe, können die neu in der Szene eingefügten Objekte aus dem Hintergrund bestimmt werden. Wenn der Abstand zwischen einem Pixel und dem Hintergrund größer als der vordefinierte Schwellwert ist, wird die Amplitude des gefilterten Bildes in diesem Pixel von originaler Amplitude übertragen. Sonst wird die Amplitude als null ersetzt. Die Anzahl der unterschiedlichen Bildpunkte wird gleichzeitig abgezählt, damit eine boolesche Ausgabe über die Verschiedenheit mit dem gefilterten Bild zusammen zurückgegeben werden kann. Der genaue Ablauf wird in Algorithmus~\ref{AFFilter} beschrieben. 

\begin{algorithm}
\caption{Filterungsphase des Abstand-Filters}
\label{AFFilter}
\begin{algorithmic}
	\State Schwellwert für Abstandsvergleich: $\epsilon$
	\State Schwellwert für Verschiedenheit: $\alpha$	
	\State $E \gets$ aktuelle \textbf{BildData}
	\For {jedes Pixel $d_i$ von $D$}
		\If {$\|d_i - e.3DKoordinaten_{i_z} \| < \epsilon$}
			\State $e.filteredAmplitude_i \gets 0$
		\Else
			\State $e.filteredAmplitude_i \gets e.Amplitude_i$
		\EndIf
	\EndFor
	\State $q \gets \| \text{veränderten Pixeln} \| / \|E\|$
	\If {$q < \alpha$}
		\State \Return Falsch
	\Else
		\State \Return True, $E$
	\EndIf
\end{algorithmic} 
\end{algorithm}

\subsection{Markenerkennung}

\subsubsection{Auswahl der Größe der Marken}
Die Erkennungsergebnisse der Objekte werden von der Größe der Marken stark beeinflusst, weshalb ein Test über die Markengröße vor der Implementierung notwendig ist. Die Testmarken werden als weiße Punkte auf einem schwarzen Papier gedruckt. Es gibt insgesamt sieben verschiedene Größenstufen. Die Marken von jeder Stufe werden jeweils durch Quadrate und Kreise dargestellt. Abbildung~\ref{MS1} zeigt die unterschiedliche Erkennungsergebnisse der Marken, wenn das Objekt zwar an verschiedenen Positionen aber auf der gleichen Höhenebene liegt. Abbildung~\ref{MS2} zeigt den Einfluss auf die Ergebnisse von der Distanz zu der Kamera. Hierbei wird deutlich, dass, je näher das Objekt zu der Kamera angebracht wird, desto kleinere Markengrößen benötigt werden. (folgt aus dem Kapitel \ref{AueM})

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{Abbildungen/MarkerSize1.png}
\caption{Die Erkennungsergebnisse der Marken für unterschiedliche Positionen.}
\label{MS1}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.75]{Abbildungen/MarkerSize2.png}
\caption{Die Erkennungsergebnisse der Marken für unterschiedliche Distanzen zur Kamera. Zu der ersten Reihe liegen die Graustufenbilder mit roten erkannten Marken; die entsprechenden 3D-Bilder in der zweiten Reihe zeigen die vertikalen Positionen des Objekts.}
\label{MS2}
\end{figure}

\subsubsection{STAR Detektor}
Wegen der in Abschnitt \ref{AusAlgo} festgestellten Gründe wird der STAR Detektor (CenSurE Algorithmus) in dieser Arbeit als Erkennungsalgorithmus ausgewählt. Die Parameter des Detektors werden im Dokument von OpenCV aufgelistet \cite{SDO} und von ,,ButterCookies'' in OpenCV Adventure \cite{SDOA} weiter deutlich erklärt. Der Parameter {\itshape maxSize} definiert die maximale Größe der Marken. {\itshape responseThreshold} ist ein Schwellwert über die Antwort von approximierten Laplacian. Die erkannten Marken mit niedrigerer Antwort als diesen Schwellwert werden dann eliminiert. Die Parameter {\itshape lineThresholdProjected} und {\itshape lineThresholdBinarized} stellen die Stärke der Linie-Suppression ein. Der letzte Parameter {\itshape suppressNonmaxSize} beschreibt die Größe des betrachteten Raums der Non-Maximal Suppression. In dieser Arbeit wird ResponseThreshold als der variable Parameter benutzt, d.h., dass dessen Wert in jedem Schritt verändert wird.

\subsubsection{Markenerkennung} 
Algorithmus~\ref{ME} zeigt den Verlauf der Markenerkennung mit STAR Detektor. Der ganze Erkennungsprozess wird als eine Schleife mit vordefinierter, maximaler Anzahl der Durchläufe dargestellt. In jedem Schleifenrumpf wird das Graustufenbild als Eingabe des STAR Detektors mit dem aktuellen Kontrast vor der Erkennung erzeugt. Nach der Erkennung schickt das Programm dann das aktuelle Graustufenbild und die Anzahl der erkannten Marken zu der Funktion \textbf{Kontraststeuerung} (siehe Kap.\ref{SHuK} und Algorithmus~\ref{KS}). Die Kontraststeuerung überprüft, ob zufriedenstellend so viel Marken erkannt werden. Eine Vorhersage über Kontrast bzw. ResponseThreshold für die nächste Schleife wird gleichzeitig erzeugt, wenn es zu wenig oder zu viel Marken gefunden werden.

\begin{algorithm}[htbp]
\caption{Markenerkennung}
\label{ME}
\begin{algorithmic}
	\State Eingabebild: $I$, maximale Schleife: $N$
	\State $a \gets$ Anfangswert für Kontrast 
	\State $r \gets$ Anfangswert für ResponseThreshold
	\State $b \gets$ feste Helligkeit  
	\For {$i = 1 \to N$}
		\State Eingabe für STAR Detektor: $H_i \gets aI+b$
		\State Erkannte Marken: $P \gets CenSurE(H_i,r)$
		\State Result $\gets$ Kontraststeuerung($H_i, a, r, \|P\|$)
		\If {Result = False}
			\State continue mit aktualisiertem Kontrast und ResponseThreshold
		\Else
			\State break
		\EndIf
	\EndFor
	\Return $P$
\end{algorithmic}
\end{algorithm}

Die Punktmenge, die durch STAR-Detektor erkannt wird, hat leider keine 1 zu 1 Korrespondenz zu den realen Marken, die auf der Oberfläche des Objektes angebracht werden. Daraus bedeutet, dass eine Marke des Objektes von dem Detektor häufig als viele verschiedene Merkmale erkannt wird. Deshalb ist eine Kombination der Merkmale nötig, die zur gleichen Marke gehören. Dieser Prozess kann in zwei Schritten zusammengefasst werden. Zuerst werden alle Merkmale durch den Algorithmus DBSCAN (siehe Kapitel~\ref{Dbscan}) in viele Gruppen verteilt. Daraufhin wird  für jede Gruppe  der zentrale Punkt berechnet, der als einziges Merkmal eine reale Marke beschreibt.

\subsubsection{Steuerung der Helligkeit und des Kontrast}
\label{SHuK}
Die Helligkeit bzw. der Kontrast kann die Ergebnisse der Markenerkennung stark beeinflussen. Eine bekannte Arbeit über die radiometrische Kalibrierung wurde von Debevec und Malik in \cite{DM08} veröffentlicht. Ihr Verfahren ist robust und präzise, benötigt aber viele Bilder der gleichen Szene mit verschiedenen Belichtungszeiten, um alle Informationen der Szene für einen jeweils helleren Bereich bzw. dunkleren Bereich zu sammeln. Wegen der Einschränkung der Kamera kann diese Bedingung in unserer Arbeit leider nicht erfüllt werden. Außerdem ist das Verfahren von Debevec sehr rechenintensiv. nur die Bilddaten um Marken sollen aber in der Markenerkennung dieser Arbeit betrachtet werden, wodurch die sonstigen Informationen des Bildes  einfach ignoriert werden können. Durch diese Grundidee kann ein Algorithmus erzeugt werden, dessen Endbedingung durch die Untersuchung der Anzahl der erkannten Marken eingestellt wird.
\\
\\
Der formulierte Durchlauf wird im Algorithmus~\ref{KS} dargestellt. Die Anzahl der erkannten Marken ist die erste Stufe der Prüfnorm der Kontraststeuerung. Wenn nicht genug Marken erkannt werden, dann wird die Strahlungsenergie des Graustufenbildes als die zweite Stufe der Prüfnorm überprüft. Bei einer zu großen bzw. zu kleinen Strahlungsenergie verringert bzw. erhöht sich der Kontrast  im erlaubten Intervall, welches  als Eingangsparameter vorher definiert wird. Wenn die Strahlungsenergie zufriedenstellend ist, aber noch nicht genug Marken gefunden sind, nimmt das ResponseThreshold ab, damit mehr Marken mit schwächeren Antworten erkannt werden können. Falls zu viele Marken erkannt werden, wird der Wert von ResponseThreshold vergrößert. Bei dieser Situation wird die Strahlungsenergie nicht mehr betrachtet, damit der Algorithmus zur Konvergenz halten kann. Ein boolescher Wert wird von dem Algorithmus zurückgegeben, der zeigt, ob befriedigend viele Marken gefunden werden.

\begin{algorithm}
\caption{Kontraststeuerung($H, \&a, \&r, \|P\|$)}
\label{KS}
\begin{algorithmic}
	\State Intervall der erlaubten Strahlungsenergie: $(E_{min}, E_{max})$
	\State Intervall des erlaubten Kontrastes: $(A_{min}, A_{max})$
	\State Intervall des erlaubten ResponseThreshold von STAR Detektor: $(R_{min}, R_{max})$
	\State Intervall des Erwartens der Anzahl des bekannten Marken: $(P_{min}, P_{max})$
	\State $e \gets$ aktuelle Strahlungsenergie von $H$
	\If {$\|P\| < P_{min}$}
		\If {$e < E_{min}$}
			\State $a \gets a-0.5$
			\If {$a < A_{min}$}
				\State $a \gets A_{min}$
				\State \Return False
			\EndIf
		\ElsIf {$e > E_{max}$}
			\State $a \gets a+0.5$
			\If {$a > A_{max}$}
				\State $a \gets A_{max}$
				\State \Return False
			\EndIf
		\Else 
			\State $r \gets r-5$
			\If {$r < R_{min}$}
				\State \Return False
			\EndIf
		\EndIf
	\ElsIf {$\|P\| > P_{max}$}
		\If {$r > R_{max}$}
			\State \Return False
		\Else
			\State $r \gets r+4$
		\EndIf
	\Else
		\State \Return True
	\EndIf
\end{algorithmic}
\end{algorithm}



\subsection{Verbesserung der Singulärwertzerlegungsverfahren}
\label{MV}
\label{VdS}
Nach erfolgreicher Festlegung der Marken jedes Bildes sollen dann die Korrespondenzen der Marken von zwei nachfolgenden Bildern untersucht werden. Die Korrespondenzpunkte können durch das Verfahren der Singulärwertzerlegung von Scott und Higgnis \cite{SL91} bestimmt werden. Die genaue Beschreibung ihres Verfahrens wird im Schnitt~\ref{KdS} aufgeführt, und Algorithmus~\ref{alg1} gibt den Peseudocode des Verfahrens an. In dieser Arbeit werden drei Verbesserungen für das Verfahren von Scott und Higgnis durchgeführt, damit das stabilere Ergebnis erzeugt werden kann. Die Qualität der Korrespondenzuntersuchung kann auch nach der Verbesserung bewertet werden. 
\\
\\
\textbf{Nebenbedingung für die Bestimmung der größten Elemente}
\\
In dem originalen Algorithmus werden die Punkte $I_i$ und $J_j$ genau dann als Korrespondenzpunkte erkannt, wenn das Element $P_{ij}$ das größte Element von beiden Zeile $i$ und Spalte $j$ ist. Aber manchmal liefert das größte Element nicht die beste Korrespondenz, insbesondere in dem Fall einer  großen Transformation zwischen zwei betrachteten Bildern. Die Lösung besteht darin, eine weitere Beschränkung für die Untersuchung der größten Elemente einzusetzen. D.h., dass die größten Elemente nur dann akzeptiert werden, wenn sie gleichzeitig größer als ein eingegebener Schwellwert $\epsilon$ sind. Der Schwellwert $\epsilon$ wird in $[0,1]$ definiert. Je höher $\epsilon$ ist, desto ordentlicher sind die gefundenen Korrespondenzpunkte. In Idealfall sind alle größten Elemente $P_{ij}$ gleich 1, z.B. wenn die beiden betrachteten Bilder identisch sind. Der Nachteil der Verwendung des Schwellwerts liegt darin, dass manchmal keine Korrespondenz zwischen zwei Bilder gefunden kann. Deshalb ist eine boolesche Ausgabe des Algorithmus notwendig, welches sich als  eine wichtige Variable der Bildsteuerung darstellt (siehe Kapitel~\ref{BildS}). Das Programm kann durch diese Variable erkennen, ob die Korrespondenzuntersuchung erfolgreich durchgeführt wird.
\\
%\\
%\textbf{Boolesche Ausgabe}
%\\
%Weniger Korrespondenzpunkte werden mit obiger stärkeren Nebenbedingung herausgefunden, was aber die weiteren Arbeitsschritte wenig beeinflusst, weil es zumindest nur 3 korrespondierenden Punktpaare benötigt, um die Orientierung zwischen zwei Bildern zu bestimmen. Trotzdem ist die Überprüfung der Anzahl der gefundenen Punktpaare notwendig, und deren Ergebnis wird als eine boolesche Ausgabe des Algorithmus zurückgegeben.
%\\
\\
\textbf{Qualitätsüberprüfung der Korrespondenz}
\\
Neben der binären Behauptung des Algorithmus ist auch die Bewertung der Korrespondenzqualität  wichtig, damit die gute und stabile Bilderkette für Markenverfolgung ausgewählt werden kann. Das wurde aber leider in der Arbeit von Scott und Higgnis nicht genannt. In der ersten Verbesserung wird die Größe der größten Elemente von Matrix $P$ mit einem Schwellwert weiter beschränkt, um ein besseres Korrespondenzergebnis zu erhalten. Deshalb kann zur Umkehr die Größe der größten Elemente von $P$ als die Messung der Korrespondenzqualität definiert werden. Dann wird die Summe aller größten Elemente $\sum P_{ij}$ in dieser Arbeit zur Bewertung der Korrespondenzuntersuchung verwendet. Hier wird kein arithmetisches Mittel benutzt, weil die Anzahl der betrachteten Punkte, die als Eingaben in den Algorithmus eingegeben werden, auch ein wichtiger Faktor der Bewertung der Korrespondenz sind.
\\
\\
\textbf{Die Auswahl von $\sigma$ mit Rückkopplung}
\\
Die Einheit des Abstandes $\sigma$ ist der wesentliche Parameter des Singulärwertzerlegungsverfahrens. Das ungeeignete $\sigma$ erzeugt dann großes Chaos in den Korrespondenzlösungen, was schon in \cite{SL91} mit Schaubildern verdeutlicht wird. Um die besten Lösungen zu finden, soll die Abstandseinheit so definiert werden, dass sie nicht kleiner als die durchschnittliche Distanz zwischen den Korrespondenzpunkten ist. Es gibt zwei Schwierigkeiten für die Bestimmung des $\sigma$. Erstens sind die korrespondierenden Punktpaare vor dem Durchlauf des Algorithmus noch nicht bekannt, weshalb die Abstandseinheit nicht direkt berechnet, sondern nur geschätzt werden kann. Zweitens ist die Auswahl einer festen Abstandseinheit schwierig und ineffizient, wenn sich die betrachteten Punkte oder Merkmale, wie z.B. in dieser Arbeit, uneingeschränkt bewegen können. Deshalb wird hier der Parameter $\sigma$ für jedes Bild mit der durchschnittlichen Distanz zwischen allen korrespondierenden Punktepaaren des vorherigen Bildes festgelegt. Wegen der festen Bildwiederholfrequenz der Eingabebilder ist im theoretischen Fall der durchschnittliche Abstand der Korrespondenzpunkte jedes Bildes gleich, wenn sich alle betrachteten Punkte gleichförmig bewegen. Aber für die schwach beschleunigenden Bewegungen der Merkmale kann das sich anpassende $\sigma$ trotzdem herausgefunden werden, und die Korrespondenzlösungen stabil erzeugen lassen.
\\
\\
Der verbesserte Algorithmus wird im Algorithmus~\ref{algSVD} gezeigt.

\begin{algorithm}                     
\caption{Korrespondenzuntersuchung($Punktpaare$ $Result$)}         
\label{algSVD}                          
\begin{algorithmic}
	%\State Korrespondierende Punktpaare: $Result$                    
    \State Eingabebilder von jeweils Zeitpunkt $t_{i-1}$ und $t_{i}$: $I,J$
    \State Aktuelle approximierte Abstandseinheit: $\sigma_{i}$
	\State Schwellwert für die Beschränkung der größten Elemente: $\epsilon$
	\State Messung der Korrespondenzqualität: $mess$ 
	\State Summe der Abstände der Korrespondenzpunkte: $sumDistance$
    \For{$i=1 \to m$, $j=1 \to n$}
    	\State $r_{ij} \gets Dis(I_i, J_j)$
    	\State $G_{ij} \gets exp(-\frac{r_{ij}^2}{\sigma_i^2})$
    \EndFor
    \State $T,U \gets$ Singulärwertzerlegung von G
    \State $E \gets m \times n$ Diagonalmatrix mit $E_{ii} = 1$
    \State $P \gets TEU$
    \State $minMN \gets Min(m, n)$
    \For{$i=1 \to minMN$}
    	\State $MaxSpalteIndex[i] \gets$ Index der Spalte des maximalen Elements an Reihe $i$.
    	%\State $MaxSpalteIndex_i \gets Max$ 
    \EndFor
    \For{$i=1 \to minMN$}
    	\If {$P_{iMaxSpalteIndex[i]}$ ist Maximum der Spalte $MaxSpalteIndex[i]$}
    		\If {$P_{iMaxSpalteIndex[i]} > \epsilon$}
    			\State $Result \gets$ Punktpaar($I_i, J_{MaxSpalteIndex[i]}$)
    			\State $mess \gets mess + P_{iMaxSpalteIndex[i]}$
    		\EndIf 
    	\EndIf
    \EndFor 
    \For {Alle Punktpaare $(I_i, J_j) \in Result$}
    	\State $sumDistance \gets sumDistance + DistanceOf(I_i,J_j)$
    \EndFor
    \State $\sigma_{i+1} \gets sumDistance / \| Result \|$
    \If {$\| Result \| < 3$ }
    	\State \Return False
    \Else
    	\State \Return True, $mess$
    \EndIf
\end{algorithmic}
\end{algorithm}

\subsection{Segmentierung}
\label{Seg}
Bis jetzt werden alle erkannten Marken eines Bildes als eine Menge der Punkte zusammen betrachtet, die dann segmentiert werden soll, um die Marken auf verschiedene Objekte zu verteilen. Die Grundidee ist, dass die Marken genau dann als die Merkmale eines gleichen Objektes erkannt werden,  wenn die Abstände zwischen diesen Marken viel kleiner sind als die Abstände von ihr zu den anderen erkannten Marken. Die Problemstellung der Segmentierung ist gleich wie ein Clusting-Problem, weshalb in dieser Arbeit der Clusting-Algorithmus DBSCAN für die Segmentierung benutzt wird. Die Beschreibung des Verfahrens wird im Abschnitt \ref{Dbscan} gezeigt und der genaue Durchlauf wird durch den Algorithmus~\ref{algDBSCAN} erklärt.	Eine Liste der Punktmengen wird von DBSCAN ausgegeben, und jedes der Elemente stimmt mit der Merkmalen eines Objektes überein. Da es zumindest drei  Punkte benötigt, um die Lage des räumlichen Körpers im dreidimensionalen Raum zu bestimmen, werden die Punktmengen von der Ausgabe der DBSCAN als Rauschen erkannt, die weniger als drei Punkte enthalten.

\section{Objektlernen}
\label{OL}
Nach der erfolgreichen Markenanalyse wird eine Menge der Marken aus den Eingabebildern herausgefunden. Weiterhin sind die Abhängigkeiten dieser Marken zwischen benachbarten Bildern auch bekannt. Durch die Segmentierung werden diese Marken danach für unterschiedliche Objekte weiter verteilt. Im diesen Abschnitt wird erklärt, wie ein charakteristischer Graph des Objektes mithilfe der erkannten Marken dargestellt werden kann. In dieser Arbeit wird im Objektlernen die vereinfachte Situation berücksichtigt, dass ein Objekt nur einmal in den Eingabebildern vorkommt. Deshalb wird nur die größte Punktmenge des Segmentierungsergebnisses für das Objektlernen ausgewählt, die als die Eingabe für die folgenden Arbeitsschritte genutzt wird. 

\subsection{Bestimmung der Orientierung}
\label{BdO}
Um ein Objekt zu lernen, muss dieses zuerst  mit Marken zu der Kamera gezeigt und langsam umdreht werden. Zwischen der Umdrehung werden dann die relativen Positionen der Marken an jeder Ebene bestimmt. Deshalb sollen die Lagen der bekannten Marken in diesem Ablauf rechtzeitig aktualisiert werden, was als ein Orientierungsproblem formuliert werden kann. In dieser Arbeit wird das Verfahren von \cite{H87} verwendet, um die Transformation der bekannten Marken zwischen nachfolgenden Bildern zu berechnen. Der genaue Durchlauf wird im Abschnitt \ref{QmE} aufgeführt. Zuerst sollen die Schwerpunkte der erkannten Marken herausgefunden werden (siehe Formel~\eqref{QuaSch}). Dann berechnet man die Vektoren, die von allen Marken zu ihren entsprechenden Schwerpunkten reichen. Als Drittes wird die Korrelationsmatrix $H$ durch Formel~\eqref{QuaH} mit diesen Vektoren bestimmt. Die Hilfsmatrix $N$ in der Formel~\eqref{QuaN} besteht aus den Elementen der Matrix $H$. Einer ihrer Eigenvektoren ist genau dann die Einheitsquaternion für die Rotation,  wenn dieser dem größten positiven Eigenwert entspricht. Die Rotations- bzw. Translationsmatrix unter kartesischem Koordinatensystem wird jeweils durch die Formel~\eqref{QuaR} und \eqref{QTran} berechnet.
\\
\\
Die Lage des Objekts in jedem Bild hängt nur von der Lage des Objekts in dem vorherigen Bild ab. Die kleinen Fehler der Orientierung zwischen den nachfolgenden Bildern werden in einer langen Bildsequenz akkumuliert, das häufig einen großen Unterschied zwischen der realen Lage und berechneten Lage des Objekts erzeugt. In dieser Arbeit wird der Kalman-Filter über die Translation des Schwerpunktes des Objekts benutzt, um die kumulative negative Wirkung jedes Schrittes zu vermindern. Die Grundlage des Kalman-Filters wird im Abschnitt~\ref{KF} aufgeführt. Die Übergangsmatrix des Schwerpunkts des Objekts im dreidimensionalen Raum kann definiert als:

\[
F = 
\begin{pmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{pmatrix},
\] 

was die Lage und Geschwindigkeit der Bewegung gleichzeitig betrachtet. Die Beobachtungsmatrix ist die Einheitsmatrix mit der Größe 3$\times$3, weil der Schwerpunkt des Objekts $\in R^{3\times 1}$ als Eingabe zur Korrekturphase eingegeben wird. OpenCV liefert eine komplette Implementierung des Kalman-Filters, was hier direkt verwendet wird.

\subsection{Markenanordnung}
\label{Ma}
Wie im Anfang des Abschnittes~\ref{BdO} beschrieben, soll während der Lernphase die Lage des betrachteten Objektes in jedem Bild bestimmt werden. Hierzu  benötigt man mindestens in zwei Bildern jeweils drei nicht kollineare Punkte des Objektes, damit die Transformation zwischen der Lage dieses Objektes in diesen zwei Bildern bestimmt werden kann. Je mehr die Punkte betrachtet werden, desto exaktere Transformationen können in der Theorie berechnet werden. Wegen der Auflösung der Kamera in dieser Arbeit werden aber die Größe der Marken bzw. die Abstände der Marken stark beschränkt. D.h. es ist unmöglich, beliebig viele Marken an dem Objekt anzubringen. Deshalb soll eine Strategie für die Anordnung der Marken entworfen werden, damit die beste Transformation mit möglichst wenigen Marken bestimmt werden kann.
\\
\\
Die Anordnungsstrategie wird in 5 Regeln zusammengefasst.
\begin{enumerate}
	\item Die Größen der Marken dürfen nicht zu klein sein.
	\item Die Abstände zweier Marken sollen nicht zu eng sein.
	\item Jedes Tripel der Marken soll nicht kollinear sein.
	\item Um die Grenzen zweier Ebenen des Objekts sollen mehr Marken als in Mitte angebracht werden.
	\item Der Strukturgraph jeder Ebene soll möglichst unsymmetrisch sein.
\end{enumerate}

Die erste und zweite Regel hängen direkt von der Auflösung der Kamera ab, was schon im Abschnitt~\ref{AueM} diskutiert wurde. Die dritte Regel garantiert, dass die Orientierung des Objekts immer durch drei beliebige Marken berechnet werden kann. Die in der ersten Regel  definierte minimale erkennbare Markengröße entspricht aber nur der Situation, in der  die Marken mit der Bildebene der Kamera parallel sind. Wenn sich ein Objekt umdreht, verändert sich der Winkel zwischen der aktuell beobachteten Ebene und der Bildebene. Deswegen werden die gleichen Marken aber mit weniger Pixel in der Kamera abgebildet. Die Größen der Marken auf dieser beobachteten Ebene erscheinen immer kleiner in dem Bild mit der Vergrößerung des Drehwinkels, bis sie komplett senkrecht zu der Bildebene liegen und die nächste Ebene vollständig unter der Kamera vorkommt. Je kleiner die Marken sind, desto schwieriger werden sie erkannt. Deshalb sollen mehr Marken im Bereich zwischen zwei benachbarten Ebenen angebracht werden, damit das Programm genug Marken für die Berechnung der Orientierung erhalten kann. Das ist genau was in der vierten Regel  erklärt wird. Die Asymmetrie von der fünften Regel   hält die Einzigartigkeit der Transformation zwischen zwei unterschiedlichen Lagen des Objekts, d.h. nur eine Lösung wird von dem Orientierungsverfahren bestimmt.


\subsection{Darstellung des Strukturgraphen}
\label{DdS}
Das Ziel des Lernens ist, dass ein Strukturgraph für jedes eingegebenes Objekt erzeugt wird, welcher als die Charakteristiken für die Wiedererkennung verwendet werden kann. Um das Ziel zu erreichen, sollen die Strukturgraphen stabil sein und genug charakteristische Information des Objektes enthalten. Die Stabilität bedeutet, dass die Strukturgraphen für ein Objekt bei öfterem Lernen gleich dargestellt werden sollen. Die Information, die zur Wiedererkennung notwendig ist, wird durch die Positionen der Marken bzw. die Abstände dazwischen erzeugt. Die Verfahren der Bestimmungen der charakteristischen Marken und Kanten werden in folgenden Abschnitten diskutiert.

\subsubsection{Bestimmung der stabilen Knoten}
\label{BdsK}
Nach der Markenerkennung werden viele Marken aus dem Bild erkannt, die aber viel Rauschen enthalten. Das Rauschen wird von den falsch erkannten Merkmalen des Objektes bzw. der Umgebung oder den schlechten Korrespondenzpunkten verursacht. Deshalb benötigt das Programm eine Strategie für die Auswahl der gültigen Marken. In dieser Arbeit basiert die Auswahl auf Erscheinungshäufigkeiten, d.h. nur die Marken, die mehrmals kontinuierlich vorgekommen sind, werden zu den gültigen Marken gerechnet und in den Strukturgraphen eingefügt. Diese Marken werden als stabile Knoten in dem Strukturgraphen markiert, und dürfen nicht verändert werden. Abgesehen von der Beschränkung der Anzahl der Erscheinungen soll aber auch der Begriff von ,,Identität'' zweier Marken in unterschiedlichen Bildern definiert werden. Zwei Marken von verschiedenen Bildern sind genau dann identisch,  wenn der Abstand von einer Marke zu dem Punkt, der von anderer Marke nach Transformation erzeugt wird, kleiner als ein vorgegebener Schwellwert ist. Die Transformation umfasst die Rotation- bzw. Translationsmatrix, die mit dem Verfahren von Abschnitt~\ref{BdO} bestimmt werden.

\subsubsection{Kanteneinfügung}
Die ungerichteten Kanten des Strukturgraphen speichern die Abstände zwischen den Knoten des Graphen, welches die wichtigste Eigenschaft für die Wiedererkennung ist. Die Knoten, die gleichzeitig beobachtet werden können, werden in einem vollständigen Graphen mit den anderen verbunden. Wenn irgendwann ein neuer Knoten in den Graphen eingefügt wird, verbindet er mit allen vorhandenen Knoten des Graphen. Der analoge Durchlauf wird während der Entfernung eines ungültigen Knotens aus dem Graphen durchgeführt. Der Algorithmus~\ref{algAG} zeigt, wie der aktuelle Strukturgraph mit neu eingegebenen Punkten aktualisiert wird.

\begin{algorithm}                     
\caption{GraphUpdate($Punkte$ $P$, $Mat$ $R$, $Mat$ $T$)}         
\label{algAG}                          
\begin{algorithmic}
\State Schwellwert des Abstand: $\epsilon$
\State Minimale Lebenszeit des stabilen Knoten: $minT$
\State Aktueller Graph: $G$
\State $G_{temp} \gets createCompleteGraph(P)$
\For {jeder Knoten $v_i \in G$}
	\State $v_i \gets R v_i + T$
\EndFor
\For {jeder Knoten $v_i \in G$}
	\For {jeder Knoten $v_{temp_j} \in G_{temp}$}
		\If {$DistanceOf(v_i, v_{temp_j}) < \epsilon$}
			\State $v_i.Lifetime \gets v_i.Lifetime+2$
			\If {$v_i.Lifetime > minT$}
				\State $v_i.isFixed \gets True$
			\EndIf
			\State Verbinden aller mit $v_{temp_j}$ verbundenen Knoten in $G_{temp}$ mit $v_i$
			\State Entfernen $v_{temp_j}$ aus $G_{temp}$
		\EndIf
	\EndFor
	\If {Kein entsprechender Knoten für $v_i$ aus $G_{temp}$ gefunden wird}
		\State $v_i.Lifetime \gets v_i.Lifetime-1$
		\If {$v_i.Lifetime<0$ und $!v_i.isFixed$}
			\State Entfernen $v_i$ aus $G$
		\EndIf
	\EndIf
\EndFor
\State Einfügen aller übrigen Knoten von $G_{temp}$ in $G$
\end{algorithmic}
\end{algorithm}

\section{Zugriff des Strukturgraphen von Datei}
Das Ergebnis des Objektlernens ist ein Strukturgraph, der die Charakteristiken des betrachteten Objektes beschreibt. Die Knoten des Graphen werden als dreidimensionale Punkte mit Gleitkommazahl definiert. Jeder Knoten enthält eine Map, in der seine Nachbarn und die Kantenlänge dazwischen paarweise gespeichert werden. Die Strukturgraphen sollen auf der Festplatte gespeichert und eingelesen werden können. In dieser Arbeit wird die Dateiform von VTK (\cite{VTK}) benutzt, welche als eine Open-Source-C++-Klassenbibliothek für die 3D-Computergraphik und wissenschaftliche Visualisierung häufig verwendet wird. Der Vorteil der Verwendung der VTK Datei ist, dass die Ergebnisgraphen einfach weiter von dem anderen Framework bzw. der Software verwendbar sind. Die Abbildung~\ref{PV} ist der Screenshot von der Visualisierung eines Strukturgraphen mit der Software ParaView \cite{PV}. Vor dem Speichern des Graphen werden alle Knoten noch einmal überprüft. Die ähnlichen Knoten sollen in einem einzigen Knoten kombiniert werden, damit nur ein vereinfachter Graph ohne verdoppelten Knoten gespeichert wird. Die VTK-Dateiform liefert viele Stichwörter für Beschreibung der unterschiedlichen grundsätzlichen, geometrischen Elemente, wie z.B. die Punkte, die Gerade und die Ebene, usw. Deshalb werden die Knoten und die Kanten des Strukturgraphen jeweils mit dem Stichwort ,,POINTS '' und ,,LINES'' in der VTK-Datei geschrieben. Diese Stichwörter sind auch die Kennzeichen beim Lesen der VTK-Datei. Die Punkte werden zuerst in einen neuen Graphen eingefügt, und dann mit anderen Knoten durch die gespeicherten Gerade verbunden.

\begin{figure}
\centering
\includegraphics[scale=0.3]{Abbildungen/ParaView.png}
\caption{Visualisierung eines Strukturgraphen eines Kästchens mit ParaView. Das Foto des Kästchens ist in Abbildung~\ref{BOXES} dargestellt.}
\label{PV}
\end{figure}

\section{Objekterkennung und Verfolgung}
Die Strukturgraphen aller vom Programm erlernten Objekte sollen am Anfang der Wiedererkennungsphase von den auf der Festplatte gespeicherten VTK-Dateien wieder in das Programm eingelesen werden. Danach wird die Markenanalyse über die Eingabebilder genau wie in der Lernphase durchgeführt, und am Ende wird eine Liste von der Punktmenge erzeugt. Das Programm versucht dann, die Teilgraphen dieser Strukturgraphen aus den von den Punktmengen neu dargestellten Graphen zu finden, was im sogenannten Teilgraph-Isomorphismus-Problem zusammengefasst wird. Für ein Bild werden alle möglichen Kombinationen der Strukturgraphen und Punktmengen dieses Bildes durchgesucht. Die Objekte, deren Teilgraphen aus dem Eingabebild gefunden wurden, werden als erkannt angenommen. Die entsprechenden Punktmengen im Eingabebild beschreiben nun die aktuellen Lagen der Objekte. Mit dem gleichen Ablauf können die Objekte in einem Bilderstrom kontinuierlich erkannt werden, was aber nicht möglich ist, ist die kontinuierliche Festlegung der entsprechenden Punktmengen, weil die Marken von jedem Bild neu segmentiert werden und keine Verbindungen zwischen den ,,gleichen'' Punktmengen von unterschiedlichen Bildern existieren. Ein direktes Lösungsverfahren besteht darin, dass jede Punktmenge aus der Segmentierung sofort als ein neues Objekt erkannt wird. Der Vergleich des Teilgraph-Isomorphismus-Problems findet dann zwischen den Strukturgraphen zweier Objekte statt. Dadurch sollen viele Lernprozesse während der Wiedererkennungsphase gleichzeitig durchgeführt werden. Das kostet aber zu viel Zeit, und ist schwierig zu implementieren. Die alternative Lösung liegt in der Darstellung der sogenannten Kandidaten. Diese Kandidaten interessieren nur die Abhängigkeiten der gleichen Punktmengen von unterschiedlichen Bildern, enthalten aber keine stabilen Knoten wie den Strukturgraphen. Die Abbildung~\ref{erAblauf} zeigt ein Beispiel über die gesamte Erkennungsphase.

\subsection{Kandidaten der Objekterkennung}
Wie im Abschnitt~\ref{Seg} beschrieben, werden viele Punktmengen nach der Segmentierung erzeugt, welche mit unterschiedlichen Objekten übereinstimmen können. Die Kandidaten bestehen aus diesen Punktmengen und werden in einer Liste im Speicher gespeichert. Wenn das Programm ein neues Eingabebild einliest, werden alle neuen von der Segmentierung erhaltenen Punktmengen mit vorhandenen Kandidaten verglichen. Der Kandidat, der mit einer der neuen Punktmengen assoziiert, wird dann aktualisiert. Die übrigen Punktmengen, die keine entsprechenden Kandidaten finden können, werden als neuen Kandidaten in der Liste eingefügt. Um die Leistungsfähigkeit zu halten, sollen die Kandidaten, die lange Zeit nicht aktualisiert wurden, aus der Liste entfernt werden. Die Erkennung wird dann für jeden neu aktualisierten Kandidaten durchgeführt. Der Index des am besten entsprechenden Objektes wird im Kandidat gespeichert. In dem Block ,,Behandlung der Kandidaten'' von Abbildung~\ref{erAblauf} wird ein Beispielablauf über die Aktualisierung bzw. Darstellung der Kandidaten aufgeführt. Die neu erzeugten Kandidaten werden in dem Ende der Kandidatenliste eingefügt. Der zweite und dritte Kandidat hat in dem Beispiel keine entsprechenden Punktmengen und wird nach der Aktualisierung aus der Liste entfernt. Nach der Behandlung der Kandidaten sollen nur drei Kandidaten im Teilgraph-Isomorphismus mit dem Eingabemodel verglichen werden.

\begin{figure}
\centering
\includegraphics[scale=0.7]{Abbildungen/Kandidaten.png}
\caption{Ausführliches Ablaufdiagramm der Objekterkennung. Die Kandidaten, die schraffiert markiert sind (K2, K3), werden nach der Aktualisierung aus der Liste entfernt.}
\label{erAblauf}
\end{figure}

\subsection{Objekterkennung}
\label{Oerk}
Die Objekterkennung basiert auf dem Vergleich zwischen den Strukturgraphen der erlernten Objekte und den Kandidaten, die von den neu segmentierten Punktmengen erzeugt werden. Ein neuer Graph wird zuerst von den Punkten eines Kandidaten dargestellt. Der Vergleich zwischen diesen Kandidaten und den vorhandenen Strukturgraphen  kann dann zum Teilgraph-Isomorphismus Problem abgleitet werden. Das Lösungsverfahren übernimmt die Idee von Rhijn und Mulder \cite{AJ05}, die bereits im Abschnitt \ref{TI} erklärt wurde. Es werden zuerst die isomorphen Knoten zwischen zwei Graphen herausgefunden, welche jeweils genug isomorphe Nachbarn haben, die in gleichen Abständen zu den isomorphen Knoten liegen. Ein Beispiel der isomorphen Knoten wird in Abbildung~\ref{IK} gezeigt. Der Teilgraph-Isomorphismus zwischen diesen Graphen existiert genau dann, wenn mindestens ein gefundener isomorpher Knoten und seine Nachbarn zusammen die gleichen Pyramiden darstellen können. In dieser Arbeit wird das Verfahren von Rhijn und Mulder in zwei Bereichen verbessert: Die effizientere Nebenbedingung für das Suchen der isomorphen Knoten und die vereinfachte Endbedingung anstatt der Feststellung der Pyramide.

\begin{figure}[hftb]
\centering
\includegraphics[scale=0.47]{Abbildungen/IsomorphKnoten.png}
\caption{Ein Beispiel der isomorphen Knoten. $P$ und $P'$ sind zwei isomorphen Knoten, die jeweils vier Nachbarn mit gleicher Abständen haben. ($(P,V_1) = (P',V'_1)$, $(P,V_2) = (P',V'_2)$, $(P,V_3) = (P',V'_3)$, $(P,V_4) = (P',V'_4)$)}
\label{IK}
\end{figure}

\textbf{Nebenbedingung für das Suchen der isomorphen Knoten}
\\
Um die isomorphen Knoten schneller zu finden, werden zwei Quoten für das Suchen definiert. Die Abstandsquote wird anstelle des absoluten Schwellwertes benutzt, damit der Vergleich der Distanzen zwischen dem betrachteten Knoten und seiner Nachbarn bewertet werden kann. Offensichtlich wird die Kante zwischen zwei Marken auf dem Objekt zu mehr Bildpunkten abgebildet, wenn sich das Objekt zur Kamera bewegt. Je größer die Auflösung des Objektes ist, desto besser ist das Erkennungsergebnis. Hieraus folgt, dass die absolute Kantenlänge von der Distanz zwischen dem Objekt und der Kamera abhängt. Analog dazu liefert der gleiche absolute Schwellwert für verschiedene Abstände zur Kamera aber unterschiedliche Vergleichsgenauigkeiten. Die Verwendung der Abstandsquote kann die Wirkung der Entfernung der Kamera gut vermeiden.
\\
\\
Die zweite Nachbarquote beschränkt die minimale Anzahl der benötigen, assoziierten Nachbarn zur Bestimmung eines isomorphen Knotens, was im Verfahren von Rhijn und Mulder aber als Invariante definiert wird. Die Beziehung zwischen der Nachbarquote und der minimalen Anzahl der Nachbarn kann in

\begin{equation}
N_{min} = \left\{
\begin{array}{l l}
3 & \text{falls } \| V \| * Nachbarquote<3 \\
\| V \| * Nachbarquote & sonst \\
\end{array}
\right.
\end{equation}

formuliert werden, wobei $V$ die Knotenmenge des Graphen beschreibt. Die minimale Anzahl der Nachbarn darf nicht kleiner als drei sein, weil es mindesten drei Nachbarn benötigt, um einen einzigen Knoten eines Graphen durch den Vergleich der Kantenlänge seiner Nachbarn im dreidimensionalen Raum zu bestimmen. Diese Veränderung liefert eine bessere Toleranz für die Erkennung. Wenn die Graphen nur mit weniger Knoten als die Eingaben des Verfahrens angegeben werden, garantiert das Verfahren aber auch genug Nachbarn für die Auswahl der isomorphen Knoten. Außerdem können die Genauigkeit und die Stabilität des Teilgraph-Isomorphismus mit dem Vergrößern der Anzahl der Knoten gleichzeitig ansteigen.
\\
\\
\textbf{Vereinfachung der Endbedingung}
\\
Im Verfahren von Rhijn und Mulder sollen schließlich die Pyramiden für den isomorphen Knoten und seine Nachbarn gefunden werden, damit der Isomorphismus bestimmt werden kann. Die Pyramide ist ein vollständiger Graph mit vier Knoten. Deshalb sollen für einen Knoten, der $n$ Nachbarn hat, höchstens 

\[
C_n^3 = \binom{n}{3} = \frac{n!}{k!(n-k)!} = \frac{1}{6} n(n-1)(n-2) \approx \mathcal{O}(\frac{1}{6}(n^3-3n^2))
\]

verschiedenen Kombinationen seiner Nachbarn betrachtet werden, um die Pyramide zu finden. Seien $m$ die Anzahl der gefundenen isomorphen Knoten und der Zeitaufwand von der Überprüfung der Nachbarschaft der Zeitkomplexität $\mathcal{O}(1)$, dann ist der Zeitaufwand der Suche der Pyramide im schlechtesten Fall $\mathcal{O}(\frac{1}{6} m (n^3-n^2))$. Offensichtlich gilt es $m < n$ wegen der Definition des isomorphen Knoten. Die Idee der Vereinfachung ist, anstatt der Nachbarn eines isomorphen Knoten die isomorphen Knoten selbst zu betrachten, um den obigen schlechtesten Fall zu vermeiden. Die Bedingung für die Pyramide kann auch geschwächt werden, d.h. man kann die einfacheren geometrischen Elemente benutzen, wie z.B. das Dreieck sogar nur zwei verbundenen Kanten. $m$ isomorphe Knoten werden durchgesucht, um zu bestimmen, ob sie sich zumindest mit zwei anderen isomorphen Knoten verbinden. Der Zeitaufwand dieses Ablaufes ist nur $\mathcal{O}(m^2)$, was deutlich kleiner als das Verfahren mit Suche der Pyramide ist. Abbildung~\ref{AI} zeigt ein Beispiel für den Vergleich der beiden Nebenbedingung des Isomorphen-Problems. Auf der linken Seite versucht das Programm, eine Pyramide an den isomorphen Knoten zu finden. Alle Kombinationen der drei Nachbarn werden für jeden isomorphen Knoten betrachtet. Dadurch ist der Zeitaufwand der linken Seite:

\[
\binom{3}{3}(V_1) + \binom{3}{3}(V_2) + \binom{4}{3}(V_3) + \binom{6}{3}(V_4) + \binom{5}{3}(V_5) = 1 + 1 + 4 + 20 + 10 = 36.
\]

An der rechten Seite werden aber nur die isomorphen Knoten selbst betrachtet, wobei $m^2=5^2$ Zeitaufwand kostet. Alle isomorphen Knoten, die diese Bedingung erfüllen, werden dann gespeichert, um die Orientierung des erkannten Objektes zu berechnen. 

\begin{figure}
\centering
\includegraphics[scale=0.5]{Abbildungen/AlgoIsomorphismus.png}
\caption{Ein Beispiel für den Vergleich der zwei Endbedingungen. $V_1$ bis $V_4$ sind die isomorphen Knoten und $N_1$ bis $N_6$ sind die Nachbarn von Knoten $V_4$. Auf der linken Seite wird das Verfahren mit der Pyramide gezeigt, wobei alle isomorphen Knoten durchgesucht werden, bis eine Pyramide $V_4 N_5 N_6 N_1$ gefunden ist. Auf der rechten Seite werden aber nur die Kanten zwischen den isomorphen Knoten betrachtet.}
\label{AI}
\end{figure}

\subsection{Bestimmung der Orientierung}
Die Verfolgung des Objekts ist die andere wichtige Aufgabe dieser Arbeit, was durch die Berechnung der Transformation realisiert wird. Die Anfangslage eines Objekts wird von dem Strukturgraphen definiert. In jedem Bild wird die Rotations- bzw. die Translationsmatrix zwischen dem Anfangs- und aktuellen Zustand mithilfe der ausgewählten isomorphen Knoten bestimmt. Das Orientierungsverfahren verläuft in gleicher Weise wie das Verfahren im Lernprozess, der im Abschnitt~\ref{BdO} erklärt wurde.

\section{Bildersteuerung}
\label{BildS}
Das Speichern und die Auswahl der Bilder sind die Hauptaufgaben des Teilprogramms der Bildersteuerung. Die Verfolgung der Marken wird durch den Vergleich der Marken von zwei nachfolgenden Bildern realisiert, d.h. das Programm soll immer zwei Bilder gleichzeitig betrachten: Das aktuelle- und das historische Bild. Außerdem werden die Marken wegen dem Rauschen und anderen Störungen aus den Eingabebildern teilweise schlecht erkannt. Die falschen Marken verstören die Korrespondenzuntersuchung und verschlechtern weiterhin die Orientierung der Objektlagen zwischen zwei Bildern, was aber ganz wichtig für das Lernen des Objekts ist. Wenn irgendwelche kleinen Fehler in der Transformation vorkommen, wird der Strukturgraph des lernenden Objekts komplett anders dargestellt. Deshalb sollen die schlechten Eingabebilder vor der Darstellung des Strukturgraphen ausgewählt und von dem Bilderstrom entfernt werden.
\\
\\
In dieser Arbeit werden alle Eingabebilder zuerst in einer Warteschlange gespeichert. Die Warteschlange wird mit fester Größe definiert. Wenn ein neues Bild eingegeben wird, wird es am Ende der Schlange eingefügt und das älteste Bild aus der ersten Stelle der Schlange gestrichen. In jedem Zyklus des Programms werden das erste und letzte Bild verglichen und die korrespondierenden Punktpaare daraus gefunden. Mit anderen Worten definiert die Größe der Warteschlange aber auch das Intervall der betrachteten Bilder. Die Markenanalyse und die Orientierung von Objektlernen werden regelmäßig für jedes Eingabebild durchgeführt, aber die davon erhaltenen Rotations- bzw. Translationsmatrix werden nicht direkt für die Aktualisierung des Strukturgraphen benutzt, sondern zuerst geprüft, damit die Qualität der Transformation bewerten werden kann. 
\\
\\
Das Prüfungsprogramm besteht aus zwei Teilen. Der erste Teil ist ein Prüfer, der eine boolesche Aussage liefert, ob aus dem Bild genug hochqualitative Marken gefunden werden können. Der Prüfer läuft analog wie die Aktualisierung der Knoten von dem Teilprogramm des Strukturgraphen, was im Abschnitt~\ref{BdsK} beschrieben wurde. Der Unterschied liegt darin, dass der Prüfer die Lebenszeit der Marken nicht betrachtet, sondern nur die neu gefundenen Marken zählt, die mit den vorhandenen Knoten des Strukturgraphen nach gerade berechneter Transformation identisch sind. Nur das Bild mit genug Marken, die die obige Bedingung erfüllen, wird von dem Prüfer akzeptiert. Die Beurteilung über die Anzahl der Marken wird durch den Vergleich der Quote, die den Anteil der mit vorhandenen Knoten übereinstimmenden Marken an allen neuen Marken beschreibt, mit einem vordefinierten Schwellwert realisiert. Die vom Prüfer akzeptierten Bilder können direkt für die Aktualisierung der Strukturgraphen genutzt werden. Die anderen Bilder sind zwar überflüssig, können aber nicht direkt verworfen werden. Zu einer schlechten Beobachtungssituation liefert die Kamera möglicherweise in langer Zeit gar keine hochqualitativen Bilder. Wenn das Programm all diese Bilder überspringt, wird die Verfolgung des Objekts abgebrochen und falsche Erkennungsergebnisse erzeugt. Ein extremes Beispiel ist bei der Drehung des Kästchens gegeben. Wenn nur die Bilder von zwei benachbarten Ebenen des Objekts von dem Programm akzeptiert, aber die Bilder des Rotationsablaufs dazwischen nicht berücksichtigt werden, hat das Programm aber gar keine Möglichkeit, diese zwei Ebenen zu unterscheiden. Sie werden als eine große Ebene erkannt und gespeichert. Der zweite Teil des Prüfungsprogramms vermeidet diese Situation. Die maximale Anzahl der übersprungenen Bilder wird beschränkt. Wenn kein hochqualitatives Bild gefunden werden kann, wird ein verhältnismäßig besseres Bild aus den schlechten Bildern ausgewählt. Die Summe aller größten Elemente von $P$, die im Abschnitt \textbf{Qualitätsüberprüfung der Markenverfolgung} von \ref{MV} erklärt wurde, wird hier als die Bewertungsvariable verwendet. Der Durchlauf des Prüfungsprogramms wird im Algorithmus~\ref{algBP} aufgeführt.

\begin{algorithm}                     
\caption{Bilderprüfer($Strukturgraph$, $Eingabebild$, $UebersprungeneAnzahl$)}         
\label{algBP}                          
\begin{algorithmic}
\State die minimale Quote der mit vorhandenen Knoten übereinstimmten Marken: $q$
\State die maximale Anzahl der Bilder, die übersprungen werden dürfen: $N_{jump}$
\State Iterator des Besten Bild in $Bildschlange$: $i_b$
\If {$Korrespondenzuntersuchung()$ == True und $Identische Quote>q$}
	\State Aktualisieren $Strukturgraph$ mit $Eingabebild$
\Else
	\If {$Korrespondenzuntersuchung()$ == False}
		\State Überspringen
	\EndIf
	\If {$Eingabebild.SumP > Bildschlange[i_b].SumP$}
		\State Entfernen $Bildschlange[i_b]$
		\State $i_b \gets Eingabebild.iterator$
	\Else
		\State Überspringen
	\EndIf
	\State $UebersprungeneAnzahl$ ++
	\If {$UebersprungeneAnzahl > N_{jump}$}
		\State Aktualisieren $Strukturgraph$ mit $Bildschlange[i_b]$
		\State $UebersprungeneAnzahl \gets 0$
	\EndIf
\EndIf
\end{algorithmic}
\end{algorithm}