\chapter{Grundlagen}

\section{Generierung der 3D-Daten}

\subsection{TOF-Sensor}

\subsubsection{TOF Kamera}
\label{TOF Kamera}
Die im MAROCO-System verwendete Kamera gehört zur Klasse der TOF-Sensoren, die außer den normalen Graufarbenbildern auch Tiefbilder liefern kann. Die Tiefmessung basiert auf dem sogenannten Laufzeitverfahren. Dazu wird die Szene durch ein Lichtpuls ausgeleuchtet und für jeden Bildpunkt wird die Zeit gemessen, die das Licht bis zum Objekt und wieder zurück benötigt. Die Distanz ist direkt proportional zu dieser  Zeit und kann durch die folgende Formel berechnet werden:

\begin{equation}
\label{tof1}
d = \frac{t_d}{2c}
\end{equation} 

wobei $t_d$ die gemessene Zeit bezeichnet. Die Konstante $c$ steht für die Lichtgeschwindigkeit. 
\\
\\
Im Vergleich zu anderen 3D-Kamerasystemen hat die TOF (englisch: time of flight) Kamera viele Vorteile \cite{TOFWiki}. Zuerst kann die TOF Kamera  die interessierenden Bereiche einfach aus einem Bild extrahieren und nur die Pixel nah vor der Kamera betrachten. Zweitens kann die TOF Kamera eine hohe Bildrate bis zu 100 fps erreichen. Diese Eigenschaft ermöglicht somit eine Echtzeitanwendungen. Außerdem benötigt die TOF Kamera weniger Platz im Vergleich zum  Triangulationssystem, und hat eine  niedrigere Abhängigkeit von der Systemstruktur gegenüber dem Stereosystem.


\subsubsection{PMD Sensor}
Der PMD (englisch: Photonic Mixing Device) Sensor ist eine wichtige Art der TOF Kamera. Er liefert eine hohe Auflösung bis zu 204x204 Pixel und einer maximalen Bildrate bis zu 25 fps. Durch die Formel \eqref{tof1} wird der maximale Distanzbereich auf 7 m festgelegt. Die anderen wichtigen Parameter findet man in Abbildung~\ref{PMDParam} \cite{pmde}.
\\
\\
%\graphicspath{}
\begin{figure}[ftb]
\centering
\includegraphics[scale=0.6]{Abbildungen/PMDParam2}
\caption{Die grundlegende Parameter der PMD CamCube2. \cite{pmde}}
\label{PMDParam}
\end{figure}

Das Hintergrundlicht, z.B. das Sonnenlicht, könnte die Messung der Distanz stark stören. Die PMD Kamera benutzt das aktive Sendersignal und einen Fremdlicht-Filter (SBI), um das Hintergrundsignal zu unterdrücken. Außerdem bietet die PMD Kamera die Möglichkeit, die Integrationszeit der Kamera für jede Messung individuell einzustellen. Die Integrationszeit bezieht sich auf die Zeitspanne, in der die Kamera zur Aufzeichnung eines Bildes dem reflektierten Licht ausgesetzt wird. Für ein schwach reflektierendes Objekt benötigt der Sensor eine längere Integrationszeit als ein stark reflektierendes Objekt, um genug Information anzusammeln. Andererseits wird aber ausreichendes Licht von hellen Objekten auf den Sensor reflektiert, wenn die Integrationszeit zu lang definiert wird. In Abbildung~\ref{PMDIntTime} wird ein Beispiel der Tiefbilder mit verschiedenen Integrationszeiten gezeigt \cite{pmdd}. 

\begin{figure}[ftb]
\centering
\includegraphics[scale=0.6]{Abbildungen/PMDIntTime}
\caption{Integrationszeit von 140 $\mu s$, 1400 $\mu s$ bzw. 14000 $\mu s$. Bitte beachten Sie die niedrige Signalstärke an der linken Seite und die Sättigung an der rechten Seite wegen der unangemessenen Integrationszeit. \cite{pmdd}}
\label{PMDIntTime}
\end{figure}

\subsubsection{Unterschied zwischen TOF Kamera und Kinect}
Kinect ist eine Hardware zur Steuerung der Videospielkonsole Xbox360, die ein sogenanntes hands-free Kontrollieren liefert, wodurch die Spieler mit einigen bestimmten Gesten oder einer kurzen Bewegung ihres Körpers das Spiel steuern können \cite{KIN}. Um dieses Ziel zu erreichen, sammelt Kinect abgesehen von der normalen Bildeingabe aber auch die Tiefdaten der Szene an. Wegen dieser Eigenschaft wird das Gerät im Bereich von Computer Vision benutzt. Mithilfe des SDK ist die Programmierung der Kinect unter normalen Betriebssystemen, wie z.B. Windows, Linux bzw. MacOS möglich.
\\
%\begin{figure}[bft]
%\centering
%\includegraphics[scale=0.6]{Abbildungen/Kinect}
%\caption{Das Arbeitsprinzip des Kinects. \cite{KINH}}
%\label{Kinect}
%\end{figure}
\\
Der Unterschied zwischen TOF Kamera und Kinect kann auf dem Arbeitsprinzip zurückgeführt werden. In TOF Kameras wird die Tiefdaten durch das Laufzeitverfahren berechnet, was im \ref{TOF Kamera} erklärt wird. Das Abtastverfahren der Kinect heißt ,,Light Coding''. Eine große Menge von Punkten wird als Mustern auf die Szene bzw. die Objekte durch infrarotes Licht projiziert. Die ganze Szene mit diesen zusätzlichen Mustern wird von einer infraroten Kamera des Kinects aufgenommen. Durch die Verzerrung zwischen dem vordefinierten Muster im infraroten Licht und dem von der infraroten Kamera erkannten Muster kann das Tiefbild der Szene ausgerechnet werden. Weitere Informationen findet man im technischen Dokument des Firma Cadet \cite{KINH}. Der Vergleich über die genauen technischen Daten von PMD Kamera und Kinect wird in Tabelle~\ref{PMD and Kinect} zusammengefasst. Obwohl Kinect eine bessere Auflösung und größeres Sichtfeld hat, ist die PMD Kamera wegen ihrer hohen Bildrate und ihres großen Messbereichs für das MAROCO-System geeignet. Außerdem ist mithilfe des SBI Systems die Arbeit der PMD Kamera unter schwieriger Umgebungsbedingung, wie z.B. außerhalb des Zimmers mit starker Störung von Sonneneinstrahlung, auch möglich.

\begin{table}[ftb]
\begin{center}
\begin{tabular}{| l || c | c |}
\hline
Sensor & PMD CamCube & Kinect \\ \hline
Auflösung & 204$\times$204 Pixel & 640$\times$480 Pixel\\ \hline
Sichtfeld & $40^\circ \times 40^\circ$ & $57^\circ \times 43^\circ$ \\ \hline
Max Bildrate & 25 fps & 30 fps \\ \hline
Messbereich & 0.3 $\rightarrow$ 7.0 m & 1.2 $\rightarrow$ 3.5 m (mit Xbox Software) \\ \hline
Lange der Tiefdaten & 8 bit (unsigned char) & 11 bit \\ \hline
\end{tabular}
\caption{Die technische Daten von PMD Kamera und Kinect. \cite{pmde}}
\label{PMD and Kinect}
\end{center}
\end{table}

\subsection{Daten der PMD Kamera}
\label{PMDData}
Die PMD Kamera CamCube kann insgesamt vier verschiedene Vermessungsdaten ausgeben. Hierzu zählen Amplitude, Intensität, Distanz und 3D-Koordinaten. Die ersten und letzten beiden Datentypen können durch die Dimension in zwei Gruppen unterteilt werden.

\subsubsection{2D Daten}
Die Intensität bezieht sich auf Graustufen. Nach einer Abbildung können diese Graustufen auf das Intervall zwischen 0 und 255 beschränkt werden. Das Ergebnis ist das Bild einer normalen Schwarz-Weiß Kamera. Die Amplitude zeigt die Stärke der Beleuchtung an, die vom Objekt wegen des aktiven Sendersignals von der PMD Kamera selbst reflektiert wird. Dieser Wert kann die Qualität der Distanzinformation abschätzen, d.h. das Objekt weit von Kamera liefert niedriger Amplitude als das Objekt in der Nähe von der Kamera, wenn sie mit identischem Material dargestellt werden. An der Gegenseite sind die Merkmale mit höherem Rückstrahlvermögen durch die Daten der Amplitude einfacher betrachtet, was für die Erkennung der großen künstlichen Marken in dieser Arbeit sinnvoll ist.

\subsubsection{3D Daten}
Die PMD Kamera liefert 3D-Daten in zwei Formen: die reine Distanzinformation und die 3D-Koordinaten. Die Distanzinformation ist die gemessene Distanz zwischen der Kamera und dem Objekt, welches direkt durch die Formel \eqref{tof1} berechnet wird. Bezüglich dieser Distanzinformation und der 2D-Daten der normalen Kamera werden die 3D-Koordinaten innerhalb der PMD Kamera berechnet und können durch die Schnittstelle abgefragt werden. Die Abbildung~\ref{C3D} zeigt jeweils die Visualisierung einer Szene mit der 3D-Koordinaten und Distanzinformation von PMD Kamera. Eine Transformation ist notwendig, wenn man die Distanzinformation direkt im kartesischen Koordinatensystem visualisieren möchte.

\subsubsection{Auflösung und erkennbare Markengröße}
\label{AueM}
Von weiterem Interesse ist, wie genau die Objekte von der Kamera in der Praxis beobachtet werden können, d.h. wie groß ein Pixel der Kamera ist, der den Bereich in realer Welt beschreibt. Die Vorderansicht des Kamerasystems dieser Arbeit ist in Abbildung~\ref{MSF} links gezeigt.

\begin{figure}[hbft]
\centering
\includegraphics[scale=0.44]{Abbildungen/MarkerSizeForme}
\caption{Die Vorderansicht (links) und die Draufsicht (rechts) des PMD Kamera Systems}
\label{MSF}
\end{figure}

$\theta$ ist der halbe Sichtwinkel und wird hier als $20^\circ$ angenommen. $H$ zeigt den Abstand von der Kamera zum Boden, der in diesem System mit 3,2m festgelegt ist. Durch Trigonometrie kann der Radius des Sichtbereiches auf dem Boden berechnet werden als:

\[
R = \tan \theta \cdot H =  0.36397 \cdot 3,2m = 1,16470m.
\]

Die halbe Seitenlänge des Sehnenquadrats $L$ (Siehe Abb~\ref{MSF} rechts) ist gleich:

\[
L = \cos 45^\circ \cdot R = 0,70711 \cdot 1,16470m = 0,82357m.
\]

Dadurch erhält man den Flächeninhalt des Sichtbereiches auf dem Boden:
\[
S = 4 \cdot L^2 = 2,7131 m^2.
\] 

Das Ergebnis der Division von dem Flächeninhalt und der Auflösung beschreibt die Größe der Zelle auf dem Boden, die in der Kamera als eigener Pixel dargestellt wird.

\[
S_z = S / (204 \times 204) = 2,7131 m^2 / 41616 = 6,5194 \times 10^{-5} m^2 = 0,65194 cm^2
\]

Die Marken, die kleiner als $S_z$ sind, werden in der Kamera kleiner als ein Pixel abgebildet und sind deshalb natürlich schwer zu erkennen. Daraus folgt, dass $S_z$ die minimale Größe der erkennbaren Marken definiert. Die minimale Seitenlänge kann nun berechnet werden durch:

\[
L_z = \sqrt{S_z} = 0,80743 cm
\]

Die minimalen Größen der Marken für andere, zum Boden parallelen Ebenen können analog berechnet werden. So kann z.B. in der normalen Arbeitsebene dieser Arbeit, die zum Boden $1,1m$ entfernt ist, die Mindestanzahl der Quadrate mit Seitenlänge von $0,52987cm$ erkannt werden.

\begin{figure}
\centering
\includegraphics[scale=0.45]{Abbildungen/Compare-3D-Dis}
\caption{Die Visualisierung von 3D Koordinaten (links) und Distanzinformation (rechts).}
\label{C3D}
\end{figure}

\section{Vorverarbeitung}
\subsection{Schwellwert-basierte Segmentierung}
\label{sbSeg}
Um bessere Erkennungsergebnisse zu erhalten, sollen die wesentlichen Bereiche von der Umgebung getrennt werden. In dieser Arbeit wird eine Schwellwert-basierte Segmentierung bezüglich der 3D-Daten verwendet. Eine Schwellwert-basierte Segmentierung kann als eine Abbildung $f$ vom originalen Bild $I$ zum Ergebnisbild $H$ definiert werden:

\[
f: I \xmapsto{} H
\]

mit 

\begin{equation}
H_{ij} = 
\begin{cases}
1 & \text{für } I_{ij} > \Theta \\
0 & \text{sonst}
\end{cases}
\end{equation}

wobei $\Theta$ der eingegebene Schwellwert ist.
 
\subsection{Steuerung der Helligkeit und Kontrast}
Außer der Umgebung beeinflussen die Helligkeit und der Kontrast die Qualität bzw. Stabilität der Markenerkennung. Deshalb ist die Optimierung dieser zwei Parameter in der Vorverarbeitungsphase notwendig. In dieser Arbeit wird ein affiner Operator auf jeden Punkt durchgeführt, um die geeignete Helligkeit bzw. den Kontrast zu bestimmen. Der affine Operator ist eine Abbildung von Originalbild $I$ zum Ergebnisbild $H$ mit:

\begin{equation}
H_{ij} = a I_{ij} + b,
\end{equation}

wobei die Parameter $a \in R^+$ und $b \in R$ jeweils den Kontrast und die Helligkeit kontrollieren. Es gibt vier Möglichkeiten für die verschiedenen Zuordnungen dieser Parameter:

\begin{itemize}
\item $a>1$, $b=0$     Kontrasterhöhung
\item $0<a<1$, $b=0$ Kontrastminderung
\item $a=1$, $b>0$     Helligkeitserhöhung
\item $a=1$, $b<0$     Helligkeitsminderung
\end{itemize}

\section{Markenerkennung}
Wie im Abschnitt \ref{mErkennung} beschrieben, sind viele Erkennungsalgorithmen in der Open Source Library OpenCV realisiert. Wegen verbreiteter Benutzung von OpenCV, werden der Vergleich dieser Algorithmen häufig gemacht. Im folgenden Abschnitt wird auf einen Vergleich eingegangen, um den geeigneten Algorithmus für diese Arbeit auszuwählen.
     
\subsection{Auswahl des Erkennungsalgorithmus}
\label{AusAlgo}

\begin{figure}[bft]
\centering
\includegraphics[scale=0.22]{Abbildungen/SampleBild}
\caption{Die vier Beispielbilder. Von links nach rechts sind Barbara, Lena, Peppers und Mandril. \cite{O11}}
\label{4Samp}
\end{figure}

\begin{figure}[bft]
\centering
\includegraphics[scale=0.7]{Abbildungen/Number-of-detected-features}
\caption{Die Anzahl der erkannten Merkmale von allen vier Beispielbildern durch verschiedene Erkennungsalgorithmen. \cite{O11}}
\label{Nodf}
\end{figure}

Odessa hat in seinem Blog einen sehr guten Vergleich für alle Erkennungsalgorithmen von OpenCV durchgeführt \cite{O11}. Vier häufig benutzte Beispielbilder wurden betrachtet (Siehe Abb.~\ref{4Samp}).
\\
\\
Von besonderem Interesse in seiner Arbeit ist der Vergleich über die Anzahl der betrachteten Punkte bzw. der durchschnittlichen Fehler. Wegen der verschiedenen Prinzipien erkennt der Algorithmus FAST viel mehr Merkmale als SURF und STAR(Name des CenSurE Algorithmus in OpenCV). Den Unterschied sieht man deutlich in der Abbildung~\ref{Nodf}. Je mehr Punkte betrachtet werden, desto mehr Rauschen wird in das System gebracht, weil viele normale Pixel auch als Merkmale erkannt werden. Das ist offensichtlich ein negativer Einfluss für die weitere Analyse der Daten. Abbildung~\ref{Ate} zeigt den durchschnittlichen Fehler in Pixeln von den Punktpaaren, der durch den gleichen Erkennungsalgorithmus von Bezugsbildern erkannt wird. Der Algorithmus STAR erzeugt deutlich weniger Fehler in der Erkennung, und das Ergebnis hängt auch leicht von der Eingabe ab. Wegen der niedrigeren Fehlerquote und der besseren Konzentration an großen kreisförmigen Merkmalen ist der STAR Algorithmus für die Erkennung der Objekte mit künstlichen Marken sehr geeignet.

\begin{figure}[bft]
\centering
\includegraphics[scale=0.7]{Abbildungen/Average-tracking-error}
\caption{Der durchschnittliche Fehler (in Pixeln) zwischen den assoziierten Punkten zweier folgender Bilder. \cite{O11}}
\label{Ate}
\end{figure}

\subsection{CenSurE Algorithmus}
Der Algorithmus CenSurE (Center Surround Extrema) wird von Agrawal et al. 2008 entwickelt \cite{AKB08}. Analog zum SIFT Algorithmus werden die Extreme zwischen den Skalenräumen herausgefunden, welche als die Merkmale des betrachteten Bilds erkannt werden. Der Bi-Level Filter wird in dieser Arbeit anstelle der Differenz der Gaussian verwendet, um die Laufzeit zu reduzieren. Weiterhin wird die Approximation des gefilterten Bereichs von Bi-Level Filter durchgeführt. Dadurch kann das Algorithmus in Echtzeit laufen lassen, obwohl alle Merkmale in allen Skalenräumen betrachtet werden sollen. 

%Sie verbessern die SIFT bzw. SURF Verfahren durch Berücksichtigung aller Merkmale in allen Skalenräume. Das Extremum durch die Skalen und Lagen werden ausgewählt, um die Merkmale zu bestimmen. Der Bi-level Filter wird hier statt Gaussian Filter verwendet, damit der Algorithmus in Echtzeit laufen kann, obwohl alle Merkmale in aller Skalenräume betrachtet werden sollen.

\subsubsection{Bi-level Filter}
Der Bi-level Filter ist eine einfache Approximation des Laplacian-Operators durch die Multiplikation der Bilder mit 1 und -1. Die Abbildung~\ref{bi-level} zeigt die Progression des Bi-level Filters mit verschiedenen Symmetrischen Stufen. Der kreisförmige Filter an linker Seite der Abbildung~\ref{bi-level} kann den Laplacian-Operator zwar am besten approximieren, ist aber leider schwierig zu implementieren. Deshalb werden die Progressionen der Filter durch Polygone angenähert, wodurch die Berechnung vereinfacht werden kann. Zum Vergleich der übrigen Formen der Abbildung~\ref{bi-level} liefert der Filter mit Achtecken die beste Leistung und der Filter mit Rechtecken die kürzeste Laufzeit. Diese Polygon-Filter können durch die integralen Bilder einfach dargestellt werden.  

\begin{figure}[bft]
\centering
\includegraphics[scale=0.6]{Abbildungen/Bi-level-filter2}
\caption{Progression der Center-Surround Bi-level Filter. Der Kreis ist die ideale voll-symmetrische Approximation der Laplacian. Die Filter daneben, von links nach rechts, haben eine niedrigere Symmetrie, brauchen aber weniger Zeit zur Berechnung. \cite{AKB08}}
\label{bi-level}
\end{figure}

\subsubsection{Integrale Bilder}
Ein integrales Bild $I$ ist eine mittlere Repräsentation eines Bildes, welches die Summe der Grauwerte von Bild $N$ mit Breite $x$ und Höhe $y$ enthält. 

\begin{equation}
I(x,y) = \sum _{x'=0}^x \sum_{y'=0}^y N(x',y')
\label{InteBild1}
\end{equation}   

Das integrale Bild ist rekursiv berechenbar und benötigt nur eine einmalige Durchführung aller Pixel des Bildes. Mithilfe des integralen Bildes kann die Intensität des beliebigen rechteckigen Bereiches einfach durch vier Additionen berechnet werden. Die Erweiterung des integralen Bildes wird für die Berechnung der Polygonen-Filter benutzt. Die Kombination von zwei verschiedenen Bildern, die schräg integriert werden, kann den für Polygone benötigten, trapezförmigen Bereich einfach darstellen. Die mathematische Beschreibung des schrägen Integrals des Bildes ist:

\begin{equation}
I_\alpha(x,y) = \sum_{y'=0}^y \sum_{x'=0}^{x+\alpha(y-y')} N(x',y'),
\label{InteBild2}
\end{equation}

wobei $\alpha$ den schrägen Winkel erklärt und wenn es 0 gleicht, ist die Formel \eqref{InteBild1} genauso wie die Formel \eqref{InteBild2}, und beschreibt ein rechteckiges integrales Bild. Die in der Abbildung~\ref{bi-level} gezeigte Achteck-Filter und Sechseck-Filter können mit jeweils drei bzw. zwei Trapezen schnell aufgebaut werden.
  
\subsubsection{Non-maximal Suppression}
Non-maximal Suppression ist eine Strategie, die das lokale Extremum finden kann. Die Response des Pixels wird unterdrückt, wenn es ein Pixel in seiner Nachbarschaft für die Lage bzw. die Skala gibt, dessen Response größer oder kleiner ist, als das zu betrachtende Pixel. Die Pixel mit entweder Maximum oder Minimum Response werden als Merkmale erkannt. Der Suchumfang wird in der Arbeit von Agrawal als 3x3x3 eingestellt, d.h. acht Pixel um den betrachteten Pixel und jeweils neun Pixel in zwei benachbarten Skalenräumen werden zusammen berücksichtigt. Die Pixel mit höherer Response können zwischen der Transformation des Bildes stabiler wiedererkannt werden, weshalb die ausgewählten Extrema nach der Non-maximal Suppression noch einmal durch einen Schwellwerte-Filter gefiltert werden sollen, um die besten Merkmale zu bestimmen. 
  
\subsection{Kalman-Filter}
\label{KF}
Der Kalman-Filter basiert auf einem linearen, dynamischen System in einem diskreten Zeitraum. Die Zustandsgleichung des Systems wird häufig durch eine Differenzengleichung beschrieben. In vielen Fällen werden die Zustände nur durch einen voneinander getrennten Zeitpunkt bestimmt. Kalman hat den Sonderfall der linearen Abhängigkeit der Zustände untereinander betrachtet, und die Zustandsgleichung zur linearen Differenzengleichung vereinfacht.

\subsubsection{Zustandsraummodellierung}
Der nächste Systemzustand kann basierend auf dem aktuellen Systemzustand durch:

\begin{equation}
X_k = F_{k-1} X_{k-1} + B_{k-1} u_{k-1} + w_{k-1} 
\end{equation}

geschätzt werden. Der Index $k$ bzw. $k-1$ bezieht sich auf den Zeitpunkt $t_{k}$ und $t_{k-1}$, wobei $t_k = t_0 + k \Delta t$ und $t_0$ der Anfangszeitpunkt und $k$ eine natürliche Zahl von Interesse ist. Deshalb beschreibt der mehrdimensionale Vektor $X_k$ den Zustand des Systems zum Zeitpunkt $t_k$. Die Matrix $F_{x-1}$ ist die Übergangsmatrix für die zeitlich aufeinanderfolgenden Zustände $X_k$ und $X_{k-1}$. Die Matrix $B_{k-1}$ und der Kontrollvektor $u_{k-1}$ stellen den deterministischen Anteil der weiteren, äußeren Einflüsse auf das System dar.  Die zufälligen, nicht erfassbaren Komponenten der äußeren Einflüsse werden durch die stochastische Größe $w_{k-1}$ geschätzt, die einer Normalverteilung mit Mittelwert 0 und Kovarianz $Q_{k-1}$ folgt.

\[
w_{k-1} \sim N(0,Q_{k-1})
\]

Wegen dieser Zufallsvariable bilden die Menge aller Zustandsvektoren eine Markov-Kette, d.h. der Zustand zu einem Zeitpunkt $k$ hängt lediglich vom unmittelbaren zeitlichen Vorgänger an $k-1$ ab.
\\
\\
Die Beobachtungen des Systems bestehen aus einer modellierbaren Verzerrung und einem unvorhersehbaren Messrauschen:

\begin{equation}
Z_k = H_k X_k + v_k.
\end{equation} 

$Z_k$ bezieht sich auf die Messung zum Zeitpunkt $k$. Die Multiplikation von der Beobachtungsmatrix $H_k$ und Zustandsvektor $X_k$ beschreibt die lineare Approximation der Verzerrung des Systems. Das Rauschen $v_k$ wird im Kalman-Filter als zeitlich unabhängig und normalverteilt angenommen:

\[
v_k \sim N(0,R_k).
\]

\subsubsection{Das Kalman-Filter}
Das Ziel eines Filters besteht darin, die Zustände durch die Informationen einer Messreihe besser schätzen zu können. Da die Rauschterme $w$ und $v$ für alle Zeit die Normalverteilung erfüllen, können die zeitdiskreten Zustände $X_k$ auch durch eine Normalverteilung mit dem Mittelwert $\hat{x}_k$ und der Kovarianz $\hat{P}_k$ ermessen werden.

\[
\hat{X}_k \sim N(\hat{x}_k , \hat{P}_k)
\]

Die Idee des Kalman-Filters ist es, eine rekursive Formulierung aufzubauen, die aber nur die Schätzung eines vorherigen Zeitpunktes und die aktuelle Messung benötigt, um die Schätzung des aktuellen Zeitpunktes zu bestimmen. Es gibt hauptsächlich zwei Phasen im Kalman-Filter.

\textbf{Prädiktion}
\\
In ersten Schritt dieser Phase wird eine vorangegangene Schätzung $X_{k|k-1}$ für den aktuellen Zeitpunkt vorausgesagt.

\begin{equation}
\hat{x}_{k|k-1} = F_{k-1} \hat{x}_{k-1} + B_{k-1} u_{k-1}
\end{equation}

Die Indizierungsschreibweise $k|k-1$ bezieht sich auf die Bedingtheit zu den Zeitpunkten $k$ und $k-1$. Für die Kovarianz gilt:

\begin{equation}
\hat{P}_{k|k-1} = F_{k-1} \hat{P}_{k-1} F_{k-1}^T + Q_{k-1}.
\end{equation}

\textbf{Korrektur}
\\
Die Vorhersagen von letztem Schritt werden hier durch die neue Messung korrigiert:

\begin{equation}
\hat{x}_k = \hat{x}_{k|k-1} + \hat{K}_k \tilde{y}_k,
\end{equation}
 
\begin{equation}
\hat{P}_k = \hat{P}_{k|k-1} - \hat{K}_k S_k \hat{K}_k^T.
\end{equation}

Die Hilfsgröße der Innovation $\tilde{y}_k$ beschreibt, wie genau die aktuellen Messungen von der vorhergesagten Schätzungen mithilfe der Beobachtungsgleichung approximiert werden:

\[
\tilde{y}_k = Z_k - H_k \hat{x}_{k|k-1}.
\]

$S_k$ bezieht sich auf die Residualkovarianz, wobei gilt:

\[
S_k = H_k \hat{P}_{k|k-1} H_k^T + R_k
\]

und $\hat{K}_k$ ist die zugehörige Kalman-Matrix:

\[
\hat{K}_k = \hat{P}_{k|k-1} H_k^T S_k^{-1}.
\]

\section{Markenverfolgung}
Die Marken, die vom Markendetektor erkannt werden, sollen während der Bildsequenz verfolgt werden. Damit kann die Transformation des betrachteten Objekts zwischen je zwei Bildern bestimmt werden, d.h. die gleichen Marken von verschiedenen Bildern sollen zuerst erkannt werden. In dieser Arbeit wird das Singulärwertzerlegungsverfahren verwendet, um diese Korrespondenzpunktpaare herauszufinden.

\subsection{Singulärwertzerlegung}
Sei M eine komplexe $m \times n$ Matrix mit Rang $r$. Die Singulärwertzerlegung bezeichnet dann das Produkt:

\begin{equation}
M = U \Sigma V^*
\end{equation}

wobei $U$ eine unitäre Matrix mit Größe $m \times m$ und $V^*$ die Adjungierte einer unitäre Matrix mit Größe $n \times n$ ist. $\Sigma$ bezieht sich auf eine $m \times n$ Diagonalmatrix:

\[
\Sigma = 
\begin{pmatrix}
\sigma_1 & & & \vline & & \vdots & \\
& \ddots & & \vline & \cdots & 0 & \cdots \\
& & \sigma_r & \vline & & \vdots & \\
\hline
& \vdots & & \vline & & \vdots & \\
\cdots & 0 & \cdots & \vline & \cdots & 0 & \cdots \\
& \vdots & & \vline & & \vdots & 
\end{pmatrix}
\]

mit $\sigma_1 \geq \cdots \geq \sigma_r > 0$, wobei $\sigma_i , i=1,\ldots , r$ als die Singulärwerte von $\Sigma$ genannt werden.

\subsection{Korrespondenzuntersuchung durch Singulärwertzerlegung}
\label{KdS}
Mithilfe der Singulärwertzerlegung haben Scott und Longuet-Higgins einen Algorithmus zur Bestimmung der assoziierenden Merkmale entwickelt \cite{SL91}. Seien $I$ und $J$ zwei nachfolgende Bilder und haben jeweils $m$ und $n$ Merkmale, die als $I_i (i=1,\ldots ,m)$ und $J_j (j=1,\ldots ,n$) bezeichnet werden, dann wird eine $m \times n$ Matrix $G$ mit den Elementen

\[
G_{ij} = exp(- \frac{r_{ij}^2}{2 \sigma^2})
\]

definiert, wobei $r_{ij}$ den Abstand zwischen den Merkmalen $I_i$ und $J_j$ beschreibt. $\sigma$ wird als ein Standard für den Abstand definiert, wodurch das Vergrößern oder Verkleinern der Verschiebung des Objekts geschätzt werden kann. Der Wert von $G_{ij}$ nimmt durch die Erhöhung der Distanz von 1 bis 0 monoton ab. Der zweite Schritt des Algorithmus von Scott und Longuet-Higgins ist die Singulärwertzerlegung der Matrix $G$:

\[
G = TDU.
\]

wobei $T$ und $U$ unitäre Matrizen mit den jeweiligen Größen $m \times m$ und $n \times n$ sind, und $D$ eine Diagonalmatrix bildet. Sei $E$ eine neue Matrix mit gleicher Größe von $D$, in der aber jedes diagonale Element als 1 ersetzt wird. Nach Austausch der Matrix $D$ durch Matrix $E$ erhält man eine neue orthogonale Matrix:

\[
P = TEU
\]

Die Aufgabe des dritten Schritts ist das Element $P_{ij}$ zu finden, welches gleichzeitig das Maximum der Reihe und Spalte ist. Wenn $P_{ij}$ diese Bedingung erfüllt, sagt man, dass es eine Eins zu Eins Korrespondenz zwischen den Merkmalen $I_i$ und $J_j$ gibt. Der ganze Algorithmus ist in Algorithmus~\ref{alg1} aufgeführt, der durch die Arbeit von Scott und Longuet-Higgins \cite{SL91} erzeugt wurde. 

\begin{algorithm}                      % enter the algorithm environment
\caption{Bestimmung der Korrespondenz der Merkmalen von zwei Bildern}          % give the algorithm a caption
\label{alg1}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}                    % enter the algorithmic environment
    \State $I,J,\sigma, Result$
    \For{$i=1 \to m$, $j=1 \to n$}
    	\State $r_{ij} \gets Dis(I_i, J_j)$
    	\State $G_{ij} \gets exp(-\frac{r_{ij}^2}{\sigma^2})$
    \EndFor
    \State $T,U \gets$ Singulärwertzerlegung von G
    \State $E \gets m \times n$ Diagonalmatrix mit $E_{ii} = 1$
    \State $P \gets TEU$
    \For{$i=1 \to m$}
    	\State $MaxSpalteIndex[i] \gets$ Index der Spalte des maximalen Elements an Reihe $i$.
    	%\State $MaxSpalteIndex_i \gets Max$ 
    \EndFor
    \For{$i=1 \to m$}
    	\If {$P_{iMaxSpalteIndex[i]}$ ist Maximum der Spalte $MaxSpalteIndex[i]$}
    		\State $Result \gets$ Punktpaar($I_i, J_{MaxSpalteIndex[i]}$) 
    	\EndIf
    \EndFor 
\end{algorithmic}
\end{algorithm}


\section{Bestimmung der Transformation}
Es gibt viele Möglichkeiten, die Lage eines Objekts im dreidimensionalen Raum zu schätzen. In dieser Arbeit wird das Verfahren von Horn \cite{H87} verwendet, das durch Einheitsquaternionen die Transformation eines Objekts zwischen zwei Zeitpunkten bestimmen kann.

\subsection{Quaternionen}
%Das Quaternion
Ein Quaternion besteht aus einem Vektor mit vier Elementen, wobei ein Element als ein Skalar bezeichnet und die anderen drei eine Richtung im dreidimensionalen Raum beschreiben. Quaternionen können aber auch als eine Erweiterung der komplexen Zahlen betrachtet werden, deren Imaginärteil nach drei neuen Zahlen $i$, $j$ und $k$ entwickelt wird. Eine Normalform einer Quaternion ist gegeben durch:

\[
q = q_0 + i q_x + j q_y + k q_z,
\]

wobei $i$, $j$ und $k$ die sogenannte Hamilton-Regeln erfüllen:

\[
i^2 = j^2 = k^2 = i j k = -1, 
\]
\[
ij = k, \quad jk = i, \quad ki = j, 
\]
\[
ji = -k, \quad kj = -i, \quad ik = -j.
\] 

Eine andere Form mit getrenntem Realteil und Imaginärteil wird definiert durch:

\begin{equation}
\label{QForm2}
q = (q_0, \vec{q}),
\end{equation}

wobei $q_0 \in \mathbb{R}$ ein Skalar und $\vec{q} \in \mathbb{R}^3$ ein Vektor ist. 
\\
\\
Sei $r$ eine andere Quaternion mit:

\[
r = r_0 + i r_x + j r_y + k r_z.
\]

Analog zu Vektoren im $\mathbb{R}^4$ wird das Skalar Produkt zwischen zwei Quaternionen definiert als:

\[
\langle q, r \rangle := q \cdot r := q_0 r_0 + q_x r_x + q_y r_y + q_z r_z.
\]

Weiterhin kann die Quaternion Multiplikation mithilfe von \eqref{QForm2} berechnet werden als:

\begin{align}
qr & = (q_0 r_0 - \vec{q} \cdot \vec{r} , \quad q_0 \vec{r} + \vec{q} r_0 + \vec{q} \times \vec{r}) \\
\label{QMulti}
& = (q_0 r_0 - q_x r_x - q_y r_y - q_z r_z) \nonumber \\
& + i ( q_0 r_x + q_x r_0 + q_y r_z - q_z r_y ) \nonumber \\
& + j ( q_0 r_y - q_x r_x + q_y r_0 + q_z r_z ) \nonumber \\
& + k ( q_0 r_z + q_x r_y - q_y r_x + q_z r_0 ).
\end{align} 

Die rechte Multiplikation von $r$ in Formel \eqref{QMulti} kann aber auch zu einer links multiplizierten Matrix umgeschrieben werden:

\begin{equation}
qr = 
\begin{pmatrix}
r_0 & -r_x & -r_y & -r_z \\
r_x & r_0 & r_z & -r_y \\
r_y & -r_z & r_0 & r_x \\
r_z & r_y & -r_x & r_0
\end{pmatrix}
 = \mathbf{R} q.
\end{equation}

Die konjugierte Quaternion von $q$ ist definiert als:

\[
\overline{q} = q_0 - i q_x - j q_y - k q_z.
\]

Das Produkt einer Quaternion und dessen Konjugierte ist eine nicht negative reelle Zahl:

\[
q \cdot \overline{q} = q_0^2 + q_x^2 + q_y^2 + q_z^2. 
\]

Mithilfe der konjugierten Quaternion kann man die Länge der Quaternion $|q|$ definieren:

\[
|q| = \sqrt{q \cdot \overline{q}}.
\]

Ist die Länge einer Quaternion gleich 1, nennt man die Quaternion eine Einheitsquaternion. Für eine Einheitsquaternion gilt:

\[
q \cdot \overline{q} = 1 \iff \overline{q} = q^{-1}.
\]

D.h. die Inverse und Konjugierte sind identisch. Für jede Einheitsquaternion $q \neq \pm 1$ gibt es eine entsprechende Polardarstellung:

\begin{equation}
q = \cos \alpha + \mathit{v} \cdot \sin \alpha
\label{polarQ}
\end{equation}

mit $\alpha = \arccos (q_0) \in (0,\pi)$ und $v = \frac{1}{\sin \alpha} (i q_x + j q_y + k q_z)$. 

\subsection{Beschreibung der Drehungen im Dreidimensionalen Raum mit Quaternionen}
Die Drehungen im dreidimensionalen Raum können durch die Einheitsquaternionen sehr gut beschrieben werden. Eine Abbildung der Rotation $\rho_q$ kann in folgender Form definiert werden:

\[
\rho_q : x \rightarrow q x \overline{q},
\] 

wobei $q$ eine Einheitsquaternion und $\overline{q}$ dessen Konjugierte ist. Mithilfe der Polardarstellung \eqref{polarQ} kann die Abbildung $\rho_q$ sich auf eine Drehung im $\mathbb{R}^3$ um die Achse $\mathit{v}$ mit Winkel $2\alpha \in (0, 2\pi)$ beziehen. Die entsprechende orthogonale Matrix von $q$ ist

\begin{equation}
\label{QuaR}
R =
\begin{pmatrix}
q_0^2 + q_x^2 - q_y^2 - q_z^2 & 2q_x q_y - 2q_0 q_z           & 2q_x q_z + 2q_0 q_y \\
2q_x q_y + 2q_0 q_z           & q_0^2 - q_x^2 + q_y^2 - q_z^2 & 2q_y q_z - 2q_0 q_z \\
2q_x q_z - 2q_0 q_y           & 2q_y q_z + 2q_0 q_z           & q_0^2 - q_x^2 - q_y^2 + q_z^2,
\end{pmatrix}
\end{equation} 

was zur Drehgruppe SO(3) gehört und eine Drehung in der Matrixform repräsentiert.

\subsection{Orientierung mit Einheitsquaternion}
\label{QmE}
Das Verfahren für die Orientierung der Objekte mithilfe der Quaternionen wurde von Horn im Jahre 1987 veröffentlicht \cite{H87}. Seien $D$ und $M$ zwei Punktmengen mit gleicher Größe $n$. Dann kann die Transformation zwischen den Punkten von zwei Mengen formuliert werden als:

\begin{equation}
d_i = \mathbf{R} m_i + \mathbf{T} + e_i,
\end{equation}

wobei $d_i$ und $m_i$ die i-ten Punkte der Punktmengen $D$ bzw. $M$ bezeichnen. $\mathbf{R}$ ist die Rotationsmatrix und $\mathbf{T}$ ist die Translationsmatrix. $e_i$ beschreibt den Fehler für die Transformation, und kann umformuliert werden als:

\begin{equation}
\label{QError}
e_i = d_i - \mathbf{R} m_i + \mathbf{T}.
\end{equation}

Das Ziel des Verfahrens ist, eine Rotations- bzw. Transformationsmatrix mit minimalem Fehler zu finden. Dadurch wird die Summe des Quadrats von $e_i$ betrachtet:

\begin{equation}
\sum_{i=1}^n \| e_i \|^2 = \sum_{i=1}^n \| d_i - \mathbf{R} m_i + \mathbf{T} \|^2. 
\end{equation} 

Seien $\overline{d}$ und $\overline{m}$ jeweils die Schwerpunkte der Punktmengen $D$ und $M$. Dann gilt

\begin{equation}
\label{QuaSch}
\overline{d} = \frac{1}{n} \sum_{i=1}^n d_i 
\quad , \quad
\overline{m} = \frac{1}{n} \sum_{i=1}^n m_i.
\end{equation}

Der Abstand von jedem Punkt zum Schwerpunkt wird berechnet als:

\begin{equation}
d_i' = d_i - \overline{d}
\quad , \quad
m_i' = m_i - \overline{m}.
\end{equation}

und die Summe der Abstände erfüllt natürlich

\begin{equation}
\label{QDis}
\sum_{i=1}^n d_i' = 0
\quad und \quad
\sum_{i=1}^n m_i' = 0.
\end{equation}

Dann kann der Fehler in Formel \eqref{QError} mit den Abständen zum Schwerpunkt $\overline{d}$ und $\overline{m}$ umgeschrieben werden:

\begin{equation}
e_i = d_i' - \mathbf{R} m_i' + \mathbf{T}',
\end{equation}

wobei $\mathbf{T}'$ als 

\[
\mathbf{T}' = \mathbf{T} - \overline{d} + \mathbf{R} \overline{m}
\]

definiert wird. Analog kann die Summe des Quadrats des Fehlers neu formuliert werden.

\begin{align}
\label{QError2}
\sum_{i=1}^n \| e_i \|^2 &=
\sum_{i=1}^n \| d_i' - \mathbf{R} m_i' + \mathbf{T}' \|^2 \nonumber \\
&= \sum_{i=1}^n \| d_i' - \mathbf{R} m_i' \|^2 - 2\mathbf{T}' \cdot \sum_{i=1}^n (d_i' - \mathbf{R} m_i')
+ n \| \mathbf{T}' \|^2
\end{align}

Wegen \eqref{QDis} ist der zweite Term gleich 0. Der dritte Term kann nicht negativ werden und wird auch 0 sein, wenn der gesamte Fehler minimiert wird, d.h.:

\begin{align}
\label{QTran}
& \mathbf{T}' = \mathbf{T} - \overline{d} + \mathbf{R} \overline{m} = 0 \nonumber \\
\Rightarrow & \mathbf{T} = \overline{d} + \mathbf{R} \overline{m}.
\end{align}

Die Formel \eqref{QTran} berechnet direkt die Translationsmatrix durch die Rotationsmatrix und die Schwerpunkte der beiden Punktmengen. Der erste Term von \eqref{QError2} kann zu

\begin{equation}
\sum_{i=1}^n \| d_i' - \mathbf{R} m_i' \|^2 = \sum_{i=1}^n (d_i'^t d_i' + m_i'^t m_i' - 2 d_i'^t \mathbf{R} m_i')
\end{equation}

weiter formuliert werden. Dann wird die Minimierung des Fehlers durch die Bestimmung des Maximums der Summe

\begin{equation}
\sum_{i=1}^n d_i'^t \mathbf{R} m_i'
\end{equation}

erreicht. Durch Ersetzen der Rotationsmatrix mit Quaternion $q$ wird das maximierte Problem umformuliert als:

\begin{equation}
\sum_{i=1}^n (q m_i'' \overline{q}) \cdot d_i'',
\end{equation}

wobei $\overline{q}$ das konjugierte Quaternion von $q$ ist, $m_i'' = (0, m_{i,x}', m_{i,y}', m_{i,z}')$ und $d_i'' = (0, d_{i,x}', d_{i,y}', d_{i,z}')$ die erweiterte Quaternion für Punkte $m_i'$ bzw. $d_i'$ sind.
Dann gilt:

\begin{align}
\label{qmdq}
\sum_{i=1}^n (q m_i'' \overline{q}) \cdot d_i''
& = \sum_{i=1}^n (q m_i'' \overline{q}) \cdot ( d_i'' q \overline{q}) \nonumber \\
& = \sum_{i=1}^n (q m_i'') \cdot (d_i'' q).
\end{align}

Die beiden Multiplikationen in Klammen können als Formel \eqref{QMulti} zum Produkt von einem Quaternion und einer Matrix umschrieben werden:

\[
q m_i'' = 
\begin{pmatrix}
0 & -m_{i,x}' & -m_{i,y}' & -m_{i,z}' \\
-m_{i,x}' & 0 & -m_{i,z}' & -m_{i,y}' \\
-m_{i,y}' & -m_{i,z}' & 0 & -m_{i,x}' \\
-m_{i,z}' & -m_{i,y}' & -m_{i,x}' & 0
\end{pmatrix}
 = \mathbf{M}_i q
\]

und 

\[
d_i'' q = 
\begin{pmatrix}
0 & -d_{i,x}' & -d_{i,y}' & -d_{i,z}' \\
d_{i,x}' & 0 & -d_{i,z}' & d_{i,y}' \\
d_{i,y}' & d_{i,z}' & 0 & -d_{i,x}' \\
d_{i,z}' & -d_{i,y}' & d_{i,x}' & 0
\end{pmatrix}
 = \mathbf{D}_i q.
\]

Dann kann \eqref{qmdq} weiter abgeleitet werden:

\begin{align}
\label{qNq}
 \sum_{i=1}^n ( \mathbf{M}_i q) \cdot ( \mathbf{D}_i q)
 & = \sum_{i=1}^n q^t \mathbf{M}_i^t \mathbf{D}_i q \nonumber \\
 & = q^t \Big( \sum_{i=1}^n \mathbf{M}_i^t \mathbf{D}_i \Big) q \nonumber \\
 & = q^t \Big( \sum_{i=1}^n \mathbf{N}_i \Big) q \nonumber \\
 & = q^t \mathbf{N} q,
\end{align}
 
wobei $\mathbf{N}_i = \mathbf{M}_i^t \mathbf{D}_i$ ist, und $\mathbf{N}$ die Summe von $\mathbf{N}_i$ beschreibt. Sei $\mathbf{H}$ die Summe der Kreuzprodukte des Punktpaars von Punktmengen $D$ und $M$:

\begin{equation}
\label{QuaH}
\mathbf{H} = \sum_{i=1}^n m_i' d_i'^t.
\end{equation}

Es ist deutlich, dass die Größe der Matrix $\mathbf{H}$ 3 $\times$ 3 ist, deshalb kann $\mathbf{H}$ auch als

\begin{equation}
\mathbf{H} = 
\begin{pmatrix}
S_{xx} & S_{xy} &S_{xz} \\
S_{yx} & S_{yy} &S_{yz} \\
S_{zx} & S_{zy} &S_{zz} 
\end{pmatrix}
\end{equation}  
  
geschrieben werden, wobei

\[
S_{xx} = \sum_{i=1}^n m_{i,x}' d_{i,x}' \quad ,\quad
S_{xy} = \sum_{i=1}^n m_{i,x}' d_{i,y}' \quad , \quad \cdots
\]

Dann kann die Matrix $\mathbf{N}$ im \eqref{qNq} durch die Elemente von $\mathbf{H}$ dargestellt werden als:

\begin{equation}
\label{QuaN}
\mathbf{N} = 
\begin{pmatrix}
S_{xx} + S_{yy} + S_{zz} & S_{yz} - S_{zy} & S_{zx} - S_{xz} & S_{xy} - S_{yx} \\
S_{yz} - S_{zy} & S_{xx} - S_{yy} - S_{zz} & S_{xy} + S_{yx} & S_{zx} + S_{xz} \\
S_{zx} - S_{xz} & S_{xy} + S_{yx} & -S_{xx} + S_{yy} - S_{zz} & S_{yz} + S_{zy} \\
S_{xy} - S_{yx} & S_{zx} + S_{xz} & S_{yz} + S_{zy} & -S_{xx} - S_{yy} + S_{zz} 
\end{pmatrix}
\end{equation}

Nach dem Beweis von Horn \cite{H87} wird die Formel \eqref{qNq} genau dann maximal, wenn $q$ der Eigenvektor der Matrix $\mathbf{N}$ ist, der zu dem maximalen, positiven Eigenwert entspricht. 

\section{Objekterkennung}
Die gesamte Objekterkennung kann in zwei Phasen aufgeteilt werden: die Segmentierung und der Vergleich des Strukturgraphen. In diesem Abschnitt werden die wichtigsten Algorithmen für beide Teile erklärt. 

\subsection{DBSCAN}
\label{Dbscan}
DBSCAN, kurz für den englischen Namen ,,Density-Based Spatial Clustering of Applications with Noise'', ist ein auf Dichte basierter Data-Mining-Algorithmus \cite{E96}. Die Hauptidee des Algorithmus hängt stark von dem Begriff ,,Dichteverbundenheit'' ab, der durch folgende Definitionen erklärt wird.
\\
\\
Seien $D$ eine Punktmenge im Raum $\mathbb{R}^n$ und $Dist(p,q)$ eine darauf definierte Distanzfunktion. $\epsilon$ und MinPts sind zwei Eingaben des Algorithmus.

\begin{definition}
\textsf{$\epsilon$-Umgebung} $N_{\epsilon}(p)$ ist eine Menge der Punkte um $p$, die 
\[
N_{\epsilon}(p) = \{ q \in D | Dist(p,q) \geq \epsilon \}
\]
\end{definition}

erfüllt.

\begin{definition}
Ein Punkt $p$ ist \textsf{direkt Dichte-erreichbar} zum Punkt $q$, g.d.w:
\begin{align*}
1) \quad & p \in N_{\epsilon}(q)  \nonumber \\
2) \quad & | N_{\epsilon}(q) | \geq MinPts \nonumber
\end{align*}
\end{definition}

\begin{definition}
Ein Punkt $p$ ist \textsf{Dichte-erreichbar} zum Punkt $q$, g.d.w es eine Kette von Punkten $p_1, \ldots , p_n$ mit $p_1 = p$ und $p_n = q$ gibt, wobei $p_{i+1}$ \textsf{direkt Dichte-erreichbar} zum $p_i$ für alle $i \in [1,n]$ ist.
\end{definition}

\begin{definition}
Zwei Punkte $p$ und $q$ heißen \textsf{Dichte-verbunden}, g.d.w es einen Punkt $o$ gibt, wobei $p$ und $q$ jeweils zu $o$ \textsf{Dichte-erreichbar} sind.
\end{definition}

Mithilfe dieser Definitionen der Beziehung zwischen den Punkten kann man eine einzige Definition des Clusters ausgeben.

\begin{definition}
Sei $C$ eine nicht leere Teilmenge von $D$. Dann heißt $C$ ein Cluster, wenn die folgenden zwei Bedingungen erfüllt werden:
\begin{align*}
1) \quad & \text{für $\forall p, q \in D$, wenn $p \in C$ und $q$ ist \textsf{Dichte-erreichbar} zum $p$, dann gilt } q \in C, \nonumber \\
2) \quad & \text{für $\forall p, q \in C$, $p$ ist \textsf{Dichte-verbunden} zu $q$.} \nonumber
\end{align*}
\end{definition}

Dann können alle Punkte von $D$ in drei verschiedene Typen zusammengefasst werden.

\begin{itemize}
\item Kernpunkt: Der Punkt, dessen $\epsilon$-Umgebung größer als MinPts ist.
\item Grenzpunkt: Der Punkt, dessen $\epsilon$-Umgebung nicht groß genug ($<$MinPts), aber zu anderen Punkten Dichte-erreichbar ist.
\item Geräusch: Der Punkt, der zu keinem Cluster gehört.  
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.3]{Abbildungen/DBSCAN}
\caption{Ein Beispiel für die drei Typen der Punkte in DBSCAN-Algorithmus. Der rote Punkt A ist ein Kernpunkt. Die gelbe Punkte B und C sind die Grenzpunkte, die von roten Punkten Dichte-erreichbar sind. Wegen fehlender Dichte-Verbindung zwischen Punkt N und anderen Punkten, wird N als Rauschen erkannt. \cite{DWiki}}
\label{DBSCAN}
\end{figure}

Abbildung~\ref{DBSCAN} zeigt ein Beispiel für alle diese drei Typen eines Punkts, wobei die Kernpunkte mit Rot, Grenzpunkte mit Gelb und Rauschen mit Blau dargestellt werden. Die Kreise zeigen die $\epsilon$-Umgebungen für die Punkte an ihren Ursprüngen.
\\
\\
Der Algorithmus durchläuft für jeden Punkt von Punktmenge $D$ und die Lösung hängt von der Reihenfolge der Punkte nicht ab. Ein Kernpunkt soll zuerst gefunden werden, und dann die andere Punkte in ihrer $\epsilon$-Umgebung betrachtet werden. Zwei $\epsilon$-Umgebungen werden verknüpft, wenn sie mindesten einen identischen Punkt haben. Der genaue Ablauf des DBSCAN-Algorithmus ist in Algorithmus~\ref{algDBSCAN} ausgegeben.

\begin{algorithm}[htbp]                      % enter the algorithm environment
\caption{DBSCAN}          % give the algorithm a caption
\label{algDBSCAN}                      % and a label for \ref{} commands later in the document
\begin{algorithmic}                    % enter the algorithmic environment
	\State $D, \epsilon, MinPts$
	\State Setzen $C$ zum ersten Cluster
	\For {jede $P_i \in D$}
		\If {$P_i$ ist nicht besucht}
			\State Markieren $P_i$ als besucht
			\State $N = \{ P_j \in D | Dist(P_i, P_j)<\epsilon \}$
			\If {Anzahl der Elemente von $N$ $<$ MinPts}
				\State Markieren $P_i$ als Geräusch
			\Else
				\State Fügen $P_i$ in aktuellem Cluster $C$ ein
				\For {jede $P_j \in N$}
					\If {$P_j$ ist noch nicht besucht}
						\State Markieren $P_j$ als besucht
						\State $N' = \{ P_k \in D | Dist(P_j, P_k)<\epsilon \}$
						\If {Anzahl der Elemente von $N'$ $\geq$ MinPts}
							\State $N = N \cup N'$
						\EndIf 
					\EndIf
					\If {$P_j$ gehört zu keinem Cluster}
						\State Fügen $P_j$ in aktuellem Cluster $C$ ein
					\EndIf
				\EndFor
				\State Setzen $C$ zum nächsten Cluster
			\EndIf
		\EndIf
	\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Teilgraph Isomorphismus}
\label{TI}
Die Objekterkennung bzw. Objektverfolgung wird durch den Vergleich der Modellgraphen und Eingabegraphen realisiert. Ein für diese Arbeit hilfreicher Algorithmus wurde von Rhijn und Mulder entwickelt \cite{AJ05}. Um das Problem des Graph Isomorphismus zu vereinfachen, wird nur ein Teilgraph $S_{min}$ von Modellgraphen betrachtet. $S_{min}$ soll ein vollständiger Graph sein, um einen Modellgraphen eindeutig bestimmen zu können. Zuerst wird ein Punkt $p_i$ vom Eingabegraph ausgewählt und die Distanzen von diesem zu allen seinen Nachbarn berechnet. Sämtliche Kantenlängen werden mit den Kanten der Modellgraphen verglichen, damit ein Kandidatenpunkt $v_k$ im Modellgraph gefunden werden kann, der genug Kanten mit dem ausgewählten Punkt $p_i$ identisch hat. Zweitens sollen drei Nachbarn von $p_i$ gefunden werden, die mit $p_i$ zusammen einen vollständigen Graphen (Pyramide) darstellen können. Wenn ein Teilgraph von Modellgraphen zum obigen vollständigen Graph assoziiert wird, ist ein Teilgraph Isomorphismus zwischen den Eingabegraphen und Modellgraphen gefunden. Der Algorithmus~\ref{isoAlgo} gibt eine genauere Beschreibung des Algorithmus von Rhijn und Mulder an.

\begin{algorithm}
\caption{Teilgraph Isomorphismus}
\label{isoAlgo}
\begin{algorithmic}
	\State Modellgraph:$G_m$, Eingabegraph:$G_d$
	\For {jede $p_i \in G_d$}
		\For {jeder Nachbar $p_j$ von $p_i$}
			\For {alle Punktpaare $(v_k, v_l) \in G_m$} 
				\If {$dist(p_i, p_j) = dist(v_k, v_l)$}
					\State Fügen assoziierte Punktpaar $<p_j, v_l>$ zum $S_i$ ein
				\EndIf 
			\EndFor
			\If {$\| S_i \| \geq \| S_{min} \|$}
				%\State Fügen den Punkt $<p_i, v_k>$ zum Kandidaten $K$ ein
				\If {Drei Punktpaare in $S_i$ gefunden können, damit deren ersten Punkte mit $p_i$ ein Pyramide aufbauen können}
					\State Teilgraph Isomorphismus ist gefunden
					\State Break
				\EndIf
			\EndIf 
		\EndFor
	\EndFor
\end{algorithmic}
\end{algorithm}


