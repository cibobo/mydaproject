\chapter{Implementierung}

\begin{figure}[ftb]
\centering
\includegraphics[scale=0.55]{Abbildungen/HauptAblauf.png}
\caption{Ablaufdiagramm}
\label{hAblauf}
\end{figure}

Die Implementierung kann in vier Hauptmodule unterteilt werden:

\begin{itemize}
\item Markenanalyse,
\item Objekteinlernen,
\item Objekterkennung und Verfolgung,
\item Bilder Steuerung.
\end{itemize}

Der allgemeine Ablaufprozess des Programms ist in Abbildung~\ref{hAblauf}. Das ganze Programm ist eine Schleife, in jedem deren Schritt die Information der PMD Kamera angesammelt und als Eingabe genutzt wird. Nach Bewertung der neuen Eingabe bzw. der Rückkopplung vom letzten Ablauf werden die entsprechenden Bilddaten vom Modul {\itshape Bilder Steuerung} für die weiteren Schritte ausgewählt. Das Modul {\itshape Markenanalyse} analysiert die eingegebene Bilddaten und versucht die Marken zu finden und zu verfolgen. Die ausgefundenen Marken werden entweder von dem {\itshape Objekteinlernen} oder durch die {\itshape Objekterkennung und Verfolgung} benutzt, was am Anfang des Programms durch den Benutzer entschieden wird. In der Einlernensphase wird zuerst die Transformation des Objekts bestimmt und dann der charakteristische Graph des Objekts durch die Marken dargestellt. Wenn man ein Objekt wieder erkennen möchte, werden die Marken von neuem Objekt zu den vorhandenen charakteristischen Graphen verglichen, die nach dem Objekteinlernen im Speicher gespeichert werden. Das Ergebnis wird in einem OpenGL Fenster ausgezeigt und als Rückkopplung für den nächsten Schritt genutzt. Alle diese Module werden in den folgenden Unterabschnitten genau erklärt. Ein ausführliches Ablaufdiagramm wird in Abbildung~\ref{gAblauf} gezeigt.   

\begin{figure}
\centering
\includegraphics[scale=0.7]{Abbildungen/Ablaufsbild.png}
\caption{Ausführliches Ablaufdiagramm}
\label{gAblauf}
\end{figure}

\section{Markenanalyse}
\label{MAna}
\subsection{Bildvorverarbeitung}

\subsubsection{Datenstruktur des Eingabebilds}
Wie im Abschnitt \ref{PMDData} erklärt, liefert die PMD Kamera insgesamt vier verschiedenen Arten der Vermessungsdaten. Alle diese Daten eines Bildes werden in einer Instanz von Klasse \textbf{BildData} gespeichert. Dazwischen sind die Amplituden und 3D Koordinaten ganz wichtig und werden in verschiedenen Teilen des Programms verwendet: die Amplituden werden für die Markenerkennung und die 3D Koordinaten mit räumlicher Information werden später für die Korrespondenzuntersuchung und Schätzung der Lage des Objekts benutzt. Da die definierte Dimension und das definierte Intervall dieser zwei Arten der Daten sich von anderem unterscheidet, ist vor der weiteren Bildverarbeitung dieser Arbeit die Übereinstimmung beider Daten für jeden Pixel notwendig. Die Klasse \textbf{PMDPoint} wird nun definiert, um den Zweck zu erfüllen. Die Klassendiagramme beider Klassen werden in der Abbildung~\ref{BD} gezeigt.

\begin{figure}[ftb]
\centering
\includegraphics[scale=1]{Abbildungen/BildData.png}
\caption{Die Klassendiagramme für \textbf{BildData} und \textbf{PMDPoint}.}
\label{BD}
\end{figure}

\subsubsection{Abstand Filter}
Die Verbesserung der Ergebnisse durch die Segmentierung des fokussierten Objekts aus der Umgebung wurde im Abschnitt \ref{sbSeg} bemerkt. Deshalb soll am Anfang der Markenanalyse einen Abstand-Filter definiert werden. Der Abstand-Filter dieser Arbeit besteht aus zwei Teilen: der Teil der Initialisierung bzw. der Teil der Filterung. Zwischen der Initialisierung des Filters werden eine Menge der Tiefdaten der Eingabebilder mit vordefinierter Größe angesammelt, damit das durchschnittliche Tiefbild der Szene erzeugt werden kann. Der Pseudocode wird in Algorithmus~\ref{AFIni} gezeigt.

\begin{algorithm}
\caption{Initialisierung des Abstand-Filters}
\label{AFIni}
\begin{algorithmic}
    \State Anzahl der notwendigen Eingabebilder: $N$
    \State Tiefbild für die durchschnittliche Szene: $D$
    \For {$i$ von $1$ bis $N$}
    	\State $E \gets$ aktuelle \textbf{BildData}
        \If {$D$ == NULL}
            \State $D \gets E.3DKoordinaten.Z$ 
        \Else
        	\For {jedes Pixel $d_j$ von $D$}
        		\State $d_j \gets \frac{1}{2}(d_j + e.3DKoordinaten_{j_z})$
        	\EndFor
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}

Durch Vergleich der in Initialisierungsphase erzeugten Szene und die aktuelle Tiefdaten der 3D Eingabe, können die neu in der Szene eingefügten Objekte aus den Hintergrund bestimmt werden. Wenn der Abstand zwischen einem Pixel und dem Hintergrund größer als der vordefinierte Schwellenwert ist, wird die Amplitude des gefilterten Bildes in diesem Pixel von originaler Amplitude übertragen. Sonst wird die Amplitude als null ersetzt. Die Anzahl der unterschiedlichen Bildpunkten wird gleichzeitig abgezählt, damit eine boolesche Ausgabe über die Verschiedenheit mit dem gefilterten Bild zusammen zurück gegeben werden kann. Der genaue Ablauf wird in Algorithmus~\ref{AFFilter} beschrieben. 

\begin{algorithm}
\caption{Filterungsphase des Abstand-Filters}
\label{AFFilter}
\begin{algorithmic}
	\State Schwellenwert für Abstandsvergleich: $\epsilon$
	\State Schwellenwert für Verschiedenheit: $\alpha$	
	\State $E \gets$ aktuelle \textbf{BildData}
	\For {jedes Pixel $d_i$ von $D$}
		\If {$\|d_i - e.3DKoordinaten_{i_z} \| < \epsilon$}
			\State $e.filteredAmplitude_i \gets 0$
		\Else
			\State $e.filteredAmplitude_i \gets e.Amplitude_i$
		\EndIf
	\EndFor
	\State $q \gets \| \text{veränderten Pixeln} \| / \|E\|$
	\If {$q < \alpha$}
		\State \Return Falsch
	\Else
		\State \Return True, $E$
	\EndIf
\end{algorithmic} 
\end{algorithm}

\subsection{Markenerkennung}

\subsubsection{Auswahl der Größe der Marken}
Die Erkennungsergebnisse der Objekte werden von der Größe der Marken stark beeinflusst, weshalb ein Test über die Markengröße vor der Implementierung notwendig ist. Die Testmarken werden als weiße Punkte auf einem schwarzen Papier gedruckt. Es gibt insgesamt 7 verschiedenen Größenstufen. Die Marken von jeder Stufe werden jeweils durch Quadrat und Kreis dargestellt. Abbildung~\ref{MS1} zeigt die unterschiedliche Erkennungsergebnisse der Marken, wenn das Objekt an verschiedenen Positionen aber auf der gleichen Höhenebene liegt. Abbildung~\ref{MS2} zeigt den Einfluss auf die Ergebnisse von der Distanz zu der Kamera. Was deutlich ist, dass je näher das Objekt zu der Kamera angebracht wird, desto kleinere Markengrößen werden benötigt.

\begin{figure}
\centering
\includegraphics[scale=0.5]{Abbildungen/MarkerSize1.png}
\caption{Die Erkennungsergebnisse der Marken für unterschiedliche Positionen.}
\label{MS1}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.75]{Abbildungen/MarkerSize2.png}
\caption{Die Erkennungsergebnisse der Marken für unterschiedliche Distanz zu der Kamera. Auf der ersten Reihe liegen die Graustufenbilder mit roten erkannten Marken; Die entsprechende 3D Bilder auf der zweiten Reihe zeigen die vertikalen Positionen des Objekts.}
\label{MS2}
\end{figure}

\subsubsection{STAR Detektor}
Wegen der in Abschnitt \ref{AusAlgo} festgestellten Gründe wird der STAR Detektor (CenSurE Algorithmus) in dieser Arbeit als Erkennungsalgorithmus ausgewählt. Die Parameter des Detektors werden im Dokument von OpenCV aufgelistet \cite{SDO} und von ,,ButterCookies'' in OpenCV Adventure \cite{SDOA} weiter deutlicher erklärt. Der Parameter {\itshape maxSize} definiert die maximale Größe der Marken. {\itshape responseThreshold} ist ein Schwellenwert über die Antwort von approximierten Laplacian. Die erkannten Marken mit niedrigerer Antwort als diesen Schwellenwert werden dann eliminiert. Die Parameter {\itshape lineThresholdProjected} und {\itshape lineThresholdBinarized} stellen die Stärke der Linie-Suppression ein. Der letzte Parameter {\itshape suppressNonmaxSize} beschreibt die Größe des betrachteten Raums der Non-Maximal Suppression. In dieser Arbeit benutzt {\itshape responseThreshold} als den variablen Parameter, d.H. dessen Wert in jedem Schritt verändert wird.

\subsubsection{Markenerkennung} 
Algorithmus~\ref{ME} zeigt den Verlauf der Markenerkennung mit STAR Detektor. Der ganze Erkennungsprozess wird als eine Schleife mit vordefinierter maximaler Anzahl der Durchläufe dargestellt. In jedem Schleifenrumpf wird das Graustufenbild als Eingabe des STAR Detektors mit dem aktuellen Kontrast vor der Erkennung erzeugt. Nach der Erkennung schickt das Programm dann das aktuelle Graustufenbild und die Anzahl der erkannten Marken zu der Funktion \textbf{Kontraststeuerung} (sehen \ref{SHuK} und Algorithmus~\ref{KS}), und wird von ihr den neuen Kontrast bzw. ResponseThreshold für nächsten Durchlauf erhalten. 

\begin{algorithm}
\caption{Markenerkennung}
\label{ME}
\begin{algorithmic}
	\State Eingabebild: $I$, maximale Schleife: $N$
	\State $a \gets$ Anfangswert für Kontrast 
	\State $r \gets$ Anfangswert für Responsethreshold
	\State $b \gets$ feste Helligkeit  
	\For {$i von 1 bis N$}
		\State Eingabe für STAR Detektor: $H_i \gets aI+b$
		\State Erkannten Marken: $P \gets CenSurE(H_i,r)$
		\State Result $\gets$ Kontraststeuerung($H_i, a, r, \|P\|$)
		\If {Result = False}
			\State continue mit aktualisiertem Kontrast und Responsethreshold
		\Else
			\State break
		\EndIf
	\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Steuerung der Helligkeit und des Kontrast}
\label{SHuK}
Die Helligkeit bzw. der Kontrast können die Ergebnisse der Markenerkennung stark beeinflussen. Eine viele bekannte Arbeit über radiometrische Kalibrierung wurde von Debevec und Malik in \cite{DM08} gemacht. Ihres Verfahren ist robust und präzise, aber benötigt viele Bilder von gleicher Szene mit verschiedenen Belichtungszeiten, damit alle Information der Szene für jeweils helleren Bereich bzw. dunkleren Bereich versammelt wird. Wegen der Beschränkung der Kamera kann diese Bedingung in unserer Arbeit leider nicht erfüllt werden. Außerdem ist das Verfahren von Debevec sehr rechenintensiv. Glücklicherweise sollen in der Markenerkennung unserer Arbeit aber nur die Bilddaten um Marken betrachtet werden. D.h. die sonstige Informationen des Bildes können einfach ignoriert werden. Durch diese Grundidee kann ein Algorithmus erzeugt werden, dessen Endbedingung durch Untersuchung der Anzahl der erkannten Marken eingestellt wird.
\\
\\
Der formulierte Durchlauf wird im Algorithmus~\ref{KS} gefunden. Die Anzahl der erkannten Marken ist die erste Stufe der Prüfnorm von Kontraststeuerung. Wenn nicht genug Marken erkannt werden, wird die Strahlungsenergie des Graustufenbildes als die zweite Stufe der Prüfnorm überprüft. Für die zu große bzw. zu kleine Strahlungsenergie, verringert bzw. erhöht der Kontrast sich im erlaubten Intervall, was als Eingangsparameter vorher definiert wird. Wenn die Strahlungsenergie zufrieden ist aber noch nicht genug Marken gefunden sind, nimmt das ResponseThreshold ab, damit mehr Marken mit schwächeren Antworten erkannt werden können. Für die Gegenseite für die zu viele erkannten Marken, wird der Wert von ResponseThreshold vergrößert. Bei dieser Situation wird die Strahlungsenergie nicht mehr betrachtet, damit der Algorithmus zur der Konvergenz halten kann. Ein boolescher Wert wird von dem Algorithmus zurückgegeben, was zeigt, ob befriedigend viele Marken gefunden werden.

\begin{algorithm}
\caption{Kontraststeuerung($H, \&a, \&r, \|P\|$)}
\label{KS}
\begin{algorithmic}
	\State Intervall der erlaubten Strahlungsenergie: $(E_{min}, E_{max})$
	\State Intervall des erlaubten Kontrastes: $(A_{min}, A_{max})$
	\State Intervall des erlaubten ResponseThreshold von STAR Detektor: $(R_{min}, R_{max})$
	\State Intervall des Erwartens der Anzahl des bekannten Marken: $(P_{min}, P_{max})$
	\State $e \gets$ aktuelle Strahlungsenergie von $H$
	\If {$\|P\| < P_{min}$}
		\If {$e < E_{min}$}
			\State $a \gets a-0.5$
			\If {$a < A_{min}$}
				\State $a \gets A_{min}$
				\State \Return False
			\EndIf
		\ElsIf {$e > E_{max}$}
			\State $a \gets a+0.5$
			\If {$a > A_{max}$}
				\State $a \gets A_{max}$
				\State \Return False
			\EndIf
		\Else 
			\State $r \gets r-5$
			\If {$r < R_{min}$}
				\State \Return False
			\EndIf
		\EndIf
	\ElsIf {$\|P\| > P_{max}$}
		\If {$r > R_{max}$}
			\State \Return False
		\Else
			\State $r \gets r+4$
		\EndIf
	\Else
		\State \Return True
	\EndIf
\end{algorithmic}
\end{algorithm}



\subsection{Markenverfolgung}
\label{MV}
\subsubsection{Verbesserung der Singulärwertzerlegungsverfahren}
Nach erfolgreicher Festlegung der Marken jedes Bildes sollen dann die Korrespondenzen der Marken von zwei nachfolgenden Bildern untersucht werden. Die Korrespondenzpunkte können durch das Verfahren der Singulärwertzerlegung von Scott und Higgnis \cite{SL91} bestimmt werden. Die genaue Beschreibung ihres Verfahrens wird im Schnitt~\ref{KdS} aufgeführt, und Algorithmus~\ref{alg1} gibt den Peseudocode des Verfahrens an. In dieser Arbeit werden vier Verbesserungen für das Verfahren von Scott und Higgnis gemacht, damit der die stabileren Ergebnisse erzeugt und die Qualität der Korrespondenzuntersuchung bewertet werden kann. 
\\
\\
\textbf{Nebenbedingung für die Bestimmung der größten Elemente}
\\
In originalem Algorithmus werden die Punkte $I_i$ und $J_j$ als Korrespondenzpunkte erkannt, genau dann, wenn das Element $P_{ij}$ das größte Element von beide Zeile $i$ und Spalte $j$ ist. Aber manchmal liefert das größte Element nicht die beste Korrespondenz, insbesondere in den Fall, dass es große Transformation zwischen zwei betrachteten Bildern gibt. Die Lösung ist, eine weitere Beschränkung für die Untersuchung der größten Elemente einzusetzen. D.h. die größten Elemente werden nur akzeptiert, wenn sie gleichzeitig größer als ein eingegebener Schwellenwert $\epsilon$ sind. Der Schwellenwert $\epsilon$ wird in $[0,1]$ definiert. Je höher $\epsilon$ ist, desto ordentlicher sind die gefundenen Korrespondenzpunkte. In dem idealen Fall sind alle größten Elemente $P_{ij}$ gleich 1, z.B. wenn die beide betrachteten Bilder identisch sind. 
\\
\\
\textbf{Erfolgreiche Behauptung}
\\
Wenigere Korrespondenzpunkte werden mit obiger stärkeren Nebenbedingung herausgefunden, was aber die weiteren Arbeitsschritte wenig beeinflusst, weil es zumindest nur 3 korrespondierenden Punktpaare benötigt, um die Orientierung zwischen zwei Bildern zu bestimmen. Trotzdem ist die Überprüfung der Anzahl der gefundenen Punktpaare notwendig, und deren Ergebnis wird als eine boolesche Ausgabe des Algorithmus zurückgegeben.
\\
\\
\textbf{Qualitätsmanagement der Korrespondenz}
\\
Außer der binären Behauptung des Algorithmus ist die Bewertung der Korrespondenzqualität auch wichtig, damit die gute und stabile Bilderkette für Markenverfolgung ausgewählt werden kann. Das wurde aber leider von Scott und Higgnis in ihrer Arbeit nicht genannt. In der ersten Verbesserung wird die Größe der größten Elemente von Matrix $P$ mit einem Schwellenwert weiter beschränkt, um das bessere Korrespondenzergebnis zu erhalten. Deshalb zur Umkehr kann die Größe der größten Elemente von $P$ als die Messung der Korrespondenzqualität definiert werden. Dann wird die Summe aller größten Elemente $\sum P_{ij}$ in dieser Arbeit zur Bewertung der Korrespondenzuntersuchung verwendet. Hier wird kein arithmetische Mittel benutzt, weil die Anzahl der betrachteten Punkte, die als Eingaben in den Algorithmus eingegebene werden, auch ein wichtiger Faktor der Bewertung der Korrespondenz ist.
\\
\\
\textbf{Die Auswahl von $\sigma$ mit Rückkopplung}
\\
Die Einheit des Abstands $\sigma$ ist der wesentliche Parameter des Singulärwertzerlegungsverfahrens. Das ungeeignete $\sigma$ erzeugt dann großes Chaos in Korrespondenzlösungen, was schon in \cite{SL91} mit Schaubildern vielmals gezeigt wird. Um die besten Lösungen zu finden, soll die Abstandseinheit nicht kleiner als die durchschnittlichen Distanz zwischen den Korrespondenzpunkten definiert werden. Es gibt zwei Schwierigkeiten für die Bestimmung des $\sigma$. Erstens sind die korrespondierenden Punktpaare vor dem Durchlauf des Algorithmus noch nicht bekannt, weshalb kann die Abstandseinheit nicht direkt berechnet sondern nur geschätzt werden. Zweitens ist die Auswahl einer festen Abstandseinheit schwierig und ineffizient, wenn sich die betrachteten Punkte oder Merkmale, z.B. in dieser Arbeit, uneingeschränkt bewegen können. Deshalb wird hier der Parameter $\sigma$ für jedes Bild mit der durchschnittlichen Distanz zwischen allen korrespondierenden Punktepaaren vorheriges Bildes festgelegt. Wegen der festen Bildwiederholfrequenz der Eingabebilder sind in theoretischen Fall die durchschnittlichen Abstand der Korrespondenzpunkte jedes Bildes gleich, wenn sich alle betrachteten Punkte gleichförmig bewegen. Aber für die schwach beschleunigenden Bewegungen der Merkmale kann das anpassende $\sigma$ trotzdem herausgefunden werden und die Korrespondenzlösungen sich stabil erzeugen lassen.
\\
\\
Das verbesserte Algorithmus wird im Algorithmus~\ref{algSVD} gezeigt.

\begin{algorithm}                     
\caption{Korrespondenzuntersuchung}         
\label{algSVD}                          
\begin{algorithmic}
	\State Korrespondierende Punktpaare: $Result$                    
    \State Eingabebilder von jeweils Zeitpunkt $t_{i-1}$ und $t_{i}$: $I,J$
    \State Aktuelle approximierte Abstandseinheit: $\sigma_{i}$
	\State Schwellenwert für die Beschränkung der größten Elemente: $\epsilon$
	\State Messung der Korrespondenzqualität: $mess$ 
	\State Summe der Abstände der Korrespondenzpunkte: $sumDistance$
    \For{$i=1 \to m$, $j=1 \to n$}
    	\State $r_{ij} \gets Dis(I_i, J_j)$
    	\State $G_{ij} \gets exp(-\frac{r_{ij}^2}{\sigma_i^2})$
    \EndFor
    \State $T,U \gets$ Singulärwertzerlegung von G
    \State $E \gets m \times n$ Diagonalmatrix mit $E_{ii} = 1$
    \State $P \gets TEU$
    \State $minMN \gets Min(m, n)$
    \For{$i=1 \to minMN$}
    	\State $MaxSpalteIndex[i] \gets$ Index der Spalte des maximalen Elements an Reihe $i$.
    	%\State $MaxSpalteIndex_i \gets Max$ 
    \EndFor
    \For{$i=1 \to minMN$}
    	\If {$P_{iMaxSpalteIndex[i]}$ ist Maximum der Spalte $MaxSpalteIndex[i]$}
    		\If {$P_{iMaxSpalteIndex[i]} > \epsilon$}
    			\State $Result \gets$ Punktpaar($I_i, J_{MaxSpalteIndex[i]}$)
    			\State $mess \gets mess + P_{iMaxSpalteIndex[i]}$
    		\EndIf 
    	\EndIf
    \EndFor 
    \For {Alle Punktpaare $(I_i, J_j) \in Result$}
    	\State $sumDistance \gets sumDistance + DistanceOf(I_i,J_j)$
    \EndFor
    \State $\sigma_{i+1} \gets sumDistance / \| Result \|$
    \If {$\| Result \| < 3$ }
    	\State \Return False
    \Else
    	\State \Return True, $mess$
    \EndIf
\end{algorithmic}
\end{algorithm}

\subsection{Segmentierung}
\label{Seg}
Bis jetzt sind alle erkannten Marken eines Bildes als eine Menge der Punkte zusammen betrachtet, die dann segmentiert werden soll, um die Marken für verschiedene Objekte zu verteilen. Die Hauptidee ist, dass die Marken als die Merkmale von gleichem Objekt erkannt werden, genau dann, wenn die Abstände zwischen diesen Marken viel kleiner als die Abstände von ihr zu den anderen erkannten Marken sind. Die Problemstellung der Segmentierung ist gleich wie ein Clusting-Problem, deshalb wird in dieser Arbeit der Clusting-Algorithmus DBSCAN für die Segmentierung benutzt. Die Beschreibung des Verfahrens wird im Abschnitt \ref{Dbscan} gezeigt und der genaue Durchlauf wird von Algorithmus~\ref{algDBSCAN} erklärt.	Eine Liste der Punktmengen werden von DBSCAN ausgegeben, und jedes deren Element stimmt mit der Merkmalen eines Objekts überein. Da es zumindest 3 Punkte benötigt, um die Lage des räumlichen Körpers im 3D Raum zu bestimmen, werden die Punktmengen von Ausgabe der DBSCAN als Rauschen erkannt, die weniger als 3 Punkte enthalten.

\section{Objektlernen}
\label{OL}
Nach der erfolgreichen Markenanalyse werden eine Menge der Marken aus den Eingabebildern herausgefunden. Weiterhin sind die Abhängigkeiten dieser Marken zwischen benachbarten Bildern auch bekannt. Durch die Segmentierung werden diese Marken danach für unterschiedlichen Objekte weiter verteilt. Im diesen Abschnitt wird es erklärt, wie ein charakteristischer Graph des Objekts mithilfe der erkannten Marken dargestellt werden kann. In dieser Arbeit wird im Objektlernen die vereinfachte Situation berücksichtigt, dass einmal nur ein Objekt in den Eingabebildern vorkommt. Deshalb wird nur die größten Punktmenge von Segmentierungsergebnis für Objektlernen ausgewählt, die als die Eingabe der folgenden Arbeitsschritte eingestellt wird. 

\subsection{Bestimmung der Orientierung}
\label{BdO}
Um ein Objekt zu lernen, muss das Objekt mit Marken zuerst zu der Kamera gezeigt und sich langsame umdreht werden. Zwischen der Umdrehung werden dann die relativen Positionen der Marken an jeder Ebene bestimmt. Deshalb sollen die Lagen der bekannten Marken in diesem Ablauf rechtzeitig aktualisiert werden, was als ein Orientierungsproblem formuliert werden kann. In dieser Arbeit wird der Verfahren von \cite{H87} verwendet, um die Transformation der bekannten Marken zwischen nachfolgenden Bildern zu berechnen. Der genaue Durchlauf wird im Abschnitt \ref{QmE} aufgeführt. Zuerst sollen die Schwerpunkte der erkannten Marken herausgefunden werden (sehen Formel~\eqref{QuaSch}). Dann berechnet man die Vektoren, die von alle Marken zu ihren entsprechenden Schwerpunkten richten. Drittens wird die Korrelationsmatrix $H$ durch Formel~\eqref{QuaH} mit diesen Vektoren bestimmt werden. Die Hilfematrix $N$ in Formel~\eqref{QuaN} bestehlt aus den Elementen der Matrix $H$, und Ein von ihren Eigenvektoren ist die Einheitsquaternion für die Rotation, genau dann, wenn der den größten positiven Eigenwert entspricht. Die Rotation- bzw. die Translationsmatrix unter kartesischem Koordinatensystem werden durch jeweils die Formel~\eqref{QuaR} und \eqref{QTran} berechnet.
\\
\\
Die Lage des Objekts von jedem Bild hängt nur von der Lage des Objekts von vorherigem Bild ab. Wegen der kontinuierlichen Bestimmung der Orientierung des Objekts während des Objektlernens, kommt häufig großes
Verschieben zwischen der realen und berechneten Lage des Objekt in einem langen Bilderstrom vor, obwohl es nur winziger Fehler zwischen jedem zwei Bildern gibt. In dieser Arbeit wird das Kalman-Filter über die Translation des Schwerpunkts von Objekt benutzt, um die kumulative negative Wirkung jedes Schritts zu verdecken. Die Grundlage des Kalma-Filters wird im Abschnitt~\ref{KF} aufgeführt. Die Übergangsmatrix des Schwerpunkts des Objekts im 3D Raum kann definiert als:

\[
F = 
\begin{pmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{pmatrix},
\] 

was die Lage und Geschwindigkeit der Bewegung gleichzeitig betrachtet. Die Beobachtungsmatrix ist die Einheitsmatrix mit Größe 3$\times$3, weil der Schwerpunkt des Objekts $\in R^{3\times 1}$ als Eingabe zur Korrektorsphase eingegeben wird. OpenCV liefert eine komplette Implementierung über Kalman-Filter, was hier direkt verwendet wird.

\subsection{Markenanordnung}
Wie im Anfang des Abschnittes~\ref{BdO} geschrieben hat, soll während der Lernensphase die Lage betrachtetes Objekts in jedem Bild bestimmt werden. Es benötigt zumindest in zwei Bildern jeweils 3 nicht kollinearen Punkten des Objekts, damit die Transformation zwischen der Lage dieses Objekts in diesen zwei Bildern bestimmt werden kann. Je mehr die Punkte betrachtet werden, desto exaktere Transformation in der Theorie berechnet werden kann. Aber wegen der Auflösung der Kamera in dieser Arbeit wird die Größe der Marken bzw. die Abstände der Marken stark beschränkt. D.h. es ist unmöglich, beliebig viele Marken an dem Objekt anzubringen. Deshalb soll eine Strategie für die Anordnung der Marken entworfen werden, damit die beste Transformation mit möglich wenigeren Marken bestimmt werden kann.
\\
\\
Die Anordnungsstrategie wird in 5 Regeln zusammengefasst.
\begin{enumerate}
	\item Die Größen der Marken dürfen nicht zu klein sein.
	\item Die Abstände zwei Marken sollen nicht zu eng.
	\item Jedes Tripel der Marken soll nicht kollinear sein.
	\item Um Grenze zweier Ebenen des Objekts sollen mehr Marken als in Mitte angebracht werden.
	\item Der Strukturgraph jeder Ebene soll möglich unsymmetrisch sein.
\end{enumerate}

Die erste und zweite Regel hängen direkt von der Auflösung der Kamera ab, was schon im Abschnitt~\ref{AueM} diskutiert wurde. Die dritte Regel garantiert, dass die Orientierung des Objekts durch beliebigen drei Marken immer berechnet werden kann. Was in Regel 1 definierte minimale erkennbare Markengröße entspricht aber nur die Situation, dass die Marken mit der Bildebene der Kamera parallel sind. Wenn ein Objekt sich umdreht, verändert der Winkel zwischen der aktuellen beobachteten Ebene und der Kamera, wodurch werden die Marken auf dem Ebene zum kleineren Bereich auf dem Bild abgebildet. Die Größen der Marken auf dieser Ebene scheinen immer kleiner in dem Bild mit der Vergrößerung des Drehwinkels, bis sie komplette senkrecht zu der Bildebene liegt und die nächste Ebene vollständig unter Kamera vorkommt. Je keiner die Marken sind, desto sie schwieriger erkannt werden. Deshalb sollen mehr Marken im Bereich zwischen zwei benachbarten Ebenen angebracht werden, damit das Programm genug Marken für die Berechnung der Orientierung erhalten kann. Das ist genau was in Regel 4 erklärt wird. Die Asymmetrie von Regel 5 hält die Einzigartigkeit der Transformation zwischen zwei unterschieden Lagen des Objekts, d.h. nur eine Lösung wird von dem Orientierungsverfahren bestimmt.


\subsection{Darstellung des Strukturgraphen}
\label{DdS}
Das Ziel des Lernens ist, dass ein Strukturgraph für jedes eingegebenen Objekt erzeugt wird, welche als die Charakteristiken für Wiedererkennung verwendet werden können. Um das Ziel zu erreichen, sollen die Strukturgraphen stabil sein und genug charakteristische Information des Objekts enthalten. Die Stabilität bedeutet, dass der Strukturgraph für ein Objekt während viel mal Lernen gleich dargestellt werden sollen. Die Information, die in Wiedererkennung benötigt ist, wird durch die Positionen der Marken bzw. die Abstände dazwischen erzeugt. Die Verfahren der Bestimmungen der charakteristische Marken und Kanten werden in folgenden Abschnitten diskutiert.

\subsubsection{Bestimmung der stabilen Knoten}
\label{BdsK}
Nach der Markenerkennung werden viele Marken aus dem Bild erkannt, die aber viel Rauschen enthalten. Das Rauschen wird von den falschen erkannten Merkmalen des Objekt bzw. der Umgebung oder den schlechten Korrespondenzpunkten verursacht. Deshalb benötigt das Programm eine Strategie für die Auswahl der gültigen Marken. In dieser Arbeit wird die Auswahl auf den Erscheinungshäufigkeiten basiert, d.h. nur die Marken, die vielmals kontinuierlich vorgekommen sind, werden zu den gültigen Marken erkannt und in den Strukturgraphen eingefügt. Diese Marken werden als stabilen Knoten in dem Strukturgraphen markiert, und dürfen nicht verändert werden. Außer der Beschränkung der Anzahl der Erscheinungen soll aber auch der Begriff von ,,Identität'' zweier Marken in unterschiedlichen Bildern definiert werden. Zwei Marken von verschiedenen Bildern sind identisch, genau dann, wenn der Abstand von einer Marke zu dem Punkt, der von anderer Marke nach Transformation erzeugt wird, kleiner als einen vorgegebenen Schwellenwert ist. Die Transformation umfasst die Rotation- bzw. Translationsmatrix, die mit dem Verfahren vom Abschnitt~\ref{BdO} bestimmt werden. 

\subsubsection{Kanteneinfügung}
Die ungerichteten Kanten von Strukturgraphen speichern die Abstände zwischen den Knoten des Graphen, was die wichtigste Eigenschaft für die Wiedererkennung ist. Die Knoten, die gleichzeitig beobachtet werden können, werden in einem vollständigen Graph mit den anderen verbunden. Wenn irgendwann ein neuer Knoten in den Graphen eingefügt wird, verbindet der mit allen vorhandenen Knoten des Graphen. Der analoge Durchlauf wird während der Entfernung eines ungültigen Knotens aus dem Graphen durchgeführt. Der Algorithmus~\ref{algAG} zeigt, wie der aktuelle Strukturgraph mit neu eingegebenen Punkten aktualisiert.

\begin{algorithm}                     
\caption{GraphUpdate($Punkte$ $P$, $Mat$ $R$, $Mat$ $T$)}         
\label{algAG}                          
\begin{algorithmic}
\State Schwellenwert des Abstand: $\epsilon$
\State Minimale Lebenszeit des stabilen Knoten: $minT$
\State Aktueller Graph: $G$
\State $G_{temp} \gets createCompleteGraph(P)$
\For {jeder Knoten $v_i \in G$}
	\State $v_i \gets R v_i + T$
\EndFor
\For {jeder Knoten $v_i \in G$}
	\For {jeder Knoten $v_{temp_j} \in G_{temp}$}
		\If {$DistanceOf(v_i, v_{temp_j}) < \epsilon$}
			\State $v_i.Lifetime \gets v_i.Lifetime+2$
			\If {$v_i.Lifetime > minT$}
				\State $v_i.isFixed \gets True$
			\EndIf
			\State Verbinden allen mit $v_{temp_j}$ verbundenen Knoten in $G_{temp}$ mit $v_i$
			\State Entfernen $v_{temp_j}$ aus $G_{temp}$
		\EndIf
	\EndFor
	\If {Keiner entsprechende Knoten für $v_i$ aus $G_{temp}$ gefunden wird}
		\State $v_i.Lifetime \gets v_i.Lifetime-1$
		\If {$v_i.Lifetime<0$ und $!v_i.isFixed$}
			\State Entfernen $v_i$ aus $G$
		\EndIf
	\EndIf
\EndFor
\State Fügen allen übrigen Knoten von $G_{temp}$ in $G$
\end{algorithmic}
\end{algorithm}

\section{Zugriff des Strukturgraphen von Datei}
Das Ergebnis des Objektlernens ist ein Strukturgraph, der die Charakteristiken des betrachteten Objekts beschreibt. Die Knoten des Graphen werden als die 3D Punkten mit Gleitkommazahl definiert. Jeder Knoten enthält ein Map, wo seine Nachbarn und die Kantenlänge dazwischen paarweise gespeichert werden. Der Zugriff des Graphen von Datei soll dann implementiert werden, damit die Lernensergebnisse in der Festplatte gespeichert werden können. In dieser Arbeit wird die Dateiform von VTK (\cite{VTK}) benutzt, was als eine Open-Source-C++-Klassenbibliothek für die 3D Computergraphik und wissenschaftliche Visualisierung häufig verwendet wird. Der Vorteil der Verwendung der VTK Datei ist, dass die Ergebnisgraphen einfach weiter von den anderen Framework bzw. Software verwendbar sind. Das Abbildung~\ref{PV} ist das Screenshot von der Visualisierung eines Strukturgraphen mit Software ParaView \cite{PV}. Vor dem Speichern des Graphen, werden alle Knoten noch mal überprüft. Die ähnlichen Knoten sollen als einen einzigen Knoten kombiniert werden, damit nur ein vereinfachter Graph ohne verdoppelten Knoten gespeichert wird. VTK-Dateiform liefert vielen Stichwörter für Beschreibung der unterschiedlichen grundsätzlichen geometrischen Elemente, z.B. die Punkte, die Gerade und die Ebene usw.. Deshalb werden die Knoten und die Kanten des Strukturgraphen mit jeweils dem Stichwort ,,POINTS '' und ,,LINES'' in der VTK-Datei geschrieben. Diese Stichwörter sind auch die Kennzeichen bei dem Lesen der VTK-Datei. Die Punkte werden zuerst in einen neuen Graphen eingefügt, und verbindet dann mit anderen Knoten durch die gespeicherten Gerade.

\begin{figure}
\centering
\includegraphics[scale=0.3]{Abbildungen/ParaView.png}
\caption{Visualisierung eines Strukturgraphen des Kästchens mit ParaView.}
\label{PV}
\end{figure}

\section{Objekterkennung und Verfolgung}
Die Strukturgraphen aller von Programm erlernten Objekte sollen am Anfang der Wiedererkennungsphase von den in der Festplatte gespeicherten VTK-Dateien wieder zum Programm eingelesen werden. Danach wird die Markenanalyse genau wie in der Lernensphase durchgeführt und wird am Ende eine Liste von Punktmenge erzeugt. Das Programm versucht dann, die Teilgraphen dieser Strukturgraphen aus die von der Punktmengen neu darstellenden Graphen zu finden, was zum sogenannten Teilgraph-Isomorphismus-Problem zusammengefasst wird. Für einziges Bild werden alle mögliche Kombination der Strukturgraphen und Punktmengen dieses Bildes durchgesucht. Die Objekte, deren Teilgraphen aus dem Eingabebild gefunden wurden, werden als erkannt genannt. Die entsprechenden Punktmengen im Eingabebild beschreiben nun die aktuellen Lagen der Objekte. Mit dem gleichen Ablauf können die Objekte in einem Bilderstrom kontinuierlich erkannt werden, aber was nicht möglich ist, die entsprechenden Punktmengen kontinuierlich festzulegen, weil die Marken von jedem Bild neu segmentiert werden und keine Verbindungen zwischen den ,,gleichen'' Punktmengen von unterschiedlichen Bildern existieren. Ein direkter Lösungsverfahren ist, dass jede Punktmenge aus Segmentierung sofort als ein neues Objekt erkannt wird. Der Vergleich von Teilgraph-Isomorphismus-Problem wird dann zwischen die Strukturgraphen zweier Objekte stattgefunden. Dadurch sollen viele Lernensprozesse während der Wiedererkennungsphase gleichzeitig durchgeführt werden, was kostet aber zu viele Zeit und ist schwierig zu implementieren. Die alternative Lösung ist, die sogenannte Kandidaten zu verwenden, in der nur die Abhängigkeiten der gleichen Punktmengen von unterschiedlichen Bildern gespeichert aber kein einziger Strukturgraph dargestellt werden.

\subsection{Kandidaten der Objekterkennung}
Wie im Abschnitt~\ref{Seg} geschrieben werden viele Punktmengen nach der Segmentierung erzeugt, welche mit unterschiedlichen Objekten übereinstimmen können. Die Kandidaten werden aus diesen Punktmengen bestehen und in einer Liste im Speicher gespeichert. Wenn das Programm ein neuen Eingabebild einliest, werden alle neue von Segmentierung erhaltenen Punktmengen mit vorhandenen Kandidaten verglichen. Der Kandidat, der mit Einer der neuen Punktmengen assoziiert, wird dann aktualisiert. Die übrige Punktmengen, die keine entsprechenden Kandidaten finden können, werden als neuen Kandidaten in der Liste eingefügt. Um die Leistungsfähigkeit zu halten, sollen die Kandidaten, die in langer Zeit nicht aktualisiert wurden, aus der Liste entfernt werden. Die Erkennung werden dann für jeden neu aktualisierten Kandidat durchgeführt. Der Index des am besten entsprechenden Objekts wird im Kandidat gespeichert.

\subsection{Objekterkennung}

\begin{figure}[ftb]
\centering
\includegraphics[scale=0.47]{Abbildungen/IsomorphKnoten.png}
\caption{Ein Beispiel der isomorphen Knoten. $P$ und $P'$ sind zwei isomorphen Knoten, die jeweils vier Nachbarn mit gleicher Abständen haben. ($(P,V_1) = (P',V'_1)$, $(P,V_2) = (P',V'_2)$, $(P,V_3) = (P',V'_3)$, $(P,V_4) = (P',V'_4)$)}
\label{IK}
\end{figure}

Die Objekterkennung basiert auf dem Vergleich zwischen den Strukturgraphen der erlernten Objekte und den Kandidaten, die von den neu segmentierten Punktmengen erzeugt werden. Ein neuer Graph wird zuerst von der Punkten eines Kandidaten dargestellt. Der Vergleich zwischen diesem Kandidat und den vorhandenen Strukturgraphen  können dann zum Teilgraph-Isomorphismus Problem abgleitet werden. Der Lösungsverfahren übernimmt die Idee von Rhijn und Mulder \cite{AJ05}, was im Abschnitt \ref{TI} erklärt wurde. Die isomorphen Knoten zwischen zwei Graphen werden zuerst herausgefunden, welche jeweils genug isomorphen Nachbarn haben, die mit gleichen Abständen zu den isomorphen Knoten liegen. Ein Beispiel der isomorphen Knoten wird in Abbildung~\ref{IK} gezeigt. Der Teilgraph-Isomorphismus zwischen diesen Graphen existiert, genau dann, wenn mindesten ein gefundener isomorpher Knoten und seine Nachbarn zusammen die gleichen Pyramiden darstellen können. In dieser Arbeit wird der Verfahren von Rhijn und Mulder in zwei Bereichen verbessert: die effizientere Nebenbedingung für die Suchung der isomorphen Knoten und die vereinfachte Endbedingung statt Feststellung der Pyramide. 
\\
\\
\textbf{Nebenbedingung für die Suchung der isomorphen Knoten}
\\
Um die isomorphen Knoten schneller zu finden, werden zwei Quoten für die Suchung definiert. Die Abstandquote wird statt dem absoluten Schwellenwert benutzt, damit der Vergleich der Distanzen zwischen dem betrachteten Knoten und seiner Nachbarn bewertet werden kann. Offensichtlich wird die Kante zwischen zwei Marken auf dem Objekt zu mehr Bildpunkten abgebildet, wenn das Objekt zu der Kamera bewegt. Je größer die Auflösung des Objekt ist, desto das bessere Erkennungsergebnis von dem Programm erzeugt werden kann. Deshalb lässt die Verwendung der Abstandquote die Genauigkeit der Erkennung nicht von der Distanz zwischen dem Objekt und der Kamera beeinflussen. 
\\
\\
Die zweite Nachbarquote beschränkt die minimale Anzahl der nötigen assoziierten Nachbarn für Bestimmung eines isomorphen Knotens, was im Verfahren von Rhijn und Mulder aber als Invariante definiert wird. Die Beziehung zwischen der Nachbarquote und der minimalen Anzahl der Nachbarn kann in

\begin{equation}
N_{min} = \left\{
\begin{array}{l l}
3 & \text{if } \| V \| * Nachbarquote<3 \\
\| V \| * Nachbarquote & sonst \\
\end{array}
\right.
\end{equation}

formuliert werden, wobei $V$ die Knotenmenge des Graphen beschreibt. Die minimale Anzahl der Nachbarn darf nicht kleiner als 3 sein, weil es mindesten 3 Nachbarn benötigt, um einziger Knoten eines Graphen durch Vergleich der Kantenlänge seiner Nachbarn im 3D Raum zu bestimmen. Diese Veränderung liefert eine bessere Toleranz für die Erkennung: wenn die Graphen mit nur wenigen Knoten als die Eingaben dem Verfahren eingegeben werden, garantiert der Verfahren aber auch genug Nachbarn, um die isomorphen Knoten auszuwählen; außerdem können die Genauigkeit und Stabilität der Teilgraph-Isomorphismus mit dem Vergrößern der Anzahl der Knoten auch ansteigen.
\\
\\
\textbf{Vereinfachung der Endbedingung}
\\
Im Verfahren von Rhijn und Mulder sollen schließlich die Pyramiden für den isomorphen Knoten und seine Nachbarn gefunden werden, damit der Isomorphismus bestimmt werden kann. Die Pyramide ist ein vollständiger Graph mit 4 Knoten, deshalb für einen Knoten, der $n$ Nachbarn hat, sollen höchstenfalls 

\[
C_n^3 = \binom{n}{3} = \frac{n!}{k!(n-k)!} = \frac{1}{6} n(n-1)(n-2) \approx \mathcal{O}(\frac{1}{6}(n^3-3n^2))
\]

verschiedenen Kombinationen seiner Nachbarn betrachtet werden, um die Pyramide zu finden. Seien $m$ die Anzahl der gefundenen isomorphen Knoten und der Zeitaufwand von Überprüfung der Nachbarschaft der Einheitszeitwand $\mathcal{O}(1)$, dann ist der Zeitaufwand der Suchung der Pyramide in den schlechtesten Fall $\mathcal{O}(\frac{1}{6} m (n^3-n^2))$. Offensichtlich gilt es $m < n$ wegen der Definition des isomorphen Knoten. Die Idee der Vereinfachung ist, statt der Nachbarn eines isomorphen Knoten aber die isomorphen Knoten selbst zu betrachten, um den obigen schlechtesten Fall zu vermeiden. Die Bedingung für die Pyramide kann auch geschwächt werden, d.h. man kann die einfacheren geometrischen Elemente benutzen, z.B. das Dreieck sogar eine Ecke mit drei Knoten aber nur zwei Kanten. $m$ isomorphe Knoten werden durchgesucht, um zu bestimmen, ob sie zumindest mit zwei anderen isomorphen Knoten verbinden. Der Zeitaufwand dieses Ablaufs ist nur $\mathcal{O}(m^2)$, was deutlich kleiner als den Verfahren mit Suchung der Pyramide. Abbildung~\ref{AI} zeigt ein Beispiel für den Vergleich der beiden Nebenbedingung des Isomorphen-Problems. An der linken Seite versucht das Programm, eine Pyramide an den isomorphen Knoten zu finden. Alle Kombinationen der drei Nachbarn werden für jeden isomorphen Knoten betrachtet, dadurch ist der Zeitaufwand der linken Seite: 

\[
\binom{3}{3}(V_1) + \binom{3}{3}(V_2) + \binom{4}{3}(V_3) + \binom{6}{3}(V_4) + \binom{5}{3}(V_5) = 1 + 1 + 4 + 20 + 10 = 36.
\]

An der rechten Seite werden aber nur die isomorphe Knoten selbst betrachtet, wobei $m^2=5^2$ Einheitszeit kostet. Alle isomorphen Knoten, die diese Bedingung erfüllen, werden dann gespeichert, um die Orientierung des erkannten Objekts zu berechnen. 

\begin{figure}
\centering
\includegraphics[scale=0.5]{Abbildungen/AlgoIsomorphismus.png}
\caption{Ein Beispiel für den Vergleich der zwei Endbedingungen. $V_1 \sim V_4$ sind die isomorphen Knoten und $N_1 \sim N_6$ sind die Nachbarn von Knoten $V_4$. An der linken Seite wird der Verfahren mit Pyramide gezeigt, wobei alle isomorphen Knoten durchgesucht werden, bis eine Pyramide $V_4 N_5 N_6 N_1$ gefunden ist. An der rechten Seite werden aber nur die Kanten zwischen den isomorphen Knoten betrachtet.}
\label{AI}
\end{figure}

\subsection{Bestimmung der Orientierung}
Verfolgung des Objekts ist die andere wichtige Aufgabe dieser Arbeit, was durch die Berechnung der Orientierung realisiert wird. Die Anfangslage eines Objekts wird von dem Strukturgraphen definiert. In jedem Bild werden die Rotation- bzw. Translationsmatrix zwischen dem Anfangs- und aktuellen Zustand mithelfe der ausgewählten isomorphen Knoten bestimmt. Der Orientierungsverfahren läuft gleich wie den Verfahren im Lernensprozess, der im Abschnitt~\ref{BdO} erklärt wurde.

\section{Bildersteuerung}
Speichern und Auswahl der Bildern sind die Hauptaufgaben des Teilprogramm der Bildersteuerung. Die Verfolgung der Marken wird durch den Vergleich der Marken von zwei nachfolgenden Bildern realisiert, d.h. soll das Programm immer zwei Bilder gleichzeitig betrachten: das aktuelle- und das historische Bild. Außerdem wegen Rauschen und andere Störung aus den Eingabebildern, werden manchmal die Marken total schlecht erkannt. Die falschen Marken verstören die Korrespondenzuntersuchung und verschlechtern weiterhin die Orientierung der Objektslagen zwischen zwei Bildern, was aber ganz wichtig für das Lernen des Objekts. Wenn irgendwelche kleinen Fehler in der Transformation vorkommen, wird der Strukturgraph des lernenden Objekts komplett andere dargestellt. Deshalb sollen die schlechten Eingabebilder vor dem Darstellung des Strukturgraphen ausgewählt und von dem Bilderstrom entfernt werden. 
\\
\\
In dieser Arbeit werden allen Eingabebilder zuerst in einer Warteschlange gespeichert. Die Warteschlange wird mit fester Größe definiert, und wenn neues Bild eingegeben wird, wird es im Ende der Schlange eingefügt und das älteste Bild aus die erste Stelle der Schlange gestrichen. In jedem Zyklus des Programms werden das erste und letzte Bild verglichen und die korrespondierenden Punktpaare daraus gefunden. Mit anderen Worten definiert die Größe der Warteschlange aber auch das Intervall der betrachteten Bilder. Die Markenanalyse und die Orientierung von Objektlernen werden regelmäßig für jedem Eingabebild durchgeführt, aber die davon erhaltenen Rotation- bzw. Translationsmatrix werden nicht direkt für die Aktualisierung des Strukturgraphen benutzt, sonder zuerst von anderem Teilprogramm geprüft werden, damit die Qualität der Transformation bewerten werden kann. 
\\
\\
Das Prüfungsprogramm besteht aus zwei Teilen. Der erste Teil ist ein Prüfer, der eine boolesche Aussage liefert, ob aus dem Bild genug hochqualitativen Marken gefunden werden können. Der Prüfer läuft analog wie die Aktualisierung der Knoten von dem Teilprogramm des Strukturgraphen, was im Abschnitt~\ref{BdsK} beschrieben wurde. Der Unterschied liegt daran, dass der Prüfer die Lebenszeit der Marken nicht betrachtet, aber nur die neu gefundenen Marken zählen, die mit den vorhandenen Knoten des Strukturgraphen nach gerade berechneter Transformation identisch sind. Nur das Bild mit genug Marken, die obige Bedingung erfüllen, wird von dem Prüfer akzeptiert. Die Beurteilung über die Anzahl der Marken wird durch den Vergleich der Quote, die der Anteil der mit vorhandenen Knoten übereinstimmten Marken an allem neuen Marken beschreibt, mit einem vordefinierten Schwellenwert realisiert. Die von Prüfer akzeptierten Bilder können direkt für die Aktualisierung der Strukturgraphen benutzt. Die anderen Bilder sind überflüssig, können aber nicht direkt alles geschmissen werden. Unter einer total schlechten Beobachtungssituation liefert die Kamera möglicherweise in langer Zeit gar keine hochqualitativen Bilder. Wenn das Programm alle diesen Bilder überspringt, wird die Verfolgung des Objekts abgebrochen und falschen Erkennungsergebnisse erzeugt. Ein extremes Beispiel wird in der Umdrehung des Kästchens gefunden. Wenn nur die Bilder von zwei benachbarten Ebenen des Objekts von dem Programm akzeptiert aber die Bilder des Rotationsablaufs dazwischen nicht berücksichtigt werden, hat das Programm aber gar keine Möglichkeit, diese zwei Ebenen zu unterscheiden. Sie werden als eine große Ebene erkannt und gespeichert. Der zweite Teil des Prüfungsprogramms vermeidet diese Situation. Die maximale Anzahl der übersprungenen Bilder wird von dem beschränkt. Wenn kein hochqualitatives Bild gefunden werden kann, wird ein verhältnismäßig besseres Bild aus die schlechten Bilder ausgewählt. Die Summe aller größten Elemente von $P$, die im Abschnitt \textbf{Qualitätsmanagement der Markenverfolgung} von \ref{MV} erklärt wurde, wird hier als die Bewertungsvariable verwendet. Der Durchlauf des Prüfungsprogramms wird im Algorithmus~\ref{algBP} aufgeführt.

\begin{algorithm}                     
\caption{Bilderprüfer($Strukturgraph$, $Eingabebild$, $UebersprungeneAnzahl$)}         
\label{algBP}                          
\begin{algorithmic}
\State die minimale Quote der mit vorhandenen Knoten übereinstimmten Marken: $q$
\State die maximale Anzahl der Bilder, die übersprungen werden dürfen: $N_{jump}$
\State Iterator des Besten Bild in $Bildschlange$: $i_b$
\If {$Korrespondenzuntersuchung()$ == True und $Identische Quote>q$}
	\State Aktualisieren $Strukturgraph$ mit $Eingabebild$
\Else
	\If {$Korrespondenzuntersuchung()$ == False}
		\State Überspringen
	\EndIf
	\If {$Eingabebild.SumP > Bildschlange[i_b].SumP$}
		\State Entfernen $Bildschlange[i_b]$
		\State $i_b \gets Eingabebild.iterator$
	\Else
		\State Überspringen
	\EndIf
	\State $UebersprungeneAnzahl$ ++
	\If {$UebersprungeneAnzahl > N_{jump}$}
		\State Aktualisieren $Strukturgraph$ mit $Bildschlange[i_b]$
		\State $UebersprungeneAnzahl \gets 0$
	\EndIf
\EndIf
\end{algorithmic}
\end{algorithm}