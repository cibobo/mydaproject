\chapter{Grundlagen}

\section{Generierung der 3D-Daten}

\subsection{TOF-Sensor}

\subsubsection{TOF Kamera}
\label{TOF Kamera}
Die im MAROCO-System verwendete Kamera gehört zur Klasse der TOF-Sensoren, die außer den normalen Graufarbenbildern auch Tiefbilder liefern kann. Die Tiefmessung basiert auf dem sogenannten Laufzeitverfahren. Dazu wird die Szene durch ein Lichtpuls ausgeleuchtet und für jeden Bildpunkt wird die Zeit gemessen, die das Licht bis zum Objekt und wieder zurück braucht. Die Distanz ist direkt proportional zu der benötigen Zeit und kann durch die folgende Formel berechnet werden:

\begin{equation}
\label{tof1}
d = \frac{t_d}{2c}
\end{equation} 

wobei $t_d$ die gemessene Zeit bezeichnet. Die Konstante $c$ steht für die Lichtgeschwindigkeit. 
\\
\\
Im Vergleich zu anderen 3D Kamerasystemen hat die TOF Kamera viele Vorteilen \cite{TOFWiki}. Zuerst kann die TOF Kamera einfach die interessierenden Bereiche aus einem Bild extrahieren und nur die Pixel nah vor der Kamera betrachten. Zweitens, kann die TOF Kamera eine hohe Bildrate bis zu 80 bps erreichen. Diese Eigenschaft ermöglicht somit Echtzeitanwendungen. Außerdem benötigt die TOF Kamera weniger Platz als z.B. das Triangulationssystem und hat niedrigere Abhängigkeit von der Systemstruktur gegenüber dem Stereosystem.


\subsubsection{PMD Sensor}
Der PMD Sensor heißt CamCube ist eine wichtige Komponente der TOF Kamera. Er liefert eine hohe Auflösung bis zum 204x204 Pixel und maximale Bildrate bis zum 40 bps. Durch Formel \eqref{tof1} wird der maximale Distanzbereich zum 7 m festgelegt. Die andere wichtigen Parameter findet man in Abbildung~\ref{PMDParam} \cite{pmde}.
\\
\\
%\graphicspath{}
\begin{figure}[ftb]
\centering
\includegraphics[scale=0.6]{Abbildungen/PMDParam2}
\caption{Die grundlegende Parameter der PMD CamCube2. \cite{pmde}}
\label{PMDParam}
\end{figure}

Das Hintergrundlicht, z.B. das Sonnenlicht, könnte die Messung der Distanz stark stören. Die PMD Kamera benutzt das aktive Sendersignal und einen Fremdlicht-Filter (SBI), um das Hintergrundsignal zu unterdrücken. Außerdem bietet die PMD Kamera die Möglichkeit, die Integrationszeit der Kamera für jede Messung individuell einzustellen. Die Integrationszeit bezieht sich auf die Zeitspanne, in der die Kamera zur Aufzeichnung eines Bildes dem reflektierten Licht ausgesetzt wird. Für ein schwach reflektierendes Objekt benötigt der Sensor längere Integrationszeit als ein stark reflektierendes Objekt, um genug Information anzusammeln. Andererseits wird aber ausreichendes Licht von hellen Objekten auf den Sensor reflektiert, wenn die Integrationszeit zu lang definiert wird. In Abbildung~\ref{PMDIntTime} wird ein Beispiel der Tiefbilder mit verschiedenen Integrationszeiten gezeigt \cite{pmdd}. 

\begin{figure}[ftb]
\centering
\includegraphics[scale=0.6]{Abbildungen/PMDIntTime}
\caption{Integrationszeit von 140 $\mu s$, 1400 $\mu s$ bzw. 14000 $\mu s$. Bitte beachten Sie die niedrige Signalstärke an der linken Seite und die Sättigung an der rechten Seite wegen der unangemessenen Integrationszeit. \cite{pmdd}}
\label{PMDIntTime}
\end{figure}

\subsubsection{Unterschied zwischen TOF Kamera und Kinect}
Kinect ist eine Hardware zur Steuerung der Videospielkonsole Xbox360, die ein sogenanntes hands-free Kontrollieren liefert, wodurch die Spieler mit einigen bestimmten Gesten oder einer kurzen Bewegung ihres Körpers das Spiel spielen können \cite{KIN}. Um dieses Ziel zu erreichen, sammelt Kinect außer der normalen Bildeingabe aber auch die Tiefdaten der Szene an. Wegen dieser Eigenschaft wird das Gerät im Bereich von Computer Vision benutzt. Mithilfe des SDK ist die Programmierung der Kinect unter normalen Betriebssystemen z.B. Windows, Linux bzw. MacOS möglich. 
\\

\begin{figure}[bft]
\centering
\includegraphics[scale=0.6]{Abbildungen/Kinect}
\caption{Das Arbeitsprinzip des Kinects. \cite{KINH}}
\label{Kinect}
\end{figure}

Der Unterschied zwischen TOF Kamera und Kinect können auf dem Arbeitsprinzip zurückgeführt werden. In TOF Kameras wird die Tiefdaten durch dem Laufzeitverfahren berechnet, was im \ref{TOF Kamera} erklärt wird. Das Abtastverfahren der Kinect heißt Light Coding. Eine große Menge von Streifen werden als Mustern auf die Szene bzw. die Objekte durch infrarotes Licht projiziert. Die ganz Szene mit diesen zusätzlichen Mustern wird von einer infraroten Kamera des Kinects aufgenommen. Durch die Verzerrung zwischen dem vordefinierten Muster im infraroten Licht und dem von der infraroten Kamera erkannten Muster kann das Tiefbild der Szene ausgerechnet werden. Die Abbildung~\ref{Kinect} zeigt dieses Arbeitsprinzip der Kinect. Die weitere Information findet man im technischen Dokument des Firma Cadet \cite{KINH}. Der Vergleich über die genauen technischen Daten von PMD Kamera und Kinect wird in Tabelle~\ref{PMD and Kinect} zusammengefasst. Obwohl Kinect eine bessere Auflösung und größeres Sichtfeld hat, ist die PMD Kamera wegen ihrer hohen Bildrate und großen Messbereich für das MAROCO-System geeignet. Außerdem mithilfe des SBI Systems ist die Arbeit der PMD Kamera unter schwieriger Umgebungsbedingung, z.B. außerhalb des Zimmers mit starker Störung von Sonneneinstrahlung, auch möglich.

\begin{table}[ftb]
\begin{center}
\begin{tabular}{| l || c | c |}
\hline
Sensor & PMD CamCube & Kinect \\ \hline
Auflösung & 204$\times$204 Pixel & 640$\times$480 Pixel\\ \hline
Sichtfeld & $40^\circ \times 40^\circ$ & $57^\circ \times 43^\circ$ \\ \hline
Max Bildrate & 40 fps & 30 fps \\ \hline
Messbereich & 0.3 $\rightarrow$ 7.0 m & 1.2 $\rightarrow$ 3.5 m (mit Xbox Software) \\ \hline
Lange der Tiefdaten & 8 bit (unsigned char) & 11 bit \\ \hline
\end{tabular}
\caption{Die technische Daten von PMD Kamera und Kinect}
\label{PMD and Kinect}
\end{center}
\end{table}

\subsection{Daten der PMD Kamera}
\label{PMDData}
Die PMD Kamera CamCube kann insgesamt 4 verschiedenen Vermessungsdaten ausgeben. Sie sind Amplitude, Intensität, Distanz und 3D Koordinaten. Die erste Zwei und letzte Zwei Typen der Daten können durch die Dimension in zwei Gruppen unterteilt werden. 

\subsubsection{2D Daten}
Die Intensität bezieht sich auf Graustufen. Nach einer Abbildung können diese Graustufen auf das Intervall 0 bis 255 beschränkt werden und das Ergebnis ist das Bild einer normalen Schwarz-Weiß Kamera. Die Amplitude zeigt die Stärke der Beleuchtung, die vom Objekt wegen des aktiven Sendersignal von PMD Kamera selbst reflektiert wird. Dieser Wert kann die Qualität der Distanzinformation schätzen, d.h. das Objekt weit von Kamera liefert niedriger Amplitude als das Objekt in der Nähe von der Kamera, wenn sie mit identischem Material dargestellt werden. An der Gegenseite sind die Merkmalen mit höherem Rückstrahlvermögen durch Amplitudedaten einfacher betrachtet, was für die Erkennung der großen künstlichen Marken in unserer Arbeit sinnvoll ist.   

\subsubsection{3D Daten}
Die PMD Kamera liefert 3D Daten in zwei Formen: die reine Distanzinformation und die 3D Koordinaten. Die Distanzinformation ist die gemessene Distanz zwischen der Kamera und Objekt, was direkt durch die Formel \eqref{tof1} berechnet wird. Bezüglich dieser Distanzinformation und der 2D Daten der normalen Kamera werden die 3D Koordinaten innerhalb der PMD Kamera berechnet und können durch die Schnittstelle abgefragt werden. Die Abbildung~\ref{C3D} zeigt die Visualisierung einer Szene mit jeweils der 3D Koordinaten und Distanzinformation von PMD Kamera. Eine Transformation ist notwendig, wenn man direkt die Distanzinformation im kartesischen Koordinatensystem visualisieren möchte.

\subsubsection{Auflösung und erkennbare Markengröße}
Die Auflösung der PMD Kamera wurde schon im vorherigen Abschnitt angegeben, wofür man sich noch interessiert ist, wie genau die Objekte von der Kamera in der Praxis beobachtet werden können. D.h. wie groß ein Pixel von Kamera den Bereich in realer Welt beschreibt. Die Vorderansicht des Kamerasystems dieser Arbeit ist in Abbildung~\ref{MSF} links gezeigt.

\begin{figure}[bft]
\centering
\includegraphics[scale=0.44]{Abbildungen/MarkerSizeForme}
\caption{Die Vorderansicht(links) und die Draufsicht(rechts) des PMD Kamera Systems}
\label{MSF}
\end{figure}

$\theta$ ist der halbe Sichtwinkel und wird hier als $20^\circ$ angenommen. $H$ zeigt den Abstand von der Kamera zum Boden, was in diesem System mit 3,2m festgelegt ist. Durch die Trigonometrie kann der Radius des Sichtbereiches auf dem Boden berechnet werden als:

\[
R = \tan \theta \cdot H =  0.36397 \cdot 3,2m = 1,16470m.
\]

Die halbe Seitenlänge des Sehnenquadrats $L$ (Sehen Abbildung~\ref{MSF} rechts) ist gleich:

\[
L = \cos 45^\circ \cdot R = 0,70711 \cdot 1,16470m = 0,82357m.
\]

Dann kann man der Flächeninhalt des Sichtbereiches auf dem Boden erhalten:

\[
S = 4 \cdot L^2 = 2,7131 m^2.
\] 

Das Ergebnis der Division von dem Flächeninhalt und der Auflösung beschreibt die Größe der Zelle auf dem Boden, die in Kamera als eigenes Pixel dargestellt wird.

\[
S_z = S / (204 \times 204) = 2,7131 m^2 / 41616 = 6,5194 \times 10^{-5} m^2 = 0,65194 cm^2
\]

Die Marken, die kleiner als $S_z$ sind, werden in der Kamera kleiner als ein Pixel abgebildet und sind natürlich schwer zu erkennen. D.h. $S_z$ definiert die minimale Größe der erkennbaren Marken. Die minimale Seitenlänge kann nun berechnet werden durch:

\[
L_z = \sqrt{S_z} = 0,80743 cm
\]

Die minimalen Größen der Marken für anderen zum Boden parallelen Ebenen können analog berechnet. Z.B. in der normalen Arbeitsebene dieser Arbeit, die zum Boden $1,1m$ entfernt, können die mindesten Quadrate mit Seitenlänge von $0,52987cm$ erkannt werden.

\begin{figure}
\centering
\includegraphics[scale=0.45]{Abbildungen/Compare-3D-Dis}
\caption{Die Visualisierung von 3D Koordinaten(link) und Distanzinformation(recht).}
\label{C3D}
\end{figure}

\section{Vorverarbeitung}
\subsection{Schwellwert-basierte Segmentierung}
\label{sbSeg}
Um bessere Erkennungsergebnisse zu erhalten, sollen die wesentlichen Bereiche von der Umgebung getrennt werden. In dieser Arbeit wird eine Schwellwert-basierte Segmentierung bezüglich der 3D Daten verwendet. Eine Schwellwert-basierte Segmentierung kann als eine Abbildung $f$ vom originalen Bild $I$ zum Ergebnisbild $H$ definiert werden:

\[
f: I \xmapsto{} H
\]

mit 

\begin{equation}
H_{ij} = 
\begin{cases}
1 & \text{für } I_{ij} > \Theta \\
0 & \text{sonst}
\end{cases}
\end{equation}

wobei $\Theta$ der eingegebene Schwellwert ist.
 
\subsection{Steuerung der Helligkeit und Kontrast}
Außer der Umgebung beeinflussen die Helligkeit und der Kontrast die Qualität bzw. Stabilität der Markenerkennung. Deshalb ist die Optimierung dieser zwei Parameter in der Vorverarbeitungsphase notwendig. In dieser Arbeit wird ein affiner Operator auf jedem Punkt durchgeführt, um die geeignete Helligkeit bzw. den Kontrast zu bestimmen. Der affine Operator ist eine Abbildung von Originalbild $I$ zum Ergebnisbild $H$ mit:

\begin{equation}
H_{ij} = a I_{ij} + b,
\end{equation}

wobei die Parameter $a \in R^+$ und $b \in R$ die Helligkeit und den Kontrast kontrollieren. Es gibt vier Möglichkeiten für die verschiedenen Zuordnungen dieser Parameter:

\begin{itemize}
\item $a>1$, $b=0$     Kontrasterhöhung
\item $0<a<1$, $b=0$ Kontrastminderung
\item $a=1$, $b>0$     Helligkeitserhöhung
\item $a=1$, $b<0$     Helligkeitsminderung
\end{itemize}

\section{Markenerkennung}
Wie im Abschnitt \ref{mErkennung} beschrieben, sind viele Erkennungsalgorithmen in der Open Source Library OpenCV realisiert. Wegen verbreiteter Benutzung von OpenCV, werden häufig Vergleiche dieser Algorithmen gemacht. Im folgenden Abschnitt wird auf einen Vergleich eingegangen, um den geeigneten Algorithmus für diese Arbeit auszuwählen.
     
\subsection{Auswahl des Erkennungsalgorithmus}
\label{AusAlgo}

\begin{figure}[bft]
\centering
\includegraphics[scale=0.22]{Abbildungen/SampleBild}
\caption{Die vier Beispielbilder. Von link nach recht sind Barbara, Lena, Peppers und Mandril. \cite{O11}}
\label{4Samp}
\end{figure}

\begin{figure}[bft]
\centering
\includegraphics[scale=0.7]{Abbildungen/Number-of-detected-features}
\caption{Anzahl der erkannten Merkmalen von alle vier Beispielbildern durch verschiedene Erkennungsalgorithmen. \cite{O11}}
\label{Nodf}
\end{figure}

Odessa hat in seinem Blog einen sehr guten Vergleich für alle Erkennungsalgorithmen von OpenCV durchgeführt \cite{O11}. Vier häufig benutzte Beispielbilder wurden betrachtet (s. Abb.~\ref{4Samp}).
\\
\\
Was besonders interessiert in seiner Arbeit, ist der Vergleich über die Anzahl der betrachteten Punkte bzw. der durchschnittlichen Fehler. Wegen der verschiedenen Prinzipien erkennt der Algorithmus FAST viel mehr Merkmale als SURF und STAR(Name von CenSurE Algorithmus in OpenCV). Den Unterschied sieht man deutlich in der Abbildung~\ref{Nodf}. Je mehr Punkte betrachtet werden, desto mehr Rauschen wird in das System gebracht, weil viele normale Pixel auch als Merkmale erkannt werden. Das ist offensichtlich ein negativer Einfluss für die weitere Analyse der Daten. Abbildung~\ref{Ate} zeigt den durchschnittlichen Fehler in Pixeln von den Punktpaaren, was durch gleichen Erkennungsalgorithmus von Bezugsbildern erkannt wird. Der Algorithmus STAR erzeugt deutlich weniger Fehler in der Erkennung, und das Ergebnis hängt auch leicht von der Eingabe ab. Wegen der niedrigeren Fehlerquote und besserer Konzentration an großen kreisförmigen Merkmalen ist der STAR Algorithmus für die Erkennung der Objekte mit künstlichen Marken sehr geeignet.

\begin{figure}[bft]
\centering
\includegraphics[scale=0.7]{Abbildungen/Average-tracking-error}
\caption{Der durchschnittliche Fehler (in Pixeln) zwischen den assoziierten Punkten von zweier folgendes Bilder. \cite{O11}}
\label{Ate}
\end{figure}

\subsection{CenSurE Algorithmus}
Der Algorithmus CenSurE (Center Surround Extrema) wird von Agrawal et al. 2008 entwickelt \cite{AKB08}. Sie verbessern die SIFT bzw. SURF Verfahren durch Berücksichtigung aller Merkmale in allen Skalenräume. Das Extremum durch die Skalen und Lagen werden ausgewählt, um die Merkmale zu bestimmen. Der Bi-level Filter wird hier statt Gaussian Filter verwendet, damit der Algorithmus in Echtzeit laufen kann, obwohl große Menge der Berechnung für alle Merkmale aller Skalenräume nötig sind. 

\subsubsection{Bi-level Filter}
Der Bi-level Filter ist eine einfache Approximation des Laplacian-Operators durch die Multiplikation der Bilder mit 1 und -1. Die Abbildung~\ref{bi-level} zeigt die Progression des Bi-level Filters mit verschiedenen Symmetrischen Stufen. Der kreisförmige Filter an linker Seite der Abbildung~\ref{bi-level} kann den Laplacian-Operator am Beste approximieren, ist aber leider schwierig zu implementieren. Deshalb werden die Progressionen der Filter durch Polygone angenähert, damit die Berechnung vereinfacht werden kann. Zum Vergleich der übrigen Formen der Abbildung~\ref{bi-level} liefert der Filter mit Achteck die beste Leistung und der Filter mit Rechtecke die kürzeste Laufzeit. Diese Polygon-Filter können durch die integralen Bilder einfach dargestellt werden.  

\begin{figure}[bft]
\centering
\includegraphics[scale=0.6]{Abbildungen/Bi-level-filter2}
\caption{Progression der Center-Surround Bi-level Filter. Der Kreis ist die ideale voll symmetrische Approximation der Laplacian. Die Filter daneben von links nach rechts haben niedriger Symmetrie, brauchen aber  weniger Zeit zur Berechnung. \cite{AKB08}}
\label{bi-level}
\end{figure}

\subsubsection{Integrale Bilder}
Ein integrales Bild $I$ ist eine mittlere Repräsentation eines Bildes, was die Summe der Grauwerte von Bild $N$ mit Breite $x$ und Höhe $y$ enthält. 

\begin{equation}
I(x,y) = \sum _{x'=0}^x \sum_{y'=0}^y N(x',y')
\label{InteBild1}
\end{equation}   

Das integrale Bild ist rekursiv berechenbar und benötigt nur einmal Durchführung aller Pixeln des Bildes. Mithilfe des integralen Bildes kann die Intensität des beliebigen rechteckigen Bereiches einfach durch vier Additionen berechnet werden. Die Erweiterung des integralen Bildes wird für die Berechnung der Polygonen Filter benutzt. Die Kombination zweier verschiedenen schrägen integralen Bilder kann den für Polygone benötigten trapezförmigen Bereich einfach darstellen. Die mathematische Beschreibung des schrägen integralen Bildes ist:

\begin{equation}
I_\alpha(x,y) = \sum_{y'=0}^y \sum_{x'=0}^{x+\alpha(y-y')} N(x',y'),
\label{InteBild2}
\end{equation}

wobei $\alpha$ den schrägen Winkel erklärt und wenn es 0 gleicht, ist die Formel \eqref{InteBild1} genau so wie die Formel \eqref{InteBild2}, und beschreibt ein rechteckiges integrales Bild. Die in der Abbildung~\ref{bi-level} gezeigte Achtecke-Filter und Sechsecke-Filter können mit jeweils 3 bzw. 2 Trapezen schnell aufgebaut werden.
  
\subsubsection{Non-maximal Suppression}
Non-maximal Suppression ist eine Strategie, die das lokale Extremum finden kann. Die Response des Pixels wird unterdrückt, wenn es ein Pixel in seiner Nachbarschaft für Lage bzw. Skala gibt, dessen Response größer oder kleiner als das betrachtende Pixel ist. Die Pixel mit entweder Maximum oder Minimum Response werden als Merkmale bekannt. Der Suchumfang wird in der Arbeit von Agrawal als 3x3x3 eingestellt, d.h. 8 Pixeln um dem betrachteten Pixel und jeweils 9 Pixeln im zwei benachbarten Skalenräumen, werden zusammen berücksichtigt. Die Pixel mit höherer Response können zwischen der Transformation des Bildes stabiler wieder erkannt werden, deshalb nach der Non-maximal Suppression werden die ausgewählten Extremums noch mal durch einem Schwellwerte-Filter gefiltert, um die besten Merkmale zu bestimmen. 
  
\subsection{Kalman-Filter}

Das Kalman-Filter basiert auf einem linearen, dynamischen System in einem diskretisierten Zeitraum. Die Zustandsgleichung des Systems wird häufig durch eine Differenzengleichung beschrieben. In vielen Fällen werden die Zustände nur durch einen voneinander getrennten Zeitpunkt bestimmt. Kalman hat den Sonderfall der linearen Abhängigkeit der Zustände untereinander betrachtet, und vereinfachte die Zustandsgleichung zur linearen Differenzengleichung.

\subsubsection{Zustandsraummodellierung}
Der nächste Systemzustand kann basierend auf dem aktuellen Systemzustand durch:

\begin{equation}
X_k = F_{k-1} X_{k-1} + B_{k-1} u_{k-1} + w_{k-1} 
\end{equation}

schätzen. Der Index $k$ bzw. $k-1$ beziehen sich auf den Zeitpunkt $t_{k}$ und $t_{k-1}$, wobei $t_k = t_0 + k \Delta t$ und $t_0$ der Anfangszeitpunkt und $k$ eine natürliche Zahl von Interesse ist. Deshalb beschreibt der mehrdimensionale Vektor $X_k$ den Zustand des Systems zur Zeitpunkt $t_k$. Die Matrix $F_{x-1}$ ist die Übergangsmatrix für die zeitlich aufeinanderfolgenden Zustände $X_k$ und $X_{k-1}$. Die Matrix $B_{k-1}$ und Kontrollvektor $u_{k-1}$ stellen den deterministischen Anteil der weiteren äußeren Einflüsse auf das System dar.  Die zufälligen, nicht erfassbaren Komponenten der äußeren Einflüssen werden durch die stochastische Größe $w_{k-1}$ geschätzt, die einer Normalverteilung mit Mittelwert 0 und Kovarianz $Q_{k-1}$ folgt.

\[
w_{k-1} \sim N(0,Q_{k-1})
\]

Wegen dieser Zufallsvariable bilden die Menge aller Zustandsvektoren eine Markov-Kette, d.h. der Zustand zu einem Zeitpunkt $k$ hängt lediglich vom unmittelbaren zeitlichen Vorgänger an $k-1$ ab.
\\
\\
Die Beobachtungen des Systems werden aus modellierbarer Verzerrung und unvorhersagbarem Messrauschen bestanden:

\begin{equation}
Z_k = H_k X_k + v_k.
\end{equation} 

$Z_k$ bezieht sich auf die Messung zum Zeitpunkt $k$. Die Multiplikation von der Beobachtungsmatrix $H_k$ und Zustandsvektor $X_k$ beschreibt die lineare Approximation der Verzerrung des Systems. Das Rauschen $v_k$ wird im Kalman-Filter als zeitlich unkorreliert und normalverteilt angenommen:

\[
v_k \sim N(0,R_k).
\]

\subsubsection{Das Kalman-Filter}
Das Ziel eines Filters ist, durch die Informationen einer Messreihe die Zustände besser schätzen zu können. Da die Rauschterme $w$ und $v$ für alle Zeit die Normalverteilung erfüllen, können die zeitdiskreten Zustände $X_k$ auch durch eine Normalverteilung mit dem Mittelwert $\hat{x}_k$ und der Kovarianz $\hat{P}_k$ ermessen werden.

\[
\hat{X}_k \sim N(\hat{x}_k , \hat{P}_k)
\]

Die Idee des Kalman-Filter ist, eine rekursive Formulierung aufzubauen, die aber nur die Schätzung eines vorherigen Zeitpunkts und die aktuelle Messung benötigt, um die Schätzung des aktuellen Zeitpunkt zu bestimmen. Es gibt hauptsächlich zwei Phasen im Kalman-Filter.

\textbf{Prädiktion}
\\
In dem ersten Schritt dieser Phase wird eine vorangegangene Schätzung $X_{k|k-1}$ für den aktuellen Zeitpunkt vorausgesagt.

\begin{equation}
\hat{x}_{k|k-1} = F_{k-1} \hat{x}_{k-1} + B_{k-1} u_{k-1}
\end{equation}

Die Indizierungsschreibweise $k|k-1$ bezieht sich auf die Bedingtheit zu den Zeitpunkten $k$ und $k-1$. Für die Kovarianz gilt:

\begin{equation}
\hat{P}_{k|k-1} = F_{k-1} \hat{P}_{k-1} F_{k-1}^T + Q_{k-1}.
\end{equation}

\textbf{Korrektur}
\\
Die Vorhersagen von letztem Schritt werden hier durch die neue Messung korrigiert:

\begin{equation}
\hat{x}_k = \hat{x}_{k|k-1} + \hat{K}_k \tilde{y}_k,
\end{equation}
 
\begin{equation}
\hat{P}_k = \hat{P}_{k|k-1} - \hat{K}_k S_k \hat{K}_k^T.
\end{equation}

Die Hilfsgröße der Innovation $\tilde{y}_k$ beschreibt, wie genau die aktuellen Messungen von der vorhergesagten Schätzungen mithilfe der Beobachtungsgleichung approximiert werden:

\[
\tilde{y}_k = Z_k - H_k \hat{x}_{k|k-1}.
\]

$S_k$ bezieht sich auf die Residualkovarianz, wobei gilt:

\[
S_k = H_k \hat{P}_{k|k-1} H_k^T + R_k
\]

und $\hat{K}_k$ ist die zugehörige Kalman-Matrix:

\[
\hat{K}_k = \hat{P}_{k|k-1} H_k^T S_k^{-1}.
\]

\section{Registrierung}
\subsection{Singulärwertzerlegung}
Sei M eine komplexe $m \times n$ Matrix mit Rang $r$. Dann bezeichnet die Singulärwertzerlegung das Produkt:

\begin{equation}
M = U \Sigma V^*
\end{equation}

wobei $U$ eine Unitäre Matrix mit Größe $m \times m$ und $V^*$ die Adjungierte einer Unitäre Matrix mit Größe $n \times n$ ist. $\Sigma$ bezieht sich auf eine $m \times n$ Diagonalmatrix:

\[
\Sigma = 
\begin{pmatrix}
\sigma_1 & & & \vline & & \vdots & \\
& \ddots & & \vline & \cdots & 0 & \cdots \\
& & \sigma_r & \vline & & \vdots & \\
\hline
& \vdots & & \vline & & \vdots & \\
\cdots & 0 & \cdots & \vline & \cdots & 0 & \cdots \\
& \vdots & & \vline & & \vdots & 
\end{pmatrix}
\]

mit $\sigma_1 \geq \cdots \geq \sigma_r > 0$, wobei $\sigma_i , i=1,\ldots , r$ als die Singulärwerte von $\Sigma$ genannt werden.

\subsection{Korrespondenzuntersuchung durch Singulärwertzerlegung}
\label{KdS}
Mithilfe der Singulärwertzerlegung haben Scott und Longuet-Higgins einen Algorithmus zur Bestimmung der assoziierenden Merkmale entwickelt \cite{SL91}. Seien $I$ und $J$ zwei nachfolgende Bilder und haben jeweils $m$ und $n$ Merkmale, die als $I_i (i=1,\ldots ,m)$ und $J_j (j=1,\ldots ,n$) bezeichnet werden. Dann wird eine $m \times n$ Matrix $G$ mit den Elementen

\[
G_{ij} = exp(- \frac{r_{ij}^2}{2 \sigma^2})
\]

definiert, wobei $r_{ij}$ den Abstand zwischen Merkmale $I_i$ und $J_j$ beschreibt. $\sigma$ wird als einen Standard für Abstand definiert, wodurch das Vergrößern oder Verkleinern der Verschiebung des Objekts geschätzt werden kann. Der Wert von $G_{ij}$ nimmt durch die Erhöhung der Distanz von 1 bis 0 monoton ab. Der zweite Schritt des Algorithmus von Scott und Longuet-Higgins ist die Singulärwertzerlegung der Matrix $G$:

\[
G = TDU.
\]

wobei $T$ und $U$ die unitäre Matrix mit jeweils Größe $m \times m$ und $n \times n$ sind, und $D$ eine Diagonalmatrix ist. Sei $E$ eine neue Matrix mit gleicher Größe von $D$, in der aber jedes diagonale Element als 1 ersetzt wird. Nach Austausch der Matrix $D$ durch Matrix $E$ erhält man eine neue orthogonale Matrix:

\[
P = TEU
\]

Die Aufgabe des dritten Schritts ist das Element $P_{ij}$ zu finden, was gleichzeitig das Maximum der Reihe und Spalte ist. Wenn $P_{ij}$ diese Bedingung erfüllt, sagt man, dass es eine Eins zu Eins Korrespondenz zwischen den Merkmalen $I_i$ und $J_j$ gibt. Der ganze Algorithmus ist in Algorithmus~\ref{alg1} aufgeführt, der durch die Eingabe von \cite{SL91} erzeugt wird. 

\begin{algorithm}                      % enter the algorithm environment
\caption{Bestimmung der Korrespondenz der Merkmalen von zwei Bildern}          % give the algorithm a caption
\label{alg1}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}                    % enter the algorithmic environment
    \State $I,J,\sigma, Result$
    \For{$i=1 \to m$, $j=1 \to n$}
    	\State $r_{ij} \gets Dis(I_i, J_j)$
    	\State $G_{ij} \gets exp(-\frac{r_{ij}^2}{\sigma^2})$
    \EndFor
    \State $T,U \gets$ Singulärwertzerlegung von G
    \State $E \gets m \times n$ Diagonalmatrix mit $E_{ii} = 1$
    \State $P \gets TEU$
    \For{$i=1 \to m$}
    	\State $MaxSpalteIndex[i] \gets$ Index der Spalte des maximalen Elements an Reihe $i$.
    	%\State $MaxSpalteIndex_i \gets Max$ 
    \EndFor
    \For{$i=1 \to m$}
    	\If {$P_{iMaxSpalteIndex[i]}$ ist Maximum der Spalte $MaxSpalteIndex[i]$}
    		\State $Result \gets$ Punktpaar($I_i, J_{MaxSpalteIndex[i]}$) 
    	\EndIf
    \EndFor 
\end{algorithmic}
\end{algorithm}


\section{Bestimmung der Transformation}

\subsection{Quaternion}
%Das Quaternion
Ein Quaternion besteht aus einem Vektor mit 4 Elementen, wobei ein Element ein Skalar bezeichnet und die anderen drei eine Richtung im 3D Raum beschreiben. Quaternionen können aber auch als eine Erweiterung der komplexen Zahlen betrachtet werden, deren Imaginärteil nach drei neuen Zahlen $i$, $j$ und $k$ entwickelt wird. Eine Normalform der Quaternion ist gegeben durch:

\[
q = q_0 + i q_x + j q_y + k q_z,
\]

wobei $i$, $j$ und $k$ die sogenannte Hamilton-Regeln erfüllen:

\[
i^2 = j^2 = k^2 = i j k = -1, 
\]
\[
ij = k, \quad jk = i, \quad ki = j, 
\]
\[
ji = -k, \quad kj = -i, \quad ik = -j,
\] 

Eine andere Form mit getrennten Realteil und Imaginärteil wird definiert durch:

\begin{equation}
\label{QForm2}
q = (q_0, \vec{q}),
\end{equation}

wobei $q_0 \in \mathbb{R}$ ein Skalar und $\vec{q} \in \mathbb{R}^3$ ein Vektor ist. 
\\
\\
Sei $r$ ein anderes Quaternion mit:

\[
r = r_0 + i r_x + j r_y + k r_z.
\]

Analog zu Vektoren im $\mathbb{R}^4$ wird das Skalarprodukt zwischen zwei Quaternion definiert als:

\[
\langle q, r \rangle := q \cdot r := q_0 r_0 + q_x r_x + q_y r_y + q_z r_z.
\]

Weiterhin kann die Quaternionmultiplikation mithilfe \eqref{QForm2} berechnet werden als:

\begin{align}
qr & = (q_0 r_0 - \vec{q} \cdot \vec{r} , \quad q_0 \vec{r} + \vec{q} r_0 + \vec{q} \times \vec{r}) \\
\label{QMulti}
& = (q_0 r_0 - q_x r_x - q_y r_y - q_z r_z) \nonumber \\
& + i ( q_0 r_x + q_x r_0 + q_y r_z - q_z r_y ) \nonumber \\
& + j ( q_0 r_y - q_x r_x + q_y r_0 + q_z r_z ) \nonumber \\
& + k ( q_0 r_z + q_x r_y - q_y r_x + q_z r_0 ).
\end{align} 

Die rechte Multiplikation von $r$ in Formel \eqref{QMulti} kann aber auch zu einer links multiplizierten Matrix umgeschrieben werden:

\begin{equation}
qr = 
\begin{pmatrix}
r_0 & -r_x & -r_y & -r_z \\
r_x & r_0 & r_z & -r_y \\
r_y & -r_z & r_0 & r_x \\
r_z & r_y & -r_x & r_0
\end{pmatrix}
 = \mathbf{R} q.
\end{equation}

Das konjugierte Quaternion von $q$ ist definiert als:

\[
\overline{q} = q_0 - i q_x - j q_y - k q_z.
\]

Das Produkt eines Quaternion und dessen Konjugierte ist eine nicht negative reelle Zahl:

\[
q \cdot \overline{q} = q_0^2 + q_x^2 + q_y^2 + q_z^2. 
\]

Mithilfe des konjugierten Quaternion kann man die Länge des Quaternion $|q|$ definieren:

\[
|q| = \sqrt{q \cdot \overline{q}}.
\]

Ist die Länge eines Quaternion gleich 1, nennt man das Quaternion ein Einheitsquaternion. Für ein Einheitsquaternion gilt:

\[
q \cdot \overline{q} = 1 \iff \overline{q} = q^{-1}.
\]

D.h. die Inverse und Konjugierte sind identisch. Für jedes Einheitsquaternion $q \neq \pm 1$ gibt es eine entsprechende Polardarstellung:

\begin{equation}
q = \cos \alpha + \mathit{v} \cdot \sin \alpha
\label{polarQ}
\end{equation}

mit $\alpha = \arccos (q_0) \in (0,\pi)$ und $v = \frac{1}{\sin \alpha} (i q_x + j q_y + k q_z)$. 

\subsection{Beschreibung der Drehungen im Dreidimensionalen Raum mit Quaternionen}
Die Drehungen im dreidimensionalen Raum können durch die Einheitsquaternionen sehr gut beschriebt werden. Eine Abbildung der Rotation $\rho_q$ kann in folgender Form definiert werden:

\[
\rho_q : x \rightarrow q x \overline{q},
\] 

wobei $q$ ein Einheitsquaternion und $\overline{q}$ dessen Konjugierte ist. Mithilfe der Polardarstellung \eqref{polarQ} kann die Abbildung $\rho_q$ sich auf eine Drehung im $\mathbb{R}^3$ um die Achse $\mathit{v}$ mit Winkel $2\alpha \in (0, 2\pi)$ beziehen. Die entsprechende orthogonale Matrix von $q$ ist

\begin{equation}
\label{QuaR}
R =
\begin{pmatrix}
q_0^2 + q_x^2 - q_y^2 - q_z^2 & 2q_x q_y - 2q_0 q_z           & 2q_x q_z + 2q_0 q_y \\
2q_x q_y + 2q_0 q_z           & q_0^2 - q_x^2 + q_y^2 - q_z^2 & 2q_y q_z - 2q_0 q_z \\
2q_x q_z - 2q_0 q_y           & 2q_y q_z + 2q_0 q_z           & q_0^2 - q_x^2 - q_y^2 + q_z^2,
\end{pmatrix}
\end{equation} 

was zur Drehgruppe SO(3) gehört und eine Drehung in der Matrixform repräsentiert.

\subsection{Orientierung mit Einheitsquaternion}
\label{QmE}
Das Verfahren für die Orientierung der Objekte mithilfe der Quaternionen wurde von Horn im 1987 veröffentlicht \cite{H87}. Seien $D$ und $M$ zwei Punktmengen mit gleicher Größe $n$. Dann kann die Transformation zwischen den Punkten von zwei Mengen formuliert werden als:

\begin{equation}
d_i = \mathbf{R} m_i + \mathbf{T} + e_i,
\end{equation}

wobei $d_i$ und $m_i$ die i-ten Punkte der Punktmengen $D$ bzw. $M$ bezeichnen. $\mathbf{R}$ ist die Rotationsmatrix und $\mathbf{T}$ ist die Translationsmatrix. $e_i$ beschreibt den Fehler für die Transformation, und kann umformuliert werden als:

\begin{equation}
\label{QError}
e_i = d_i - \mathbf{R} m_i + \mathbf{T}.
\end{equation}

Das Ziel des Verfahrens ist, eine Rotations- bzw. Transformationsmatrix mit minimalem Fehler zu finden, dadurch wird die Summe des Quadrats von $e_i$ betrachtet:

\begin{equation}
\sum_{i=1}^n \| e_i \|^2 = \sum_{i=1}^n \| d_i - \mathbf{R} m_i + \mathbf{T} \|^2. 
\end{equation} 

Seien $\overline{d}$ und $\overline{m}$ jeweils die Schwerpunkte der Punktmengen $D$ und $M$. Dann gilt

\begin{equation}
\label{QuaSch}
\overline{d} = \frac{1}{n} \sum_{i=1}^n d_i 
\quad , \quad
\overline{m} = \frac{1}{n} \sum_{i=1}^n m_i.
\end{equation}

Der Abstand von jedem Punkt zum Schwerpunkt wird berechnet als:

\begin{equation}
d_i' = d_i - \overline{d}
\quad , \quad
m_i' = m_i - \overline{m}.
\end{equation}

und die Summe der Abstände erfüllt natürlich

\begin{equation}
\label{QDis}
\sum_{i=1}^n d_i' = 0
\quad und \quad
\sum_{i=1}^n m_i' = 0.
\end{equation}

Dann kann der Fehler in Formel \eqref{QError} mit den Abständen zum Schwerpunkt $\overline{d}$ und $\overline{m}$ umgeschrieben werden:

\begin{equation}
e_i = d_i' - \mathbf{R} m_i' + \mathbf{T}',
\end{equation}

wobei $\mathbf{T}'$ als 

\[
\mathbf{T}' = \mathbf{T} - \overline{d} + \mathbf{R} \overline{m}
\]

definiert wird. Analog kann die Summe des Quadrats des Fehlers neu formuliert werden.

\begin{align}
\label{QError2}
\sum_{i=1}^n \| e_i \|^2 &=
\sum_{i=1}^n \| d_i' - \mathbf{R} m_i' + \mathbf{T}' \|^2 \nonumber \\
&= \sum_{i=1}^n \| d_i' - \mathbf{R} m_i' \|^2 - 2\mathbf{T}' \cdot \sum_{i=1}^n (d_i' - \mathbf{R} m_i')
+ n \| \mathbf{T}' \|^2
\end{align}

Wegen \eqref{QDis} ist der zweite Term gleich 0. Der dritte Term kann nicht negativ und wird auch 0 sein, wenn der gesamte Fehler minimiert wird. D.h.:

\begin{align}
\label{QTran}
& \mathbf{T}' = \mathbf{T} - \overline{d} + \mathbf{R} \overline{m} = 0 \nonumber \\
\Rightarrow & \mathbf{T} = \overline{d} + \mathbf{R} \overline{m}.
\end{align}

Die Formel \eqref{QTran} berechnet direkt die Translationsmatrix durch die Rotationsmatrix und die Schwerpunkte der beiden Punktmengen. Der erste Term von \eqref{QError2} kann zu

\begin{equation}
\sum_{i=1}^n \| d_i' - \mathbf{R} m_i' \|^2 = \sum_{i=1}^n (d_i'^t d_i' + m_i'^t m_i' - 2 d_i'^t \mathbf{R} m_i')
\end{equation}

weiter formuliert werden. Dann wird die Minimierung des Fehlers durch die Bestimmung des Maximums der Summe

\begin{equation}
\sum_{i=1}^n d_i'^t \mathbf{R} m_i'
\end{equation}

erreicht. Durch Ersetzen der Rotationsmatrix mit Quaternion $q$ wird das maximierte Problem umformuliert als:

\begin{equation}
\sum_{i=1}^n (q m_i'' \overline{q}) \cdot d_i'',
\end{equation}

wobei $\overline{q}$ das konjugierte Quaternion von $q$ ist, $m_i'' = (0, m_{i,x}', m_{i,y}', m_{i,z}')$ und $d_i'' = (0, d_{i,x}', d_{i,y}', d_{i,z}')$ die erweiterte Quaternion für Punkte $m_i'$ bzw. $d_i'$ sind.
Dann gilt:

\begin{align}
\label{qmdq}
\sum_{i=1}^n (q m_i'' \overline{q}) \cdot d_i''
& = \sum_{i=1}^n (q m_i'' \overline{q}) \cdot ( d_i'' q \overline{q}) \nonumber \\
& = \sum_{i=1}^n (q m_i'') \cdot (d_i'' q).
\end{align}

Die beiden Multiplikationen in Klammen können als Formel \eqref{QMulti} zum Produkt von einem Quaternion und einer Matrix umschrieben werden:

\[
q m_i'' = 
\begin{pmatrix}
0 & -m_{i,x}' & -m_{i,y}' & -m_{i,z}' \\
-m_{i,x}' & 0 & -m_{i,z}' & -m_{i,y}' \\
-m_{i,y}' & -m_{i,z}' & 0 & -m_{i,x}' \\
-m_{i,z}' & -m_{i,y}' & -m_{i,x}' & 0
\end{pmatrix}
 = \mathbf{M}_i q
\]

und 

\[
d_i'' q = 
\begin{pmatrix}
0 & -d_{i,x}' & -d_{i,y}' & -d_{i,z}' \\
d_{i,x}' & 0 & -d_{i,z}' & d_{i,y}' \\
d_{i,y}' & d_{i,z}' & 0 & -d_{i,x}' \\
d_{i,z}' & -d_{i,y}' & d_{i,x}' & 0
\end{pmatrix}
 = \mathbf{D}_i q.
\]

Dann kann \eqref{qmdq} weiter abgeleitet werden:

\begin{align}
\label{qNq}
 \sum_{i=1}^n ( \mathbf{M}_i q) \cdot ( \mathbf{D}_i q)
 & = \sum_{i=1}^n q^t \mathbf{M}_i^t \mathbf{D}_i q \nonumber \\
 & = q^t \Big( \sum_{i=1}^n \mathbf{M}_i^t \mathbf{D}_i \Big) q \nonumber \\
 & = q^t \Big( \sum_{i=1}^n \mathbf{N}_i \Big) q \nonumber \\
 & = q^t \mathbf{N} q,
\end{align}
 
wobei $\mathbf{N}_i = \mathbf{M}_i^t \mathbf{D}_i$ ist, und $\mathbf{N}$ die Summe von $\mathbf{N}_i$ beschreibt. Sei $\mathbf{H}$ die Summe der Kreuzprodukte des Punktpaars von Punktmengen $D$ und $M$:

\begin{equation}
\label{QuaH}
\mathbf{H} = \sum_{i=1}^n m_i d_i^t.
\end{equation}

Es ist deutlich, dass die Größe der Matrix $\mathbf{H}$ 3 $\times$ 3 ist, deshalb kann $\mathbf{H}$ auch als

\begin{equation}
\mathbf{H} = 
\begin{pmatrix}
S_{xx} & S_{xy} &S_{xz} \\
S_{yx} & S_{yy} &S_{yz} \\
S_{zx} & S_{zy} &S_{zz} 
\end{pmatrix}
\end{equation}  
  
geschrieben werden, wobei

\[
S_{xx} = \sum_{i=1}^n m_{i,x}' d_{i,x}' \quad ,\quad
S_{xy} = \sum_{i=1}^n m_{i,x}' d_{i,y}' \quad , \quad \cdots
\]

Dann kann die Matrix $\mathbf{N}$ im \eqref{qNq} durch die Elemente von $\mathbf{H}$ dargestellt werden als:

\begin{equation}
\label{QuaN}
\mathbf{N} = 
\begin{pmatrix}
S_{xx} + S_{yy} + S_{zz} & S_{yz} - S_{zy} & S_{zx} - S_{xz} & S_{xy} - S_{yx} \\
S_{yz} - S_{zy} & S_{xx} - S_{yy} - S_{zz} & S_{xy} + S_{yx} & S_{zx} + S_{xz} \\
S_{zx} - S_{xz} & S_{xy} + S_{yx} & -S_{xx} + S_{yy} - S_{zz} & S_{yz} + S_{zy} \\
S_{xy} - S_{yx} & S_{zx} + S_{xz} & S_{yz} + S_{zy} & -S_{xx} - S_{yy} + S_{zz} 
\end{pmatrix}
\end{equation}

Nach dem Beweis von Horn \cite{H87} wird die Formel \eqref{qNq} maximal, genau dann, wenn $q$ der zu dem maximalen positiven Eigenwert der Matrix $\mathbf{N}$ entsprechende Eigenvektor ist. 

\section{Objekterkennung}

\subsection{DBSCAN}
\label{Dbscan}
DBSCAN, kurz von dem englischen Namen ,,Density-Based Spatial CLustering of Applications with Noise, ist ein auf Dichte basierter Data-Mining-Algorithmus'' \cite{E96}. Die Hauptidee des Algorithmus hängt stark von dem Begriff ,,Dichteverbundenheit'' ab, was durch folgende Definitionen erklärt wird.
\\
\\
Seien $D$ eine Punktmenge im Raum $\mathbb{R}^n$ und $Dist(p,q)$ eine darauf definierte Distanzfunktion. $\epsilon$ und MinPts sind zwei Eingaben des Algorithmus.

\begin{definition}
\textsf{$\epsilon$-Umgebung} $N_{\epsilon}(p)$ ist eine Menge der Punkte um $p$, die 
\[
N_{\epsilon}(p) = \{ q \in D | Dist(p,q) \geq \epsilon \}
\]
\end{definition}

erfüllt.

\begin{definition}
Ein Punkt $p$ ist \textsf{direkt Dichte-erreichbar} zum Punkt $q$, g.d.w:
\begin{align*}
1) \quad & p \in N_{\epsilon}(q)  \nonumber \\
2) \quad & | N_{\epsilon}(q) | \geq MinPts \nonumber
\end{align*}
\end{definition}

\begin{definition}
Ein Punkt $p$ ist \textsf{Dichte-erreichbar} zum Punkt $q$, g.d.w es eine Kette von Punkten $p_1, \ldots , p_n$ mit $p_1 = p$ und $p_n = q$ gibt, wobei $p_{i+1}$ \textsf{direkt Dichte-erreichbar} zum $p_i$ für alle $i \in [1,n]$ ist.
\end{definition}

\begin{definition}
Zwei Punkte $p$ und $q$ heißt \textsf{Dichte-verbunden}, g.d.w es einen Punkt $o$ gibt, wobei $p$ und $q$ jeweils zu $o$ \textsf{Dichte-erreichbar} sind.
\end{definition}

Mithilfe dieser Definitionen der Beziehung zwischen den Punkten kann man eine einzige Definition des Clusters ausgeben.

\begin{definition}
Sei $C$ eine nicht leere Teilmenge von $D$. Dann heißt $C$ ein Cluster, wenn die folgenden zwei Bedingungen erfüllt werden:
\begin{align*}
1) \quad & \text{für $\forall p, q \in D$, wenn $p \in C$ und $q$ ist \textsf{Dichte-erreichbar} zum $p$, dann gilt } q \in C, \nonumber \\
2) \quad & \text{für $\forall p, q \in C$, $p$ ist \textsf{Dichte-verbunden} zu $q$.} \nonumber
\end{align*}
\end{definition}

Dann können alle Punkte von $D$ in drei verschiedene Typen zusammengefasst werden.

\begin{itemize}
\item Kernpunkt: Der Punkt, dessen $\epsilon$-Umgebung größer als MinPts ist.
\item Grenzpunkt: Der Punkt, dessen $\epsilon$-Umgebung nicht groß genug ($<$MinPts), aber zu anderen Punkten Dichte-erreichbar ist.
\item Geräusch: Der Punkt, der zu keinem Cluster gehört.  
\end{itemize}

\begin{figure}[ftb]
\centering
\includegraphics[scale=0.3]{Abbildungen/DBSCAN}
\caption{Ein Beispiel für die drei Typen der Punkte in DBSCAN-Algorithmus. Der rote Punkt A ist ein Kernpunkt. Die gelbe Punkte B und C sind die Grenzpunkte, die von roten Punkten Dichte-erreichbar sind. Wegen keine Dichte-Verbindung zwischen blau Punkt N und andere Punkte, wird N als Geräusch erkannt. \cite{DWiki}}
\label{DBSCAN}
\end{figure}

Abbildung~\ref{DBSCAN} zeigt ein Beispiel für alle diesen drei Typen eines Punkts, wobei die Kernpunkte mit Rot, Grenzpunkte mit Gelb und Rauschen mit Blau dargestellt werden. Die Kreise zeigen die $\epsilon$-Umgebungen für die Punkte an ihren Ursprüngen.
\\
\\
Der Algorithmus durchläuft für jeden Punkt von Punktmenge $D$ und die Lösung hängt von der Reihenfolge der Punkte nicht ab. Ein Kernpunkt soll zuerst gefunden werden, und dann die andere Punkte in ihrer $\epsilon$-Umgebung betrachtet werden. Zwei $\epsilon$-Umgebung werden verknüpft, wenn sie mindesten einen identische Punkt haben. Der genaue Ablauf des DBSCAN-Algorithmus ist in Algorithmus~\ref{algDBSCAN} ausgegeben.

\begin{algorithm}                      % enter the algorithm environment
\caption{DBSCAN}          % give the algorithm a caption
\label{algDBSCAN}                      % and a label for \ref{} commands later in the document
\begin{algorithmic}                    % enter the algorithmic environment
	\State $D, \epsilon, MinPts$
	\State Setzen $C$ zum ersten Cluster
	\For {jede $P_i \in D$}
		\If {$P_i$ ist nicht besucht}
			\State Markieren $P_i$ als besucht
			\State $N = \{ P_j \in D | Dist(P_i, P_j)<\epsilon \}$
			\If {Anzahl der Elemente von $N$ $<$ MinPts}
				\State Markieren $P_i$ als Geräusch
			\Else
				\State Fügen $P_i$ in aktuellem Cluster $C$ ein
				\For {jede $P_j \in N$}
					\If {$P_j$ ist noch nicht besucht}
						\State Markieren $P_j$ als besucht
						\State $N' = \{ P_k \in D | Dist(P_j, P_k)<\epsilon \}$
						\If {Anzahl der Elemente von $N'$ $\geq$ MinPts}
							\State $N = N \cup N'$
						\EndIf 
					\EndIf
					\If {$P_j$ gehört zu keinem Cluster}
						\State Fügen $P_j$ in aktuellem Cluster $C$ ein
					\EndIf
				\EndFor
				\State Setzen $C$ zum nächsten Cluster
			\EndIf
		\EndIf
	\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Teilgraph Isomorphismus}
Die Objekterkennung bzw. Objektverfolgung wird durch dem Vergleich der Modellgraphen und Eingabegraphen realisiert. Ein für diese Arbeit hilfreicher Algorithmus wurde von Rhijn und Mulder entwickelt \cite{AJ05}. Um das Problem des Graphisomorphismus zu vereinfachen, werden nur ein Teilgraph $S_{min}$ von Modellgraphen betrachtet. $S_{min}$ soll ein vollständiger Graph sein und einen Modellgraphen eindeutig bestimmen können. Zuerst wird ein Punkt $p_i$ vom Eingabegraph ausgewählt und die Distanzen von diesem zu allen seinen Nachbarn berechnet. Alle diese Kantenlängen werden mit den Kanten der Modellgraphen verglichen, damit ein Kandidatenpunkt $v_k$ im Modellgraph gefunden werden kann, der genug Kanten mit dem ausgewählten Punkt $p_i$ identisch hat. Zweitens sollen drei Nachbarn von $p_i$ gefunden werden, die mit $p_i$ zusammen einen vollständigen Graphen (Pyramide) darstellen können. Wenn ein Teilgraph von Modellgraphen zum obigen vollständigen Graph assoziiert wird, ist ein Teilgraph Isomorphismus zwischen den Eingabegraphen und Modellgraphen gefunden. Der Algorithm~\ref{isoAlgo} gibt eine genauere Beschreibung des Algorithmus von Rhijn und Mulder an.

\begin{algorithm}
\caption{Teilgraph Isomorphismus}
\label{isoAlgo}
\begin{algorithmic}
	\State Modellgraph:$G_m$, Eingabegraph:$G_d$
	\For {jede $p_i \in G_d$}
		\For {jeder Nachbar $p_j$ von $p_i$}
			\For {alle Punktpaare $(v_k, v_l) \in G_m$} 
				\If {$dist(p_i, p_j) = dist(v_k, v_l)$}
					\State Fügen assoziierte Punktpaar $<p_j, v_l>$ zum $S_i$ ein
				\EndIf 
			\EndFor
			\If {$\| S_i \| \geq \| S_{min} \|$}
				%\State Fügen den Punkt $<p_i, v_k>$ zum Kandidaten $K$ ein
				\If {Drei Punktpaare in $S_i$ gefunden können, damit deren ersten Punkte mit $p_i$ ein Pyramid aufbauen können}
					\State Teilgraph Isomorphismus ist gefunden
					\State Break
				\EndIf
			\EndIf 
		\EndFor
	\EndFor
\end{algorithmic}
\end{algorithm}


